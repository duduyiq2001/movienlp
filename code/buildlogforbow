NN_shortnetbow:
Epoch 1/30
625/625 [==============================] - 4s 5ms/step - loss: 0.6734 - accuracy: 0.5833 - val_loss: 0.6272 - val_accuracy: 0.6882
Epoch 2/30
625/625 [==============================] - 3s 4ms/step - loss: 0.6030 - accuracy: 0.7046 - val_loss: 0.5675 - val_accuracy: 0.7537
Epoch 3/30
625/625 [==============================] - 3s 4ms/step - loss: 0.5455 - accuracy: 0.7578 - val_loss: 0.5139 - val_accuracy: 0.7927
Epoch 4/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4953 - accuracy: 0.7895 - val_loss: 0.4747 - val_accuracy: 0.8081
Epoch 5/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4619 - accuracy: 0.8057 - val_loss: 0.4473 - val_accuracy: 0.8163
Epoch 6/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4381 - accuracy: 0.8139 - val_loss: 0.4300 - val_accuracy: 0.8209
Epoch 7/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4188 - accuracy: 0.8228 - val_loss: 0.4172 - val_accuracy: 0.8245
Epoch 8/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4066 - accuracy: 0.8280 - val_loss: 0.4086 - val_accuracy: 0.8277
Epoch 9/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3946 - accuracy: 0.8318 - val_loss: 0.4026 - val_accuracy: 0.8297
Epoch 10/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3845 - accuracy: 0.8366 - val_loss: 0.3982 - val_accuracy: 0.8317
Epoch 11/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3814 - accuracy: 0.8391 - val_loss: 0.3948 - val_accuracy: 0.8345
Epoch 12/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3720 - accuracy: 0.8440 - val_loss: 0.3922 - val_accuracy: 0.8349
Epoch 13/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3712 - accuracy: 0.8415 - val_loss: 0.3903 - val_accuracy: 0.8367
Epoch 14/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3657 - accuracy: 0.8476 - val_loss: 0.3893 - val_accuracy: 0.8339
Epoch 15/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3616 - accuracy: 0.8479 - val_loss: 0.3876 - val_accuracy: 0.8355
Epoch 16/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3568 - accuracy: 0.8490 - val_loss: 0.3864 - val_accuracy: 0.8367
Epoch 17/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3518 - accuracy: 0.8522 - val_loss: 0.3853 - val_accuracy: 0.8371
Epoch 18/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3504 - accuracy: 0.8524 - val_loss: 0.3845 - val_accuracy: 0.8369
Epoch 19/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3492 - accuracy: 0.8522 - val_loss: 0.3840 - val_accuracy: 0.8391
Epoch 20/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3435 - accuracy: 0.8554 - val_loss: 0.3835 - val_accuracy: 0.8405
Epoch 21/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3409 - accuracy: 0.8581 - val_loss: 0.3840 - val_accuracy: 0.8401
Epoch 22/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3392 - accuracy: 0.8583 - val_loss: 0.3832 - val_accuracy: 0.8365
Epoch 23/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3351 - accuracy: 0.8583 - val_loss: 0.3823 - val_accuracy: 0.8403
Epoch 24/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3309 - accuracy: 0.8597 - val_loss: 0.3821 - val_accuracy: 0.8377
Epoch 25/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3332 - accuracy: 0.8608 - val_loss: 0.3819 - val_accuracy: 0.8401
Epoch 26/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3244 - accuracy: 0.8646 - val_loss: 0.3825 - val_accuracy: 0.8431
Epoch 27/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3259 - accuracy: 0.8642 - val_loss: 0.3817 - val_accuracy: 0.8407
Epoch 28/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3219 - accuracy: 0.8667 - val_loss: 0.3816 - val_accuracy: 0.8389
Epoch 29/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3202 - accuracy: 0.8669 - val_loss: 0.3825 - val_accuracy: 0.8435
Epoch 30/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3172 - accuracy: 0.8680 - val_loss: 0.3812 - val_accuracy: 0.8405
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 500)               250500

 dropout (Dropout)           (None, 500)               0

 dense_1 (Dense)             (None, 200)               100200

 dense_2 (Dense)             (None, 1)                 201

=================================================================
Total params: 350,901
Trainable params: 350,901
Non-trainable params: 0
_________________________________________________________________
Epoch 0: Training loss = 0.6733809113502502
Epoch 1: Training loss = 0.6030099391937256
Epoch 2: Training loss = 0.5455402731895447
Epoch 3: Training loss = 0.49529239535331726
Epoch 4: Training loss = 0.46187683939933777
Epoch 5: Training loss = 0.4380578398704529
Epoch 6: Training loss = 0.41883736848831177
Epoch 7: Training loss = 0.4066373109817505
Epoch 8: Training loss = 0.3946172595024109
Epoch 9: Training loss = 0.38446271419525146
Epoch 10: Training loss = 0.38144737482070923
Epoch 11: Training loss = 0.3720119297504425
Epoch 12: Training loss = 0.37120652198791504
Epoch 13: Training loss = 0.36573612689971924
Epoch 14: Training loss = 0.36160194873809814
Epoch 15: Training loss = 0.3568111062049866
Epoch 16: Training loss = 0.351774126291275
Epoch 17: Training loss = 0.3503856658935547
Epoch 18: Training loss = 0.349163293838501
Epoch 19: Training loss = 0.34354883432388306
Epoch 20: Training loss = 0.34090277552604675
Epoch 21: Training loss = 0.3392292559146881
Epoch 22: Training loss = 0.33510684967041016
Epoch 23: Training loss = 0.3309310972690582
Epoch 24: Training loss = 0.3331918716430664
Epoch 25: Training loss = 0.3244441747665405
Epoch 26: Training loss = 0.3258587718009949
Epoch 27: Training loss = 0.32185667753219604
Epoch 28: Training loss = 0.32022494077682495
Epoch 29: Training loss = 0.3172255754470825
1/1 [==============================] - 0s 60ms/step
[[0.00407646]
 [0.02623948]
 [0.02227231]
 [0.7283625 ]
 [0.9944185 ]
 [0.60188574]
 [0.01237879]
 [0.42580384]
 [0.75909084]
 [0.97218174]
 [0.18651251]
 [0.98165715]
 [0.8199679 ]
 [0.70934844]
 [0.88288397]
 [0.30204812]
 [0.65141165]
 [0.27049407]
 [0.885017  ]
 [0.9980475 ]]
24985
781/781 [==============================] - 1s 1ms/step
0.15349209525715424
NN_longresnetwithbow:
Epoch 1/30
625/625 [==============================] - 5s 6ms/step - loss: 0.7117 - accuracy: 0.5340 - val_loss: 0.6776 - val_accuracy: 0.5803
Epoch 2/30
625/625 [==============================] - 3s 5ms/step - loss: 0.5987 - accuracy: 0.6829 - val_loss: 0.5192 - val_accuracy: 0.7725
Epoch 3/30
625/625 [==============================] - 3s 5ms/step - loss: 0.4797 - accuracy: 0.7832 - val_loss: 0.4518 - val_accuracy: 0.8069
Epoch 4/30
625/625 [==============================] - 3s 5ms/step - loss: 0.4372 - accuracy: 0.8072 - val_loss: 0.4288 - val_accuracy: 0.8163
Epoch 5/30
625/625 [==============================] - 3s 5ms/step - loss: 0.4184 - accuracy: 0.8182 - val_loss: 0.4167 - val_accuracy: 0.8195
Epoch 6/30
625/625 [==============================] - 3s 5ms/step - loss: 0.4024 - accuracy: 0.8262 - val_loss: 0.4087 - val_accuracy: 0.8245
Epoch 7/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3932 - accuracy: 0.8315 - val_loss: 0.4042 - val_accuracy: 0.8281
Epoch 8/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3843 - accuracy: 0.8351 - val_loss: 0.4003 - val_accuracy: 0.8297
Epoch 9/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3780 - accuracy: 0.8386 - val_loss: 0.3966 - val_accuracy: 0.8315
Epoch 10/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3715 - accuracy: 0.8415 - val_loss: 0.3956 - val_accuracy: 0.8289
Epoch 11/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3690 - accuracy: 0.8434 - val_loss: 0.3928 - val_accuracy: 0.8325
Epoch 12/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3630 - accuracy: 0.8447 - val_loss: 0.3927 - val_accuracy: 0.8309
Epoch 13/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3558 - accuracy: 0.8509 - val_loss: 0.3916 - val_accuracy: 0.8333
Epoch 14/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3517 - accuracy: 0.8512 - val_loss: 0.3916 - val_accuracy: 0.8325
Epoch 15/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3494 - accuracy: 0.8538 - val_loss: 0.3924 - val_accuracy: 0.8353
Epoch 16/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3457 - accuracy: 0.8550 - val_loss: 0.3910 - val_accuracy: 0.8359
Epoch 17/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3410 - accuracy: 0.8555 - val_loss: 0.3898 - val_accuracy: 0.8351
Epoch 18/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3371 - accuracy: 0.8601 - val_loss: 0.3906 - val_accuracy: 0.8357
Epoch 19/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3330 - accuracy: 0.8591 - val_loss: 0.3905 - val_accuracy: 0.8371
Epoch 20/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3289 - accuracy: 0.8621 - val_loss: 0.3910 - val_accuracy: 0.8363
Epoch 21/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3267 - accuracy: 0.8633 - val_loss: 0.3907 - val_accuracy: 0.8371
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 500)]        0           []

 dense (Dense)                  (None, 500)          250500      ['input_1[0][0]']

 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']

 dense_1 (Dense)                (None, 400)          200400      ['dropout[0][0]']

 dense_2 (Dense)                (None, 300)          120300      ['dense_1[0][0]']

 dense_3 (Dense)                (None, 200)          60200       ['dense_2[0][0]']

 dense_4 (Dense)                (None, 100)          20100       ['dense_3[0][0]']

 concatenate (Concatenate)      (None, 600)          0           ['input_1[0][0]',
                                                                  'dense_4[0][0]']

 dense_5 (Dense)                (None, 1)            601         ['concatenate[0][0]']

==================================================================================================
Total params: 652,101
Trainable params: 652,101
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 0: Training loss = 0.711698591709137
Epoch 1: Training loss = 0.5986895561218262
Epoch 2: Training loss = 0.4797225594520569
Epoch 3: Training loss = 0.43719589710235596
Epoch 4: Training loss = 0.4183647632598877
Epoch 5: Training loss = 0.4023691415786743
Epoch 6: Training loss = 0.3932183086872101
Epoch 7: Training loss = 0.38427045941352844
Epoch 8: Training loss = 0.3779729902744293
Epoch 9: Training loss = 0.37152963876724243
Epoch 10: Training loss = 0.36898794770240784
Epoch 11: Training loss = 0.3630083501338959
Epoch 12: Training loss = 0.3558371067047119
Epoch 13: Training loss = 0.35170167684555054
Epoch 14: Training loss = 0.34939253330230713
Epoch 15: Training loss = 0.34571757912635803
Epoch 16: Training loss = 0.34101250767707825
Epoch 17: Training loss = 0.3370911180973053
Epoch 18: Training loss = 0.33304116129875183
Epoch 19: Training loss = 0.32889360189437866
Epoch 20: Training loss = 0.32666024565696716
e: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
24985
781/781 [==============================] - 2s 1ms/step
0.15601360816489895
NN_longbow:
Epoch 1/30
625/625 [==============================] - 4s 5ms/step - loss: 0.6735 - accuracy: 0.6063 - val_loss: 0.6369 - val_accuracy: 0.7060
Epoch 2/30
625/625 [==============================] - 3s 4ms/step - loss: 0.5740 - accuracy: 0.7378 - val_loss: 0.4993 - val_accuracy: 0.7879
Epoch 3/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4780 - accuracy: 0.7854 - val_loss: 0.4437 - val_accuracy: 0.8133
Epoch 4/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4448 - accuracy: 0.8023 - val_loss: 0.4211 - val_accuracy: 0.8245
Epoch 5/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4223 - accuracy: 0.8155 - val_loss: 0.4127 - val_accuracy: 0.8271
Epoch 6/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4072 - accuracy: 0.8208 - val_loss: 0.4018 - val_accuracy: 0.8285
Epoch 7/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3982 - accuracy: 0.8294 - val_loss: 0.4005 - val_accuracy: 0.8305
Epoch 8/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3908 - accuracy: 0.8302 - val_loss: 0.3924 - val_accuracy: 0.8333
Epoch 9/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3821 - accuracy: 0.8350 - val_loss: 0.3897 - val_accuracy: 0.8335
Epoch 10/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3756 - accuracy: 0.8370 - val_loss: 0.3883 - val_accuracy: 0.8345
Epoch 11/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3685 - accuracy: 0.8416 - val_loss: 0.3865 - val_accuracy: 0.8367
Epoch 12/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3611 - accuracy: 0.8494 - val_loss: 0.3862 - val_accuracy: 0.8393
Epoch 13/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3570 - accuracy: 0.8502 - val_loss: 0.3841 - val_accuracy: 0.8395
Epoch 14/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3537 - accuracy: 0.8492 - val_loss: 0.3827 - val_accuracy: 0.8405
Epoch 15/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3469 - accuracy: 0.8520 - val_loss: 0.3829 - val_accuracy: 0.8401
Epoch 16/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3466 - accuracy: 0.8518 - val_loss: 0.3822 - val_accuracy: 0.8407
Epoch 17/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3399 - accuracy: 0.8591 - val_loss: 0.3837 - val_accuracy: 0.8419
Epoch 18/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3341 - accuracy: 0.8577 - val_loss: 0.3829 - val_accuracy: 0.8395
Epoch 19/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3299 - accuracy: 0.8601 - val_loss: 0.3870 - val_accuracy: 0.8415
Epoch 20/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3263 - accuracy: 0.8618 - val_loss: 0.3844 - val_accuracy: 0.8431
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 500)               250500

 dropout (Dropout)           (None, 500)               0

 dense_1 (Dense)             (None, 400)               200400

 dense_2 (Dense)             (None, 300)               120300

 dense_3 (Dense)             (None, 200)               60200

 dense_4 (Dense)             (None, 100)               20100

 dense_5 (Dense)             (None, 1)                 101

=================================================================
Total params: 651,601
Trainable params: 651,601
Non-trainable params: 0
_________________________________________________________________
Epoch 0: Training loss = 0.6734841465950012
Epoch 1: Training loss = 0.5739713311195374
Epoch 2: Training loss = 0.4780489206314087
Epoch 3: Training loss = 0.44477617740631104
Epoch 4: Training loss = 0.42227283120155334
Epoch 5: Training loss = 0.4071732461452484
Epoch 6: Training loss = 0.3981558084487915
Epoch 7: Training loss = 0.3908429741859436
Epoch 8: Training loss = 0.382063090801239
Epoch 9: Training loss = 0.3755987882614136
Epoch 10: Training loss = 0.36849305033683777
Epoch 11: Training loss = 0.36106377840042114
Epoch 12: Training loss = 0.35695919394493103
Epoch 13: Training loss = 0.3536606729030609
Epoch 14: Training loss = 0.3468775153160095
Epoch 15: Training loss = 0.3465577960014343
Epoch 16: Training loss = 0.3398754894733429
Epoch 17: Training loss = 0.33407482504844666
Epoch 18: Training loss = 0.32991138100624084
Epoch 19: Training loss = 0.32628387212753296
1/1 [==============================] - 0s 70ms/step
[[0.00894439]
 [0.02034382]
 [0.039757  ]
 [0.59606683]
 [0.9932899 ]
 [0.65586215]
 [0.01728731]
 [0.42491615]
 [0.8481288 ]
 [0.9671011 ]
 [0.23881067]
 [0.9719059 ]
 [0.81090266]
 [0.7779865 ]
 [0.8935956 ]
 [0.4538333 ]
 [0.7718073 ]
 [0.2099615 ]
 [0.7781818 ]
 [0.99460137]]
e: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
24985
781/781 [==============================] - 2s 1ms/step
0.15721432859715834
NN_longresnetwithbowadd:
Epoch 1/30
625/625 [==============================] - 4s 5ms/step - loss: 0.6914 - accuracy: 0.5599 - val_loss: 0.6545 - val_accuracy: 0.6172
Epoch 2/30
625/625 [==============================] - 3s 4ms/step - loss: 0.5852 - accuracy: 0.6959 - val_loss: 0.5074 - val_accuracy: 0.7691
Epoch 3/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4819 - accuracy: 0.7805 - val_loss: 0.4508 - val_accuracy: 0.8031
Epoch 4/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4436 - accuracy: 0.8050 - val_loss: 0.4291 - val_accuracy: 0.8149
Epoch 5/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4214 - accuracy: 0.8170 - val_loss: 0.4195 - val_accuracy: 0.8195
Epoch 6/30
625/625 [==============================] - 3s 4ms/step - loss: 0.4107 - accuracy: 0.8225 - val_loss: 0.4112 - val_accuracy: 0.8227
Epoch 7/30
625/625 [==============================] - 3s 5ms/step - loss: 0.4003 - accuracy: 0.8276 - val_loss: 0.4058 - val_accuracy: 0.8245
Epoch 8/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3897 - accuracy: 0.8341 - val_loss: 0.4015 - val_accuracy: 0.8289
Epoch 9/30
625/625 [==============================] - 3s 5ms/step - loss: 0.3817 - accuracy: 0.8391 - val_loss: 0.4026 - val_accuracy: 0.8269
Epoch 10/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3770 - accuracy: 0.8385 - val_loss: 0.3978 - val_accuracy: 0.8317
Epoch 11/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3700 - accuracy: 0.8434 - val_loss: 0.3955 - val_accuracy: 0.8339
Epoch 12/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3684 - accuracy: 0.8440 - val_loss: 0.3941 - val_accuracy: 0.8339
Epoch 13/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3604 - accuracy: 0.8467 - val_loss: 0.3929 - val_accuracy: 0.8357
Epoch 14/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3552 - accuracy: 0.8488 - val_loss: 0.3924 - val_accuracy: 0.8353
Epoch 15/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3497 - accuracy: 0.8535 - val_loss: 0.3914 - val_accuracy: 0.8327
Epoch 16/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3418 - accuracy: 0.8543 - val_loss: 0.3920 - val_accuracy: 0.8353
Epoch 17/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3385 - accuracy: 0.8576 - val_loss: 0.3911 - val_accuracy: 0.8363
Epoch 18/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3335 - accuracy: 0.8593 - val_loss: 0.3897 - val_accuracy: 0.8357
Epoch 19/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3295 - accuracy: 0.8627 - val_loss: 0.3888 - val_accuracy: 0.8351
Epoch 20/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3261 - accuracy: 0.8615 - val_loss: 0.3886 - val_accuracy: 0.8371
Epoch 21/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3231 - accuracy: 0.8615 - val_loss: 0.3907 - val_accuracy: 0.8347
Epoch 22/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3176 - accuracy: 0.8629 - val_loss: 0.3900 - val_accuracy: 0.8363
Epoch 23/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3159 - accuracy: 0.8665 - val_loss: 0.3953 - val_accuracy: 0.8371
Epoch 24/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3085 - accuracy: 0.8699 - val_loss: 0.3913 - val_accuracy: 0.8377
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 500)]        0           []

 dense (Dense)                  (None, 500)          250500      ['input_1[0][0]']

 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']

 dense_1 (Dense)                (None, 400)          200400      ['dropout[0][0]']

 dense_2 (Dense)                (None, 300)          120300      ['dense_1[0][0]']

 dense_3 (Dense)                (None, 200)          60200       ['dense_2[0][0]']

 dense_4 (Dense)                (None, 500)          100500      ['dense_3[0][0]']

 add (Add)                      (None, 500)          0           ['input_1[0][0]',
                                                                  'dense_4[0][0]']

 dense_5 (Dense)                (None, 1)            501         ['add[0][0]']

==================================================================================================
Total params: 732,401
Trainable params: 732,401
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 0: Training loss = 0.6913565397262573
Epoch 1: Training loss = 0.5852197408676147
Epoch 2: Training loss = 0.4818977117538452
Epoch 3: Training loss = 0.4435579776763916
Epoch 4: Training loss = 0.4214293956756592
Epoch 5: Training loss = 0.4106610119342804
Epoch 6: Training loss = 0.40034663677215576
Epoch 7: Training loss = 0.38968849182128906
Epoch 8: Training loss = 0.38169869780540466
Epoch 9: Training loss = 0.37700894474983215
Epoch 10: Training loss = 0.36996889114379883
Epoch 11: Training loss = 0.3684077858924866
Epoch 12: Training loss = 0.36039748787879944
Epoch 13: Training loss = 0.3552013039588928
Epoch 14: Training loss = 0.3497328460216522
Epoch 15: Training loss = 0.34180018305778503
Epoch 16: Training loss = 0.33848342299461365
Epoch 17: Training loss = 0.33354559540748596
Epoch 18: Training loss = 0.3294698894023895
Epoch 19: Training loss = 0.32611575722694397
Epoch 20: Training loss = 0.3230940103530884
Epoch 21: Training loss = 0.31758156418800354
Epoch 22: Training loss = 0.3158566951751709
Epoch 23: Training loss = 0.308450311422348
24985
781/781 [==============================] - 2s 2ms/step
0.15777466479887936
NN_shortresnetwithbowadd:
Epoch 1/30
625/625 [==============================] - 3s 4ms/step - loss: 0.7036 - accuracy: 0.5515 - val_loss: 0.6451 - val_accuracy: 0.6348
Epoch 2/30
625/625 [==============================] - 2s 4ms/step - loss: 0.6093 - accuracy: 0.6730 - val_loss: 0.5651 - val_accuracy: 0.7314
Epoch 3/30
625/625 [==============================] - 2s 4ms/step - loss: 0.5368 - accuracy: 0.7462 - val_loss: 0.5050 - val_accuracy: 0.7759
Epoch 4/30
625/625 [==============================] - 2s 4ms/step - loss: 0.4866 - accuracy: 0.7816 - val_loss: 0.4642 - val_accuracy: 0.8013
Epoch 5/30
625/625 [==============================] - 2s 4ms/step - loss: 0.4516 - accuracy: 0.7998 - val_loss: 0.4389 - val_accuracy: 0.8153
Epoch 6/30
625/625 [==============================] - 2s 4ms/step - loss: 0.4280 - accuracy: 0.8148 - val_loss: 0.4227 - val_accuracy: 0.8227
Epoch 7/30
625/625 [==============================] - 2s 4ms/step - loss: 0.4103 - accuracy: 0.8241 - val_loss: 0.4122 - val_accuracy: 0.8279
Epoch 8/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3976 - accuracy: 0.8324 - val_loss: 0.4046 - val_accuracy: 0.8321
Epoch 9/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3907 - accuracy: 0.8319 - val_loss: 0.4007 - val_accuracy: 0.8327
Epoch 10/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3847 - accuracy: 0.8360 - val_loss: 0.3956 - val_accuracy: 0.8361
Epoch 11/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3766 - accuracy: 0.8395 - val_loss: 0.3923 - val_accuracy: 0.8379
Epoch 12/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3718 - accuracy: 0.8428 - val_loss: 0.3919 - val_accuracy: 0.8377
Epoch 13/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3660 - accuracy: 0.8447 - val_loss: 0.3889 - val_accuracy: 0.8407
Epoch 14/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3636 - accuracy: 0.8461 - val_loss: 0.3874 - val_accuracy: 0.8421
Epoch 15/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3562 - accuracy: 0.8496 - val_loss: 0.3857 - val_accuracy: 0.8437
Epoch 16/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3552 - accuracy: 0.8510 - val_loss: 0.3859 - val_accuracy: 0.8405
Epoch 17/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3495 - accuracy: 0.8515 - val_loss: 0.3846 - val_accuracy: 0.8449
Epoch 18/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3476 - accuracy: 0.8550 - val_loss: 0.3839 - val_accuracy: 0.8445
Epoch 19/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3440 - accuracy: 0.8544 - val_loss: 0.3832 - val_accuracy: 0.8439
Epoch 20/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3375 - accuracy: 0.8570 - val_loss: 0.3829 - val_accuracy: 0.8439
Epoch 21/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3368 - accuracy: 0.8597 - val_loss: 0.3830 - val_accuracy: 0.8437
Epoch 22/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3353 - accuracy: 0.8590 - val_loss: 0.3830 - val_accuracy: 0.8435
Epoch 23/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3323 - accuracy: 0.8621 - val_loss: 0.3833 - val_accuracy: 0.8445
Epoch 24/30
625/625 [==============================] - 2s 4ms/step - loss: 0.3278 - accuracy: 0.8620 - val_loss: 0.3831 - val_accuracy: 0.8433
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 500)]        0           []

 dense (Dense)                  (None, 500)          250500      ['input_1[0][0]']

 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']

 dense_1 (Dense)                (None, 500)          250500      ['dropout[0][0]']

 add (Add)                      (None, 500)          0           ['input_1[0][0]',
                                                                  'dense_1[0][0]']

 dense_2 (Dense)                (None, 1)            501         ['add[0][0]']

==================================================================================================
Total params: 501,501
Trainable params: 501,501
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 0: Training loss = 0.703596830368042
Epoch 1: Training loss = 0.6092506051063538
Epoch 2: Training loss = 0.5367544293403625
Epoch 3: Training loss = 0.48662039637565613
Epoch 4: Training loss = 0.45160701870918274
Epoch 5: Training loss = 0.4279855489730835
Epoch 6: Training loss = 0.41033899784088135
Epoch 7: Training loss = 0.397553026676178
Epoch 8: Training loss = 0.3906552791595459
Epoch 9: Training loss = 0.384653776884079
Epoch 10: Training loss = 0.3766118586063385
Epoch 11: Training loss = 0.371839702129364
Epoch 12: Training loss = 0.3660103678703308
Epoch 13: Training loss = 0.3635789752006531
Epoch 14: Training loss = 0.3562251925468445
Epoch 15: Training loss = 0.3552405536174774
Epoch 16: Training loss = 0.3495405614376068
Epoch 17: Training loss = 0.34759053587913513
Epoch 18: Training loss = 0.34396079182624817
Epoch 19: Training loss = 0.3374522626399994
Epoch 20: Training loss = 0.33679503202438354
Epoch 21: Training loss = 0.3352566063404083
Epoch 22: Training loss = 0.33228954672813416
Epoch 23: Training loss = 0.32781127095222473
e: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
24985
781/781 [==============================] - 1s 1ms/step
0.15357214328597157
logreg:
2023-03-16 21:03:15.456930: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-16 21:03:16.092204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
[0 0 0 1 1 0 0 0 1 1 0 1 1 1 1 0 1 0 1 1]
Training error rate: 0.14661383285302598
Validation error rate: 0.15153091855113066
decision tree maxdepth:30 minleaf:30
2023-03-16 21:16:44.985255: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
Training error rate: 0.21265609990393852
Validation error rate: 0.25571342805683406
GBDT
Model: "gradient_boosted_trees_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
=================================================================
Total params: 1
Trainable params: 0
Non-trainable params: 1
_________________________________________________________________
Type: "GRADIENT_BOOSTED_TREES"
Task: CLASSIFICATION
Label: "__LABEL"

Input Features (500):
	data:0.0
	data:0.1
	data:0.10
	data:0.100
	data:0.101
	data:0.102
	data:0.103
	data:0.104
	data:0.105
	data:0.106
	data:0.107
	data:0.108
	data:0.109
	data:0.11
	data:0.110
	data:0.111
	data:0.112
	data:0.113
	data:0.114
	data:0.115
	data:0.116
	data:0.117
	data:0.118
	data:0.119
	data:0.12
	data:0.120
	data:0.121
	data:0.122
	data:0.123
	data:0.124
	data:0.125
	data:0.126
	data:0.127
	data:0.128
	data:0.129
	data:0.13
	data:0.130
	data:0.131
	data:0.132
	data:0.133
	data:0.134
	data:0.135
	data:0.136
	data:0.137
	data:0.138
	data:0.139
	data:0.14
	data:0.140
	data:0.141
	data:0.142
	data:0.143
	data:0.144
	data:0.145
	data:0.146
	data:0.147
	data:0.148
	data:0.149
	data:0.15
	data:0.150
	data:0.151
	data:0.152
	data:0.153
	data:0.154
	data:0.155
	data:0.156
	data:0.157
	data:0.158
	data:0.159
	data:0.16
	data:0.160
	data:0.161
	data:0.162
	data:0.163
	data:0.164
	data:0.165
	data:0.166
	data:0.167
	data:0.168
	data:0.169
	data:0.17
	data:0.170
	data:0.171
	data:0.172
	data:0.173
	data:0.174
	data:0.175
	data:0.176
	data:0.177
	data:0.178
	data:0.179
	data:0.18
	data:0.180
	data:0.181
	data:0.182
	data:0.183
	data:0.184
	data:0.185
	data:0.186
	data:0.187
	data:0.188
	data:0.189
	data:0.19
	data:0.190
	data:0.191
	data:0.192
	data:0.193
	data:0.194
	data:0.195
	data:0.196
	data:0.197
	data:0.198
	data:0.199
	data:0.2
	data:0.20
	data:0.200
	data:0.201
	data:0.202
	data:0.203
	data:0.204
	data:0.205
	data:0.206
	data:0.207
	data:0.208
	data:0.209
	data:0.21
	data:0.210
	data:0.211
	data:0.212
	data:0.213
	data:0.214
	data:0.215
	data:0.216
	data:0.217
	data:0.218
	data:0.219
	data:0.22
	data:0.220
	data:0.221
	data:0.222
	data:0.223
	data:0.224
	data:0.225
	data:0.226
	data:0.227
	data:0.228
	data:0.229
	data:0.23
	data:0.230
	data:0.231
	data:0.232
	data:0.233
	data:0.234
	data:0.235
	data:0.236
	data:0.237
	data:0.238
	data:0.239
	data:0.24
	data:0.240
	data:0.241
	data:0.242
	data:0.243
	data:0.244
	data:0.245
	data:0.246
	data:0.247
	data:0.248
	data:0.249
	data:0.25
	data:0.250
	data:0.251
	data:0.252
	data:0.253
	data:0.254
	data:0.255
	data:0.256
	data:0.257
	data:0.258
	data:0.259
	data:0.26
	data:0.260
	data:0.261
	data:0.262
	data:0.263
	data:0.264
	data:0.265
	data:0.266
	data:0.267
	data:0.268
	data:0.269
	data:0.27
	data:0.270
	data:0.271
	data:0.272
	data:0.273
	data:0.274
	data:0.275
	data:0.276
	data:0.277
	data:0.278
	data:0.279
	data:0.28
	data:0.280
	data:0.281
	data:0.282
	data:0.283
	data:0.284
	data:0.285
	data:0.286
	data:0.287
	data:0.288
	data:0.289
	data:0.29
	data:0.290
	data:0.291
	data:0.292
	data:0.293
	data:0.294
	data:0.295
	data:0.296
	data:0.297
	data:0.298
	data:0.299
	data:0.3
	data:0.30
	data:0.300
	data:0.301
	data:0.302
	data:0.303
	data:0.304
	data:0.305
	data:0.306
	data:0.307
	data:0.308
	data:0.309
	data:0.31
	data:0.310
	data:0.311
	data:0.312
	data:0.313
	data:0.314
	data:0.315
	data:0.316
	data:0.317
	data:0.318
	data:0.319
	data:0.32
	data:0.320
	data:0.321
	data:0.322
	data:0.323
	data:0.324
	data:0.325
	data:0.326
	data:0.327
	data:0.328
	data:0.329
	data:0.33
	data:0.330
	data:0.331
	data:0.332
	data:0.333
	data:0.334
	data:0.335
	data:0.336
	data:0.337
	data:0.338
	data:0.339
	data:0.34
	data:0.340
	data:0.341
	data:0.342
	data:0.343
	data:0.344
	data:0.345
	data:0.346
	data:0.347
	data:0.348
	data:0.349
	data:0.35
	data:0.350
	data:0.351
	data:0.352
	data:0.353
	data:0.354
	data:0.355
	data:0.356
	data:0.357
	data:0.358
	data:0.359
	data:0.36
	data:0.360
	data:0.361
	data:0.362
	data:0.363
	data:0.364
	data:0.365
	data:0.366
	data:0.367
	data:0.368
	data:0.369
	data:0.37
	data:0.370
	data:0.371
	data:0.372
	data:0.373
	data:0.374
	data:0.375
	data:0.376
	data:0.377
	data:0.378
	data:0.379
	data:0.38
	data:0.380
	data:0.381
	data:0.382
	data:0.383
	data:0.384
	data:0.385
	data:0.386
	data:0.387
	data:0.388
	data:0.389
	data:0.39
	data:0.390
	data:0.391
	data:0.392
	data:0.393
	data:0.394
	data:0.395
	data:0.396
	data:0.397
	data:0.398
	data:0.399
	data:0.4
	data:0.40
	data:0.400
	data:0.401
	data:0.402
	data:0.403
	data:0.404
	data:0.405
	data:0.406
	data:0.407
	data:0.408
	data:0.409
	data:0.41
	data:0.410
	data:0.411
	data:0.412
	data:0.413
	data:0.414
	data:0.415
	data:0.416
	data:0.417
	data:0.418
	data:0.419
	data:0.42
	data:0.420
	data:0.421
	data:0.422
	data:0.423
	data:0.424
	data:0.425
	data:0.426
	data:0.427
	data:0.428
	data:0.429
	data:0.43
	data:0.430
	data:0.431
	data:0.432
	data:0.433
	data:0.434
	data:0.435
	data:0.436
	data:0.437
	data:0.438
	data:0.439
	data:0.44
	data:0.440
	data:0.441
	data:0.442
	data:0.443
	data:0.444
	data:0.445
	data:0.446
	data:0.447
	data:0.448
	data:0.449
	data:0.45
	data:0.450
	data:0.451
	data:0.452
	data:0.453
	data:0.454
	data:0.455
	data:0.456
	data:0.457
	data:0.458
	data:0.459
	data:0.46
	data:0.460
	data:0.461
	data:0.462
	data:0.463
	data:0.464
	data:0.465
	data:0.466
	data:0.467
	data:0.468
	data:0.469
	data:0.47
	data:0.470
	data:0.471
	data:0.472
	data:0.473
	data:0.474
	data:0.475
	data:0.476
	data:0.477
	data:0.478
	data:0.479
	data:0.48
	data:0.480
	data:0.481
	data:0.482
	data:0.483
	data:0.484
	data:0.485
	data:0.486
	data:0.487
	data:0.488
	data:0.489
	data:0.49
	data:0.490
	data:0.491
	data:0.492
	data:0.493
	data:0.494
	data:0.495
	data:0.496
	data:0.497
	data:0.498
	data:0.499
	data:0.5
	data:0.50
	data:0.51
	data:0.52
	data:0.53
	data:0.54
	data:0.55
	data:0.56
	data:0.57
	data:0.58
	data:0.59
	data:0.6
	data:0.60
	data:0.61
	data:0.62
	data:0.63
	data:0.64
	data:0.65
	data:0.66
	data:0.67
	data:0.68
	data:0.69
	data:0.7
	data:0.70
	data:0.71
	data:0.72
	data:0.73
	data:0.74
	data:0.75
	data:0.76
	data:0.77
	data:0.78
	data:0.79
	data:0.8
	data:0.80
	data:0.81
	data:0.82
	data:0.83
	data:0.84
	data:0.85
	data:0.86
	data:0.87
	data:0.88
	data:0.89
	data:0.9
	data:0.90
	data:0.91
	data:0.92
	data:0.93
	data:0.94
	data:0.95
	data:0.96
	data:0.97
	data:0.98
	data:0.99

No weights

Variable Importance: INV_MEAN_MIN_DEPTH:
    1.  "data:0.33"  0.147958 ################
    2. "data:0.183"  0.142765 ##########
    3. "data:0.488"  0.141592 #########
    4.  "data:0.41"  0.140086 ########
    5. "data:0.137"  0.139499 #######
    6. "data:0.471"  0.139261 #######
    7.  "data:0.36"  0.139245 #######
    8. "data:0.258"  0.139004 #######
    9. "data:0.474"  0.138812 ######
   10.  "data:0.30"  0.138682 ######
   11. "data:0.317"  0.138429 ######
   12. "data:0.151"  0.138342 ######
   13.  "data:0.49"  0.137954 #####
   14. "data:0.298"  0.137390 #####
   15. "data:0.145"  0.137311 #####
   16. "data:0.122"  0.137114 #####
   17. "data:0.318"  0.137113 #####
   18. "data:0.277"  0.137047 #####
   19.  "data:0.15"  0.137030 #####
   20.  "data:0.12"  0.136713 ####
   21. "data:0.419"  0.136702 ####
   22. "data:0.248"  0.136652 ####
   23.  "data:0.14"  0.136573 ####
   24. "data:0.423"  0.136552 ####
   25. "data:0.413"  0.136297 ####
   26. "data:0.375"  0.136257 ####
   27.  "data:0.18"  0.136178 ####
   28. "data:0.330"  0.136120 ####
   29. "data:0.303"  0.136105 ####
   30. "data:0.108"  0.135871 ###
   31. "data:0.487"  0.135696 ###
   32. "data:0.283"  0.135621 ###
   33. "data:0.430"  0.135420 ###
   34. "data:0.441"  0.135388 ###
   35. "data:0.368"  0.135361 ###
   36. "data:0.206"  0.135329 ###
   37. "data:0.448"  0.135303 ###
   38. "data:0.199"  0.135287 ###
   39. "data:0.168"  0.135245 ###
   40. "data:0.457"  0.135175 ###
   41.  "data:0.99"  0.135065 ###
   42. "data:0.242"  0.135064 ###
   43. "data:0.129"  0.135040 ###
   44. "data:0.327"  0.135022 ###
   45. "data:0.255"  0.134955 ##
   46. "data:0.220"  0.134955 ##
   47. "data:0.379"  0.134917 ##
   48. "data:0.439"  0.134906 ##
   49. "data:0.234"  0.134899 ##
   50.  "data:0.52"  0.134873 ##
   51. "data:0.425"  0.134844 ##
   52. "data:0.361"  0.134829 ##
   53. "data:0.195"  0.134826 ##
   54. "data:0.224"  0.134782 ##
   55. "data:0.446"  0.134665 ##
   56. "data:0.353"  0.134661 ##
   57.  "data:0.28"  0.134633 ##
   58.  "data:0.82"  0.134630 ##
   59. "data:0.181"  0.134481 ##
   60. "data:0.486"  0.134437 ##
   61. "data:0.127"  0.134384 ##
   62. "data:0.417"  0.134191 ##
   63. "data:0.334"  0.134187 ##
   64. "data:0.351"  0.134080 ##
   65. "data:0.490"  0.134042 ##
   66. "data:0.449"  0.133956 #
   67. "data:0.399"  0.133948 #
   68. "data:0.496"  0.133934 #
   69. "data:0.185"  0.133921 #
   70. "data:0.380"  0.133913 #
   71. "data:0.483"  0.133817 #
   72.   "data:0.2"  0.133793 #
   73. "data:0.280"  0.133770 #
   74.  "data:0.21"  0.133759 #
   75. "data:0.415"  0.133755 #
   76. "data:0.305"  0.133738 #
   77.  "data:0.42"  0.133723 #
   78. "data:0.124"  0.133678 #
   79. "data:0.378"  0.133676 #
   80. "data:0.333"  0.133661 #
   81. "data:0.157"  0.133660 #
   82. "data:0.308"  0.133539 #
   83.  "data:0.97"  0.133484 #
   84. "data:0.214"  0.133479 #
   85. "data:0.105"  0.133479 #
   86. "data:0.266"  0.133467 #
   87. "data:0.187"  0.133349 #
   88. "data:0.282"  0.133347 #
   89.  "data:0.44"  0.133318 #
   90. "data:0.312"  0.133315 #
   91. "data:0.147"  0.133310 #
   92. "data:0.296"  0.133309 #
   93. "data:0.250"  0.133289 #
   94. "data:0.207"  0.133284 #
   95. "data:0.489"  0.133253 #
   96. "data:0.259"  0.133231 #
   97.   "data:0.3"  0.133215 #
   98. "data:0.268"  0.133202 #
   99.  "data:0.72"  0.133172 #
  100. "data:0.498"  0.133123 #
  101. "data:0.253"  0.133069 #
  102. "data:0.200"  0.133050 #
  103. "data:0.120"  0.133043 #
  104. "data:0.285"  0.133039 #
  105. "data:0.325"  0.133028
  106. "data:0.468"  0.133010
  107. "data:0.226"  0.133008
  108. "data:0.159"  0.133004
  109. "data:0.433"  0.132993
  110. "data:0.117"  0.132982
  111. "data:0.113"  0.132968
  112. "data:0.384"  0.132918
  113. "data:0.469"  0.132915
  114. "data:0.494"  0.132901
  115. "data:0.356"  0.132889
  116. "data:0.275"  0.132885
  117. "data:0.211"  0.132882
  118. "data:0.364"  0.132878
  119. "data:0.123"  0.132867
  120. "data:0.264"  0.132851
  121. "data:0.174"  0.132835
  122. "data:0.131"  0.132834
  123. "data:0.398"  0.132829
  124.  "data:0.89"  0.132801
  125.  "data:0.23"  0.132773
  126. "data:0.350"  0.132773
  127. "data:0.126"  0.132769
  128. "data:0.401"  0.132763
  129. "data:0.371"  0.132761
  130.  "data:0.93"  0.132745
  131. "data:0.472"  0.132743
  132. "data:0.391"  0.132736
  133. "data:0.178"  0.132730
  134. "data:0.400"  0.132725
  135. "data:0.470"  0.132723
  136. "data:0.161"  0.132706
  137. "data:0.263"  0.132689
  138. "data:0.473"  0.132683
  139. "data:0.426"  0.132679
  140. "data:0.370"  0.132649
  141.  "data:0.66"  0.132645
  142. "data:0.387"  0.132644
  143. "data:0.112"  0.132642
  144. "data:0.434"  0.132635
  145. "data:0.485"  0.132629
  146. "data:0.369"  0.132628
  147.  "data:0.67"  0.132624
  148. "data:0.216"  0.132614
  149. "data:0.121"  0.132612
  150. "data:0.463"  0.132607
  151. "data:0.316"  0.132604
  152. "data:0.435"  0.132596
  153. "data:0.260"  0.132593
  154. "data:0.293"  0.132584
  155. "data:0.354"  0.132564
  156. "data:0.107"  0.132563
  157. "data:0.344"  0.132563
  158. "data:0.491"  0.132562
  159.  "data:0.55"  0.132548
  160. "data:0.153"  0.132547
  161.  "data:0.79"  0.132546
  162. "data:0.269"  0.132537
  163. "data:0.246"  0.132534
  164. "data:0.445"  0.132533
  165. "data:0.252"  0.132533
  166. "data:0.179"  0.132519
  167. "data:0.482"  0.132515
  168. "data:0.186"  0.132515
  169. "data:0.357"  0.132511
  170. "data:0.294"  0.132511
  171. "data:0.389"  0.132504
  172. "data:0.238"  0.132498
  173. "data:0.138"  0.132485
  174. "data:0.339"  0.132471
  175.   "data:0.4"  0.132470
  176. "data:0.450"  0.132467
  177. "data:0.232"  0.132464
  178. "data:0.499"  0.132459
  179. "data:0.458"  0.132458
  180. "data:0.338"  0.132457
  181. "data:0.480"  0.132457
  182. "data:0.411"  0.132453
  183. "data:0.362"  0.132451
  184.  "data:0.88"  0.132449
  185. "data:0.244"  0.132447
  186. "data:0.221"  0.132431
  187. "data:0.150"  0.132431
  188. "data:0.144"  0.132430
  189.  "data:0.56"  0.132426
  190. "data:0.257"  0.132417
  191. "data:0.132"  0.132414
  192. "data:0.454"  0.132411
  193. "data:0.336"  0.132401
  194.  "data:0.85"  0.132401
  195. "data:0.340"  0.132400
  196. "data:0.166"  0.132396
  197. "data:0.227"  0.132387
  198. "data:0.359"  0.132385
  199.  "data:0.63"  0.132383
  200.  "data:0.17"  0.132382
  201. "data:0.299"  0.132382
  202. "data:0.193"  0.132378
  203.  "data:0.40"  0.132376
  204.  "data:0.71"  0.132375
  205. "data:0.237"  0.132373
  206. "data:0.114"  0.132371
  207. "data:0.271"  0.132370
  208.  "data:0.43"  0.132370
  209.  "data:0.86"  0.132366
  210. "data:0.367"  0.132364
  211. "data:0.228"  0.132363
  212. "data:0.251"  0.132353
  213. "data:0.286"  0.132350
  214.  "data:0.74"  0.132343
  215.   "data:0.6"  0.132342
  216. "data:0.119"  0.132340
  217.  "data:0.35"  0.132339
  218. "data:0.239"  0.132331
  219. "data:0.497"  0.132331
  220. "data:0.125"  0.132328
  221. "data:0.111"  0.132327
  222. "data:0.432"  0.132326
  223.  "data:0.37"  0.132325
  224. "data:0.176"  0.132325
  225. "data:0.476"  0.132323
  226. "data:0.229"  0.132320
  227. "data:0.205"  0.132318
  228. "data:0.208"  0.132318
  229. "data:0.405"  0.132311
  230.  "data:0.11"  0.132310
  231. "data:0.274"  0.132305
  232. "data:0.309"  0.132304
  233. "data:0.385"  0.132303
  234. "data:0.328"  0.132300
  235. "data:0.291"  0.132299
  236. "data:0.148"  0.132299
  237. "data:0.139"  0.132299
  238. "data:0.335"  0.132297
  239. "data:0.331"  0.132294
  240. "data:0.447"  0.132290
  241. "data:0.443"  0.132288
  242. "data:0.116"  0.132288
  243.  "data:0.13"  0.132286
  244. "data:0.304"  0.132285
  245. "data:0.212"  0.132280
  246. "data:0.313"  0.132279
  247.  "data:0.47"  0.132277
  248.  "data:0.16"  0.132277
  249. "data:0.478"  0.132277
  250. "data:0.376"  0.132276
  251. "data:0.149"  0.132276
  252. "data:0.154"  0.132275
  253.   "data:0.8"  0.132275
  254. "data:0.360"  0.132273
  255. "data:0.270"  0.132273
  256.  "data:0.48"  0.132270
  257. "data:0.231"  0.132269
  258. "data:0.302"  0.132269
  259. "data:0.440"  0.132268
  260. "data:0.460"  0.132261
  261. "data:0.189"  0.132260
  262. "data:0.345"  0.132259
  263.  "data:0.73"  0.132257
  264. "data:0.492"  0.132254
  265. "data:0.218"  0.132252
  266. "data:0.355"  0.132249
  267. "data:0.444"  0.132248
  268.  "data:0.25"  0.132247
  269.  "data:0.50"  0.132246
  270. "data:0.273"  0.132245
  271. "data:0.287"  0.132244
  272.  "data:0.75"  0.132241
  273. "data:0.143"  0.132240
  274. "data:0.169"  0.132237
  275.  "data:0.65"  0.132236
  276. "data:0.392"  0.132236
  277. "data:0.160"  0.132234
  278. "data:0.172"  0.132234
  279. "data:0.421"  0.132233
  280. "data:0.128"  0.132232
  281. "data:0.225"  0.132231
  282. "data:0.477"  0.132230
  283. "data:0.456"  0.132229
  284.  "data:0.32"  0.132229
  285. "data:0.167"  0.132229
  286. "data:0.329"  0.132229
  287.  "data:0.24"  0.132228
  288. "data:0.410"  0.132228
  289. "data:0.436"  0.132226
  290. "data:0.365"  0.132226
  291. "data:0.347"  0.132226
  292. "data:0.390"  0.132224
  293. "data:0.428"  0.132223
  294. "data:0.320"  0.132222
  295.  "data:0.19"  0.132222
  296. "data:0.424"  0.132220
  297. "data:0.452"  0.132220
  298. "data:0.249"  0.132220
  299. "data:0.307"  0.132217
  300. "data:0.310"  0.132215
  301.  "data:0.31"  0.132214
  302. "data:0.462"  0.132214
  303. "data:0.431"  0.132212
  304. "data:0.109"  0.132211
  305.  "data:0.45"  0.132211
  306. "data:0.396"  0.132211
  307. "data:0.459"  0.132208
  308.  "data:0.29"  0.132207
  309. "data:0.272"  0.132206
  310. "data:0.348"  0.132205
  311. "data:0.241"  0.132205
  312.   "data:0.9"  0.132203
  313. "data:0.204"  0.132203
  314.   "data:0.5"  0.132202
  315.  "data:0.90"  0.132202
  316. "data:0.164"  0.132201
  317. "data:0.429"  0.132200
  318.  "data:0.68"  0.132199
  319. "data:0.311"  0.132198
  320.  "data:0.20"  0.132198
  321. "data:0.133"  0.132197
  322.  "data:0.96"  0.132197
  323. "data:0.341"  0.132197
  324. "data:0.175"  0.132196
  325. "data:0.409"  0.132196
  326. "data:0.173"  0.132195
  327. "data:0.395"  0.132195
  328. "data:0.182"  0.132192
  329. "data:0.290"  0.132191
  330.  "data:0.95"  0.132191
  331. "data:0.495"  0.132191
  332. "data:0.451"  0.132190
  333. "data:0.393"  0.132190
  334. "data:0.198"  0.132189
  335. "data:0.162"  0.132189
  336. "data:0.165"  0.132189
  337. "data:0.416"  0.132189
  338. "data:0.261"  0.132186
  339.  "data:0.64"  0.132186
  340. "data:0.346"  0.132185
  341. "data:0.158"  0.132184
  342. "data:0.337"  0.132184
  343.  "data:0.58"  0.132183
  344. "data:0.461"  0.132183
  345.  "data:0.59"  0.132183
  346. "data:0.136"  0.132183
  347. "data:0.427"  0.132181
  348. "data:0.453"  0.132181
  349. "data:0.209"  0.132181
  350.  "data:0.27"  0.132179
  351. "data:0.104"  0.132178
  352. "data:0.213"  0.132177
  353. "data:0.202"  0.132177
  354. "data:0.210"  0.132176
  355. "data:0.420"  0.132175
  356. "data:0.177"  0.132173
  357. "data:0.306"  0.132173
  358. "data:0.192"  0.132172
  359. "data:0.278"  0.132172
  360. "data:0.481"  0.132172
  361. "data:0.171"  0.132172
  362. "data:0.203"  0.132171
  363.  "data:0.83"  0.132168
  364. "data:0.196"  0.132168
  365.  "data:0.53"  0.132167
  366. "data:0.142"  0.132165
  367.  "data:0.81"  0.132165
  368. "data:0.394"  0.132165
  369. "data:0.288"  0.132165
  370. "data:0.455"  0.132165
  371.  "data:0.70"  0.132164
  372. "data:0.152"  0.132163
  373. "data:0.115"  0.132163
  374. "data:0.422"  0.132163
  375. "data:0.254"  0.132162
  376. "data:0.377"  0.132162
  377. "data:0.184"  0.132162
  378.   "data:0.1"  0.132161
  379. "data:0.358"  0.132159
  380. "data:0.130"  0.132158
  381. "data:0.243"  0.132158
  382. "data:0.146"  0.132156
  383. "data:0.135"  0.132156
  384. "data:0.343"  0.132154
  385. "data:0.101"  0.132153
  386. "data:0.464"  0.132153
  387. "data:0.106"  0.132153
  388. "data:0.295"  0.132149
  389. "data:0.323"  0.132148
  390. "data:0.397"  0.132145
  391. "data:0.279"  0.132144
  392. "data:0.467"  0.132144
  393. "data:0.324"  0.132144
  394.   "data:0.0"  0.132144
  395. "data:0.118"  0.132142
  396. "data:0.289"  0.132142
  397. "data:0.322"  0.132141
  398. "data:0.402"  0.132139
  399. "data:0.230"  0.132138
  400. "data:0.180"  0.132138
  401.  "data:0.54"  0.132137
  402. "data:0.372"  0.132137
  403.  "data:0.98"  0.132137
  404. "data:0.197"  0.132136
  405. "data:0.140"  0.132135
  406. "data:0.300"  0.132135
  407. "data:0.215"  0.132134
  408.  "data:0.10"  0.132134
  409. "data:0.190"  0.132133
  410. "data:0.110"  0.132131
  411. "data:0.292"  0.132131
  412. "data:0.414"  0.132131
  413. "data:0.191"  0.132131
  414. "data:0.321"  0.132130
  415.  "data:0.60"  0.132128
  416. "data:0.373"  0.132127
  417. "data:0.332"  0.132126
  418.  "data:0.38"  0.132125
  419. "data:0.297"  0.132125
  420. "data:0.201"  0.132125
  421. "data:0.155"  0.132125
  422. "data:0.276"  0.132124
  423.  "data:0.94"  0.132121
  424.  "data:0.57"  0.132121
  425. "data:0.103"  0.132121
  426.  "data:0.84"  0.132120
  427. "data:0.134"  0.132120
  428. "data:0.245"  0.132118
  429. "data:0.265"  0.132118
  430. "data:0.262"  0.132116
  431. "data:0.100"  0.132116
  432.  "data:0.39"  0.132116
  433. "data:0.374"  0.132114
  434. "data:0.319"  0.132114
  435. "data:0.381"  0.132114
  436. "data:0.188"  0.132113
  437. "data:0.236"  0.132112
  438. "data:0.222"  0.132112
  439. "data:0.442"  0.132111
  440.  "data:0.22"  0.132110
  441.  "data:0.69"  0.132109
  442. "data:0.240"  0.132108
  443. "data:0.247"  0.132107
  444. "data:0.163"  0.132107
  445. "data:0.404"  0.132106
  446. "data:0.314"  0.132106
  447. "data:0.217"  0.132105
  448. "data:0.493"  0.132105
  449. "data:0.315"  0.132104
  450.  "data:0.91"  0.132103
  451. "data:0.102"  0.132102
  452. "data:0.170"  0.132101
  453. "data:0.382"  0.132100
  454.  "data:0.76"  0.132100
  455.  "data:0.62"  0.132099
  456. "data:0.403"  0.132096
  457. "data:0.386"  0.132095
  458. "data:0.479"  0.132094
  459. "data:0.406"  0.132092
  460. "data:0.465"  0.132091
  461. "data:0.437"  0.132090
  462. "data:0.407"  0.132089
  463. "data:0.256"  0.132089
  464. "data:0.194"  0.132088
  465. "data:0.363"  0.132087
  466. "data:0.156"  0.132086
  467. "data:0.349"  0.132085
  468.  "data:0.77"  0.132085
  469. "data:0.383"  0.132085
  470.  "data:0.46"  0.132084
  471.  "data:0.80"  0.132083
  472.  "data:0.78"  0.132083
  473. "data:0.438"  0.132083
  474. "data:0.326"  0.132082
  475.  "data:0.51"  0.132080
  476. "data:0.484"  0.132079
  477. "data:0.141"  0.132079
  478. "data:0.352"  0.132079
  479. "data:0.388"  0.132078
  480. "data:0.342"  0.132078
  481. "data:0.466"  0.132076
  482. "data:0.418"  0.132074
  483. "data:0.284"  0.132074
  484. "data:0.219"  0.132073
  485. "data:0.475"  0.132071
  486. "data:0.233"  0.132071
  487. "data:0.366"  0.132070
  488. "data:0.235"  0.132069
  489. "data:0.408"  0.132066
  490. "data:0.223"  0.132065
  491.  "data:0.87"  0.132064
  492.  "data:0.26"  0.132064
  493.  "data:0.34"  0.132060
  494. "data:0.301"  0.132060
  495. "data:0.412"  0.132059
  496. "data:0.267"  0.132058
  497. "data:0.281"  0.132057
  498.  "data:0.61"  0.132048
  499.  "data:0.92"  0.132046
  500.   "data:0.7"  0.132036

Variable Importance: NUM_AS_ROOT:
    1.  "data:0.33" 71.000000 ################
    2. "data:0.488" 36.000000 ########
    3. "data:0.183" 28.000000 ######
    4.  "data:0.41" 28.000000 ######
    5. "data:0.151" 26.000000 #####
    6. "data:0.258" 25.000000 #####
    7.  "data:0.36" 25.000000 #####
    8.  "data:0.30" 24.000000 #####
    9. "data:0.137" 23.000000 #####
   10. "data:0.474" 21.000000 ####
   11.  "data:0.15" 20.000000 ####
   12. "data:0.318" 20.000000 ####
   13.  "data:0.49" 20.000000 ####
   14.  "data:0.14" 19.000000 ####
   15. "data:0.145" 18.000000 ###
   16. "data:0.317" 18.000000 ###
   17. "data:0.471" 18.000000 ###
   18. "data:0.122" 16.000000 ###
   19. "data:0.298" 16.000000 ###
   20. "data:0.277" 15.000000 ###
   21. "data:0.413" 15.000000 ###
   22. "data:0.419" 15.000000 ###
   23.  "data:0.18" 14.000000 ##
   24. "data:0.423" 14.000000 ##
   25. "data:0.248" 13.000000 ##
   26.  "data:0.12" 12.000000 ##
   27. "data:0.303" 12.000000 ##
   28. "data:0.199" 11.000000 ##
   29. "data:0.206" 11.000000 ##
   30.  "data:0.99" 11.000000 ##
   31. "data:0.195" 10.000000 ##
   32. "data:0.220" 10.000000 ##
   33. "data:0.330" 10.000000 ##
   34. "data:0.368" 10.000000 ##
   35. "data:0.425" 10.000000 ##
   36. "data:0.446" 10.000000 ##
   37. "data:0.448" 10.000000 ##
   38. "data:0.224"  9.000000 #
   39.  "data:0.28"  9.000000 #
   40. "data:0.327"  9.000000 #
   41. "data:0.441"  9.000000 #
   42. "data:0.487"  9.000000 #
   43.  "data:0.52"  9.000000 #
   44. "data:0.255"  8.000000 #
   45. "data:0.375"  8.000000 #
   46. "data:0.168"  7.000000 #
   47. "data:0.234"  7.000000 #
   48. "data:0.334"  7.000000 #
   49. "data:0.353"  7.000000 #
   50. "data:0.361"  7.000000 #
   51. "data:0.399"  7.000000 #
   52. "data:0.417"  7.000000 #
   53.  "data:0.82"  7.000000 #
   54. "data:0.108"  6.000000 #
   55. "data:0.127"  6.000000 #
   56.  "data:0.21"  6.000000 #
   57. "data:0.282"  6.000000 #
   58. "data:0.312"  6.000000 #
   59. "data:0.379"  6.000000 #
   60. "data:0.449"  6.000000 #
   61. "data:0.147"  5.000000
   62. "data:0.185"  5.000000
   63. "data:0.242"  5.000000
   64. "data:0.351"  5.000000
   65. "data:0.378"  5.000000
   66. "data:0.430"  5.000000
   67. "data:0.457"  5.000000
   68. "data:0.496"  5.000000
   69. "data:0.124"  4.000000
   70. "data:0.333"  4.000000
   71. "data:0.380"  4.000000
   72. "data:0.483"  4.000000
   73. "data:0.486"  4.000000
   74. "data:0.117"  3.000000
   75. "data:0.181"  3.000000
   76. "data:0.280"  3.000000
   77. "data:0.401"  3.000000
   78. "data:0.415"  3.000000
   79. "data:0.439"  3.000000
   80.  "data:0.44"  3.000000
   81. "data:0.468"  3.000000
   82. "data:0.498"  3.000000
   83.  "data:0.72"  3.000000
   84. "data:0.105"  2.000000
   85. "data:0.113"  2.000000
   86. "data:0.120"  2.000000
   87. "data:0.123"  2.000000
   88.   "data:0.2"  2.000000
   89. "data:0.200"  2.000000
   90. "data:0.207"  2.000000
   91. "data:0.211"  2.000000
   92. "data:0.214"  2.000000
   93. "data:0.259"  2.000000
   94. "data:0.283"  2.000000
   95.   "data:0.3"  2.000000
   96. "data:0.356"  2.000000
   97. "data:0.357"  2.000000
   98. "data:0.384"  2.000000
   99.  "data:0.42"  2.000000
  100. "data:0.469"  2.000000
  101. "data:0.489"  2.000000
  102.  "data:0.93"  2.000000
  103. "data:0.159"  1.000000
  104. "data:0.187"  1.000000
  105. "data:0.216"  1.000000
  106. "data:0.226"  1.000000
  107. "data:0.237"  1.000000
  108. "data:0.296"  1.000000
  109. "data:0.367"  1.000000
  110. "data:0.369"  1.000000
  111. "data:0.387"  1.000000
  112. "data:0.398"  1.000000
  113. "data:0.450"  1.000000
  114.  "data:0.47"  1.000000
  115. "data:0.490"  1.000000
  116. "data:0.491"  1.000000
  117. "data:0.494"  1.000000
  118. "data:0.499"  1.000000
  119.  "data:0.55"  1.000000
  120.  "data:0.89"  1.000000
  121.  "data:0.97"  1.000000

Variable Importance: NUM_NODES:
    1. "data:0.283" 1265.000000 ################
    2.  "data:0.33" 1184.000000 ##############
    3. "data:0.157" 1060.000000 #############
    4. "data:0.183" 948.000000 ###########
    5. "data:0.305" 934.000000 ###########
    6. "data:0.129" 814.000000 ##########
    7. "data:0.474" 800.000000 ##########
    8. "data:0.439" 746.000000 #########
    9. "data:0.258" 728.000000 #########
   10. "data:0.250" 675.000000 ########
   11.  "data:0.12" 650.000000 ########
   12. "data:0.181" 626.000000 #######
   13.  "data:0.41" 620.000000 #######
   14. "data:0.488" 593.000000 #######
   15. "data:0.490" 554.000000 ######
   16. "data:0.415" 552.000000 ######
   17. "data:0.248" 549.000000 ######
   18. "data:0.255" 540.000000 ######
   19. "data:0.263" 515.000000 ######
   20. "data:0.298" 506.000000 ######
   21. "data:0.378" 496.000000 ######
   22. "data:0.122" 489.000000 ######
   23.  "data:0.67" 485.000000 ######
   24. "data:0.174" 475.000000 #####
   25.   "data:0.2" 453.000000 #####
   26.  "data:0.82" 445.000000 #####
   27.  "data:0.36" 443.000000 #####
   28. "data:0.379" 443.000000 #####
   29. "data:0.137" 442.000000 #####
   30. "data:0.413" 442.000000 #####
   31. "data:0.318" 432.000000 #####
   32. "data:0.350" 430.000000 #####
   33. "data:0.448" 415.000000 #####
   34. "data:0.285" 406.000000 ####
   35. "data:0.472" 406.000000 ####
   36. "data:0.471" 397.000000 ####
   37. "data:0.266" 394.000000 ####
   38. "data:0.317" 382.000000 ####
   39.  "data:0.30" 377.000000 ####
   40. "data:0.108" 376.000000 ####
   41. "data:0.433" 370.000000 ####
   42. "data:0.327" 368.000000 ####
   43. "data:0.430" 367.000000 ####
   44. "data:0.325" 364.000000 ####
   45. "data:0.178" 355.000000 ####
   46. "data:0.375" 354.000000 ####
   47. "data:0.473" 353.000000 ####
   48. "data:0.371" 349.000000 ####
   49.  "data:0.49" 348.000000 ####
   50. "data:0.419" 346.000000 ####
   51. "data:0.121" 343.000000 ####
   52. "data:0.330" 342.000000 ####
   53. "data:0.486" 340.000000 ####
   54. "data:0.277" 338.000000 ####
   55.  "data:0.14" 331.000000 ####
   56. "data:0.434" 330.000000 ####
   57. "data:0.260" 327.000000 ###
   58. "data:0.145" 326.000000 ###
   59. "data:0.380" 323.000000 ###
   60. "data:0.496" 321.000000 ###
   61. "data:0.316" 319.000000 ###
   62. "data:0.161" 316.000000 ###
   63. "data:0.426" 310.000000 ###
   64. "data:0.391" 303.000000 ###
   65. "data:0.441" 303.000000 ###
   66. "data:0.131" 298.000000 ###
   67.  "data:0.18" 293.000000 ###
   68. "data:0.159" 292.000000 ###
   69. "data:0.308" 292.000000 ###
   70. "data:0.370" 290.000000 ###
   71. "data:0.423" 290.000000 ###
   72. "data:0.151" 289.000000 ###
   73. "data:0.206" 289.000000 ###
   74. "data:0.232" 288.000000 ###
   75.  "data:0.42" 281.000000 ###
   76. "data:0.457" 280.000000 ###
   77. "data:0.242" 276.000000 ###
   78. "data:0.264" 272.000000 ###
   79. "data:0.485" 262.000000 ###
   80. "data:0.469" 260.000000 ###
   81. "data:0.224" 256.000000 ###
   82. "data:0.220" 255.000000 ###
   83. "data:0.368" 254.000000 ###
   84. "data:0.253" 253.000000 ###
   85. "data:0.303" 252.000000 ###
   86. "data:0.454" 252.000000 ###
   87. "data:0.361" 251.000000 ###
   88. "data:0.293" 250.000000 ##
   89. "data:0.268" 248.000000 ##
   90.  "data:0.99" 246.000000 ##
   91. "data:0.351" 241.000000 ##
   92. "data:0.487" 240.000000 ##
   93. "data:0.252" 237.000000 ##
   94. "data:0.483" 237.000000 ##
   95. "data:0.168" 235.000000 ##
   96.  "data:0.74" 233.000000 ##
   97. "data:0.435" 229.000000 ##
   98. "data:0.176" 228.000000 ##
   99. "data:0.400" 227.000000 ##
  100.  "data:0.44" 225.000000 ##
  101.   "data:0.4" 223.000000 ##
  102.  "data:0.15" 221.000000 ##
  103. "data:0.364" 221.000000 ##
  104. "data:0.296" 216.000000 ##
  105. "data:0.425" 215.000000 ##
  106. "data:0.105" 209.000000 ##
  107. "data:0.153" 209.000000 ##
  108.  "data:0.97" 209.000000 ##
  109. "data:0.124" 207.000000 ##
  110. "data:0.127" 207.000000 ##
  111.  "data:0.21" 202.000000 ##
  112. "data:0.353" 200.000000 ##
  113. "data:0.187" 199.000000 ##
  114. "data:0.280" 198.000000 ##
  115. "data:0.107" 197.000000 ##
  116. "data:0.257" 197.000000 ##
  117. "data:0.221" 188.000000 ##
  118. "data:0.234" 188.000000 ##
  119. "data:0.199" 185.000000 ##
  120.  "data:0.28" 184.000000 ##
  121.   "data:0.3" 184.000000 ##
  122. "data:0.313" 184.000000 ##
  123.   "data:0.6" 184.000000 ##
  124. "data:0.458" 183.000000 ##
  125. "data:0.498" 181.000000 ##
  126. "data:0.214" 180.000000 ##
  127.  "data:0.93" 174.000000 ##
  128. "data:0.186" 172.000000 #
  129.  "data:0.52" 172.000000 #
  130. "data:0.294" 170.000000 #
  131. "data:0.446" 169.000000 #
  132.  "data:0.63" 167.000000 #
  133. "data:0.123" 166.000000 #
  134. "data:0.207" 166.000000 #
  135. "data:0.362" 166.000000 #
  136.  "data:0.32" 164.000000 #
  137. "data:0.384" 164.000000 #
  138. "data:0.344" 163.000000 #
  139.  "data:0.72" 162.000000 #
  140. "data:0.417" 160.000000 #
  141. "data:0.328" 158.000000 #
  142. "data:0.126" 157.000000 #
  143. "data:0.147" 157.000000 #
  144.  "data:0.79" 156.000000 #
  145. "data:0.195" 155.000000 #
  146. "data:0.200" 152.000000 #
  147. "data:0.385" 151.000000 #
  148. "data:0.470" 151.000000 #
  149. "data:0.489" 151.000000 #
  150. "data:0.113" 150.000000 #
  151. "data:0.336" 150.000000 #
  152.  "data:0.40" 150.000000 #
  153. "data:0.411" 149.000000 #
  154.  "data:0.19" 147.000000 #
  155. "data:0.226" 147.000000 #
  156. "data:0.275" 145.000000 #
  157. "data:0.449" 145.000000 #
  158. "data:0.185" 142.000000 #
  159. "data:0.144" 138.000000 #
  160. "data:0.482" 137.000000 #
  161. "data:0.132" 136.000000 #
  162. "data:0.348" 135.000000 #
  163. "data:0.282" 134.000000 #
  164.  "data:0.25" 132.000000 #
  165. "data:0.269" 132.000000 #
  166. "data:0.436" 132.000000 #
  167. "data:0.338" 131.000000 #
  168. "data:0.334" 130.000000 #
  169. "data:0.148" 129.000000 #
  170. "data:0.229" 129.000000 #
  171. "data:0.238" 129.000000 #
  172. "data:0.259" 129.000000 #
  173. "data:0.166" 128.000000 #
  174. "data:0.459" 128.000000 #
  175. "data:0.451" 127.000000 #
  176.  "data:0.43" 126.000000 #
  177. "data:0.480" 124.000000 #
  178. "data:0.369" 123.000000 #
  179. "data:0.210" 122.000000 #
  180. "data:0.399" 122.000000 #
  181. "data:0.494" 122.000000 #
  182. "data:0.287" 121.000000 #
  183. "data:0.169" 120.000000 #
  184. "data:0.387" 120.000000 #
  185. "data:0.304" 119.000000 #
  186.  "data:0.56" 118.000000 #
  187.  "data:0.85" 117.000000 #
  188. "data:0.246" 116.000000 #
  189. "data:0.251" 116.000000 #
  190. "data:0.333" 116.000000 #
  191. "data:0.112" 115.000000 #
  192. "data:0.410" 115.000000 #
  193. "data:0.106" 114.000000 #
  194. "data:0.117" 114.000000 #
  195.  "data:0.89" 114.000000 #
  196. "data:0.463" 113.000000 #
  197. "data:0.158" 112.000000 #
  198. "data:0.228" 112.000000 #
  199. "data:0.182" 111.000000 #
  200.  "data:0.37" 111.000000 #
  201.  "data:0.48" 111.000000 #
  202. "data:0.116" 110.000000 #
  203. "data:0.139" 110.000000 #
  204. "data:0.211" 110.000000 #
  205. "data:0.244" 110.000000 #
  206. "data:0.337" 109.000000 #
  207. "data:0.354" 108.000000 #
  208. "data:0.398" 108.000000 #
  209.  "data:0.23" 107.000000 #
  210.  "data:0.66" 107.000000 #
  211. "data:0.288" 106.000000 #
  212. "data:0.445" 106.000000 #
  213. "data:0.340" 105.000000 #
  214. "data:0.347" 105.000000 #
  215.  "data:0.17" 104.000000 #
  216.   "data:0.9" 104.000000 #
  217. "data:0.150" 103.000000 #
  218.  "data:0.16" 103.000000 #
  219. "data:0.227" 103.000000 #
  220.  "data:0.31" 103.000000 #
  221. "data:0.120" 102.000000 #
  222. "data:0.239" 102.000000 #
  223. "data:0.477" 101.000000 #
  224. "data:0.499" 101.000000 #
  225. "data:0.292" 100.000000 #
  226. "data:0.331" 100.000000 #
  227. "data:0.424" 100.000000 #
  228. "data:0.119" 98.000000 #
  229. "data:0.193" 98.000000 #
  230. "data:0.356" 98.000000 #
  231. "data:0.149" 97.000000 #
  232. "data:0.216" 97.000000 #
  233. "data:0.270" 97.000000 #
  234. "data:0.205" 96.000000 #
  235. "data:0.320" 96.000000 #
  236. "data:0.390" 96.000000 #
  237. "data:0.497" 96.000000 #
  238. "data:0.339" 95.000000 #
  239. "data:0.394" 95.000000 #
  240. "data:0.405" 95.000000 #
  241.   "data:0.8" 95.000000 #
  242. "data:0.189" 94.000000
  243. "data:0.324" 94.000000
  244. "data:0.429" 94.000000
  245. "data:0.175" 92.000000
  246. "data:0.341" 92.000000
  247.  "data:0.75" 92.000000
  248. "data:0.179" 91.000000
  249. "data:0.231" 91.000000
  250. "data:0.389" 91.000000
  251. "data:0.456" 91.000000
  252.  "data:0.55" 91.000000
  253. "data:0.286" 90.000000
  254. "data:0.346" 90.000000
  255. "data:0.114" 89.000000
  256. "data:0.208" 89.000000
  257. "data:0.478" 88.000000
  258.  "data:0.24" 87.000000
  259. "data:0.481" 87.000000
  260.  "data:0.71" 87.000000
  261. "data:0.196" 86.000000
  262. "data:0.254" 86.000000
  263. "data:0.111" 85.000000
  264.  "data:0.86" 85.000000
  265.  "data:0.88" 85.000000
  266.  "data:0.20" 84.000000
  267. "data:0.468" 84.000000
  268. "data:0.143" 83.000000
  269. "data:0.225" 83.000000
  270. "data:0.312" 83.000000
  271. "data:0.401" 83.000000
  272. "data:0.447" 83.000000
  273. "data:0.172" 82.000000
  274. "data:0.198" 82.000000
  275. "data:0.355" 82.000000
  276.  "data:0.38" 82.000000
  277. "data:0.461" 82.000000
  278. "data:0.218" 81.000000
  279. "data:0.299" 81.000000
  280. "data:0.360" 81.000000
  281. "data:0.440" 81.000000
  282. "data:0.310" 80.000000
  283. "data:0.452" 80.000000
  284. "data:0.138" 79.000000
  285. "data:0.142" 79.000000
  286. "data:0.365" 79.000000
  287. "data:0.278" 78.000000
  288. "data:0.491" 78.000000
  289.  "data:0.73" 78.000000
  290. "data:0.432" 77.000000
  291. "data:0.165" 76.000000
  292. "data:0.271" 76.000000
  293. "data:0.306" 76.000000
  294. "data:0.345" 76.000000
  295. "data:0.133" 75.000000
  296.  "data:0.96" 75.000000
  297. "data:0.152" 74.000000
  298. "data:0.392" 74.000000
  299. "data:0.495" 74.000000
  300. "data:0.125" 73.000000
  301.  "data:0.13" 73.000000
  302. "data:0.302" 73.000000
  303. "data:0.335" 73.000000
  304. "data:0.359" 73.000000
  305. "data:0.416" 73.000000
  306. "data:0.460" 73.000000
  307. "data:0.209" 72.000000
  308. "data:0.290" 72.000000
  309. "data:0.367" 72.000000
  310. "data:0.428" 72.000000
  311. "data:0.236" 71.000000
  312.  "data:0.29" 71.000000
  313. "data:0.212" 70.000000
  314. "data:0.393" 70.000000
  315. "data:0.309" 69.000000
  316. "data:0.450" 69.000000
  317. "data:0.134" 68.000000
  318. "data:0.272" 68.000000
  319. "data:0.274" 68.000000
  320. "data:0.358" 68.000000
  321. "data:0.154" 66.000000
  322. "data:0.162" 66.000000
  323. "data:0.167" 66.000000
  324. "data:0.203" 66.000000
  325. "data:0.237" 66.000000
  326. "data:0.261" 66.000000
  327. "data:0.276" 66.000000
  328. "data:0.323" 66.000000
  329. "data:0.396" 66.000000
  330.  "data:0.60" 66.000000
  331. "data:0.177" 65.000000
  332. "data:0.377" 65.000000
  333. "data:0.409" 65.000000
  334. "data:0.444" 65.000000
  335. "data:0.188" 64.000000
  336.  "data:0.35" 64.000000
  337.  "data:0.50" 64.000000
  338. "data:0.160" 63.000000
  339. "data:0.374" 63.000000
  340. "data:0.427" 63.000000
  341.  "data:0.58" 63.000000
  342.  "data:0.98" 63.000000
  343. "data:0.357" 62.000000
  344. "data:0.476" 62.000000
  345.  "data:0.84" 62.000000
  346.  "data:0.10" 61.000000
  347.  "data:0.11" 61.000000
  348. "data:0.110" 61.000000
  349. "data:0.135" 61.000000
  350. "data:0.406" 61.000000
  351. "data:0.104" 60.000000
  352. "data:0.163" 60.000000
  353. "data:0.249" 60.000000
  354. "data:0.279" 60.000000
  355. "data:0.382" 60.000000
  356. "data:0.455" 60.000000
  357.  "data:0.59" 60.000000
  358. "data:0.291" 59.000000
  359. "data:0.421" 59.000000
  360.  "data:0.53" 59.000000
  361.  "data:0.95" 59.000000
  362. "data:0.204" 58.000000
  363. "data:0.329" 58.000000
  364.  "data:0.64" 58.000000
  365. "data:0.128" 57.000000
  366. "data:0.180" 57.000000
  367. "data:0.192" 57.000000
  368. "data:0.343" 57.000000
  369. "data:0.376" 57.000000
  370. "data:0.462" 57.000000
  371. "data:0.164" 56.000000
  372.  "data:0.27" 56.000000
  373. "data:0.295" 56.000000
  374. "data:0.422" 56.000000
  375. "data:0.155" 55.000000
  376. "data:0.243" 55.000000
  377. "data:0.245" 55.000000
  378. "data:0.420" 55.000000
  379. "data:0.146" 54.000000
  380. "data:0.171" 54.000000
  381. "data:0.332" 54.000000
  382. "data:0.492" 54.000000
  383. "data:0.190" 53.000000
  384. "data:0.297" 53.000000
  385.  "data:0.45" 53.000000
  386. "data:0.191" 52.000000
  387. "data:0.202" 52.000000
  388. "data:0.222" 52.000000
  389. "data:0.240" 52.000000
  390.   "data:0.1" 51.000000
  391. "data:0.101" 51.000000
  392. "data:0.273" 51.000000
  393. "data:0.442" 51.000000
  394.  "data:0.54" 51.000000
  395. "data:0.102" 50.000000
  396. "data:0.115" 50.000000
  397. "data:0.136" 50.000000
  398. "data:0.464" 50.000000
  399. "data:0.241" 49.000000
  400. "data:0.307" 49.000000
  401. "data:0.402" 48.000000
  402. "data:0.414" 48.000000
  403. "data:0.443" 48.000000
  404.   "data:0.5" 48.000000
  405. "data:0.118" 47.000000
  406. "data:0.173" 47.000000
  407. "data:0.184" 47.000000
  408. "data:0.289" 47.000000
  409. "data:0.453" 47.000000
  410.  "data:0.57" 47.000000
  411. "data:0.319" 46.000000
  412. "data:0.373" 46.000000
  413. "data:0.431" 46.000000
  414. "data:0.109" 45.000000
  415. "data:0.230" 45.000000
  416. "data:0.311" 45.000000
  417.  "data:0.62" 45.000000
  418.  "data:0.70" 45.000000
  419.  "data:0.91" 44.000000
  420.  "data:0.94" 44.000000
  421. "data:0.383" 43.000000
  422.  "data:0.39" 43.000000
  423. "data:0.437" 43.000000
  424.  "data:0.65" 43.000000
  425. "data:0.103" 42.000000
  426. "data:0.213" 42.000000
  427.  "data:0.22" 42.000000
  428. "data:0.381" 42.000000
  429. "data:0.140" 41.000000
  430. "data:0.386" 41.000000
  431. "data:0.201" 40.000000
  432. "data:0.397" 40.000000
  433.  "data:0.83" 40.000000
  434. "data:0.130" 39.000000
  435.  "data:0.69" 39.000000
  436. "data:0.265" 38.000000
  437. "data:0.300" 38.000000
  438. "data:0.484" 38.000000
  439. "data:0.197" 37.000000
  440.  "data:0.26" 37.000000
  441. "data:0.372" 37.000000
  442.  "data:0.47" 37.000000
  443. "data:0.247" 36.000000
  444. "data:0.284" 36.000000
  445. "data:0.395" 36.000000
  446.  "data:0.77" 36.000000
  447. "data:0.100" 35.000000
  448. "data:0.141" 35.000000
  449. "data:0.403" 35.000000
  450. "data:0.408" 35.000000
  451. "data:0.217" 34.000000
  452. "data:0.321" 34.000000
  453. "data:0.404" 34.000000
  454. "data:0.438" 34.000000
  455. "data:0.467" 34.000000
  456. "data:0.493" 34.000000
  457.  "data:0.78" 34.000000
  458.  "data:0.90" 34.000000
  459.   "data:0.0" 33.000000
  460. "data:0.156" 33.000000
  461.  "data:0.87" 33.000000
  462. "data:0.301" 32.000000
  463.  "data:0.68" 32.000000
  464. "data:0.215" 31.000000
  465. "data:0.256" 31.000000
  466. "data:0.418" 31.000000
  467. "data:0.479" 31.000000
  468.  "data:0.51" 31.000000
  469. "data:0.314" 30.000000
  470. "data:0.315" 30.000000
  471. "data:0.322" 30.000000
  472. "data:0.352" 30.000000
  473. "data:0.465" 30.000000
  474.  "data:0.81" 30.000000
  475. "data:0.363" 29.000000
  476.  "data:0.80" 29.000000
  477. "data:0.194" 28.000000
  478. "data:0.219" 28.000000
  479. "data:0.262" 28.000000
  480. "data:0.366" 28.000000
  481. "data:0.407" 28.000000
  482.  "data:0.46" 28.000000
  483. "data:0.170" 27.000000
  484. "data:0.281" 27.000000
  485. "data:0.326" 27.000000
  486. "data:0.349" 26.000000
  487.  "data:0.34" 25.000000
  488. "data:0.412" 24.000000
  489. "data:0.235" 23.000000
  490. "data:0.267" 23.000000
  491. "data:0.388" 23.000000
  492. "data:0.466" 23.000000
  493. "data:0.475" 23.000000
  494. "data:0.223" 22.000000
  495. "data:0.342" 22.000000
  496.  "data:0.76" 22.000000
  497. "data:0.233" 21.000000
  498.  "data:0.61" 18.000000
  499.  "data:0.92" 18.000000
  500.   "data:0.7" 16.000000

Variable Importance: SUM_SCORE:
    1.  "data:0.33" 12222.572121 ################
    2. "data:0.488" 8179.064385 ##########
    3. "data:0.471" 6677.366822 ########
    4. "data:0.183" 5313.074891 ######
    5.  "data:0.30" 4056.183100 #####
    6. "data:0.137" 3463.219096 ####
    7.  "data:0.49" 3057.509913 ###
    8. "data:0.258" 2781.685229 ###
    9.  "data:0.41" 2513.485614 ###
   10. "data:0.419" 2065.468735 ##
   11. "data:0.298" 1993.840743 ##
   12. "data:0.317" 1982.855397 ##
   13. "data:0.330" 1898.351934 ##
   14.  "data:0.36" 1868.370333 ##
   15. "data:0.430" 1798.866612 ##
   16. "data:0.122" 1550.491813 ##
   17. "data:0.474" 1443.820371 #
   18. "data:0.206" 1397.947938 #
   19. "data:0.423" 1348.018153 #
   20. "data:0.151" 1309.433650 #
   21. "data:0.145" 1259.784445 #
   22. "data:0.487" 1248.084995 #
   23. "data:0.375" 1122.856572 #
   24. "data:0.108" 1094.799095 #
   25. "data:0.277" 1068.363671 #
   26. "data:0.361" 1009.793730 #
   27. "data:0.327" 990.305759 #
   28. "data:0.129" 977.862493 #
   29.  "data:0.12" 924.360468 #
   30. "data:0.283" 911.638840 #
   31. "data:0.318" 895.929800 #
   32.  "data:0.15" 891.730220 #
   33. "data:0.248" 884.693831 #
   34. "data:0.303" 874.798889 #
   35. "data:0.368" 826.215874 #
   36.  "data:0.18" 822.136624 #
   37. "data:0.441" 693.959608
   38. "data:0.199" 649.886393
   39. "data:0.457" 642.434855
   40. "data:0.181" 637.479234
   41. "data:0.486" 623.083690
   42. "data:0.220" 620.191044
   43.  "data:0.14" 606.880554
   44. "data:0.448" 581.817730
   45. "data:0.413" 575.125012
   46.  "data:0.99" 570.129651
   47. "data:0.157" 565.566239
   48. "data:0.439" 550.671804
   49. "data:0.379" 550.409578
   50. "data:0.280" 526.492302
   51. "data:0.305" 513.601214
   52. "data:0.446" 473.518263
   53.  "data:0.52" 471.926433
   54.   "data:0.2" 459.810366
   55. "data:0.168" 451.612942
   56. "data:0.490" 447.405881
   57. "data:0.234" 443.324457
   58. "data:0.242" 430.800392
   59.  "data:0.82" 420.454200
   60. "data:0.353" 412.316561
   61. "data:0.255" 409.201383
   62. "data:0.224" 407.475096
   63. "data:0.195" 404.153442
   64.  "data:0.28" 395.823285
   65. "data:0.425" 394.660813
   66. "data:0.483" 392.442093
   67. "data:0.415" 382.933935
   68. "data:0.250" 375.091031
   69.  "data:0.42" 363.360498
   70. "data:0.378" 349.832563
   71. "data:0.127" 345.442899
   72. "data:0.380" 340.606926
   73. "data:0.351" 340.040101
   74.  "data:0.97" 333.312787
   75. "data:0.496" 330.989751
   76. "data:0.266" 316.085208
   77. "data:0.325" 299.325675
   78. "data:0.253" 280.505677
   79. "data:0.187" 273.443787
   80. "data:0.308" 268.001594
   81.  "data:0.21" 267.197818
   82. "data:0.131" 263.139021
   83. "data:0.268" 260.294880
   84. "data:0.174" 256.499349
   85. "data:0.159" 254.396679
   86. "data:0.417" 252.522009
   87. "data:0.105" 248.823872
   88. "data:0.200" 247.097062
   89. "data:0.263" 243.074395
   90. "data:0.296" 241.284348
   91. "data:0.334" 238.688171
   92. "data:0.285" 238.643100
   93. "data:0.350" 236.308688
   94. "data:0.364" 235.535561
   95. "data:0.399" 233.273073
   96. "data:0.264" 231.805888
   97. "data:0.433" 228.924485
   98.  "data:0.44" 227.804412
   99.  "data:0.67" 226.449077
  100. "data:0.185" 224.380682
  101. "data:0.426" 216.521799
  102. "data:0.498" 214.959165
  103. "data:0.178" 214.179384
  104. "data:0.214" 212.748395
  105. "data:0.472" 211.618457
  106. "data:0.124" 209.914581
  107. "data:0.121" 204.974361
  108. "data:0.354" 202.235190
  109. "data:0.473" 198.848453
  110. "data:0.120" 196.084992
  111. "data:0.449" 194.784443
  112. "data:0.371" 192.626062
  113. "data:0.391" 191.112130
  114.   "data:0.3" 188.123769
  115. "data:0.161" 188.117178
  116.  "data:0.72" 182.243933
  117. "data:0.384" 181.665164
  118. "data:0.435" 180.891740
  119. "data:0.470" 177.164478
  120. "data:0.207" 175.417409
  121. "data:0.260" 175.317552
  122. "data:0.333" 173.948938
  123. "data:0.316" 173.819293
  124. "data:0.400" 173.566165
  125. "data:0.434" 173.498781
  126. "data:0.259" 171.895264
  127. "data:0.226" 170.373618
  128. "data:0.370" 168.941579
  129. "data:0.469" 166.849531
  130. "data:0.147" 162.440694
  131. "data:0.458" 161.696000
  132. "data:0.485" 159.359314
  133. "data:0.232" 158.060034
  134. "data:0.126" 152.966949
  135. "data:0.112" 152.741291
  136. "data:0.275" 152.486391
  137. "data:0.107" 148.417232
  138.  "data:0.89" 147.993857
  139. "data:0.293" 147.431197
  140. "data:0.282" 146.691411
  141. "data:0.123" 144.778432
  142. "data:0.252" 144.191798
  143. "data:0.489" 142.900820
  144. "data:0.454" 142.089663
  145. "data:0.294" 139.776466
  146. "data:0.113" 138.371028
  147. "data:0.153" 137.770024
  148.  "data:0.93" 137.570224
  149. "data:0.494" 131.587519
  150. "data:0.238" 129.827602
  151. "data:0.356" 127.014436
  152. "data:0.176" 126.296469
  153. "data:0.186" 125.570053
  154. "data:0.344" 122.732703
  155.  "data:0.23" 122.444883
  156. "data:0.398" 120.018739
  157. "data:0.257" 119.919075
  158.   "data:0.4" 119.414564
  159. "data:0.221" 118.651787
  160. "data:0.211" 117.464914
  161. "data:0.387" 116.732392
  162.  "data:0.74" 116.088689
  163. "data:0.244" 115.844614
  164. "data:0.482" 115.409636
  165. "data:0.269" 115.363353
  166.  "data:0.43" 113.232747
  167. "data:0.463" 111.519600
  168. "data:0.362" 111.240389
  169.  "data:0.79" 111.037205
  170. "data:0.499" 110.648245
  171. "data:0.313" 109.838499
  172.  "data:0.17" 109.362367
  173. "data:0.328" 105.359173
  174. "data:0.369" 105.211587
  175.  "data:0.63" 104.788838
  176.  "data:0.71" 104.730213
  177. "data:0.401" 104.011704
  178.   "data:0.6" 103.308823
  179. "data:0.117" 103.180980
  180. "data:0.216" 101.574532
  181. "data:0.338" 101.307288
  182. "data:0.339" 100.304044
  183. "data:0.411" 99.480756
  184.  "data:0.56" 98.475604
  185. "data:0.336" 98.235230
  186. "data:0.150" 98.087519
  187.  "data:0.32" 97.380585
  188.  "data:0.85" 96.289969
  189.   "data:0.8" 96.261544
  190. "data:0.312" 93.989462
  191. "data:0.445" 93.970812
  192. "data:0.166" 93.578746
  193. "data:0.229" 93.302853
  194.  "data:0.48" 91.574971
  195. "data:0.148" 91.389870
  196. "data:0.331" 91.169593
  197. "data:0.132" 90.766616
  198.  "data:0.55" 89.824611
  199.  "data:0.40" 89.798429
  200. "data:0.385" 89.767050
  201. "data:0.225" 89.523919
  202. "data:0.480" 89.333883
  203. "data:0.340" 88.752491
  204. "data:0.468" 88.072701
  205. "data:0.246" 88.003236
  206. "data:0.302" 87.956621
  207. "data:0.179" 87.424649
  208.  "data:0.66" 86.393357
  209.  "data:0.88" 86.142559
  210. "data:0.210" 84.317967
  211. "data:0.227" 83.844163
  212.  "data:0.37" 83.222680
  213. "data:0.286" 82.699893
  214. "data:0.228" 82.698802
  215. "data:0.144" 82.692687
  216. "data:0.193" 81.921906
  217. "data:0.251" 81.889679
  218. "data:0.447" 81.680037
  219.  "data:0.25" 81.495935
  220. "data:0.389" 81.040621
  221. "data:0.139" 80.657848
  222. "data:0.287" 78.365856
  223. "data:0.320" 77.947128
  224. "data:0.410" 77.551255
  225. "data:0.436" 77.383015
  226.  "data:0.19" 77.252457
  227.  "data:0.16" 77.105358
  228. "data:0.119" 76.549672
  229. "data:0.288" 75.127280
  230. "data:0.304" 74.772685
  231. "data:0.299" 74.168673
  232. "data:0.422" 72.134660
  233. "data:0.239" 72.062837
  234. "data:0.451" 71.738658
  235. "data:0.491" 71.478213
  236.  "data:0.86" 71.274673
  237. "data:0.347" 71.083247
  238. "data:0.169" 70.697688
  239. "data:0.208" 70.559375
  240.  "data:0.31" 70.522870
  241. "data:0.459" 70.442206
  242. "data:0.114" 70.302246
  243. "data:0.450" 70.264829
  244. "data:0.432" 70.226803
  245. "data:0.309" 70.100234
  246. "data:0.392" 70.067749
  247. "data:0.348" 69.976039
  248. "data:0.359" 69.470066
  249. "data:0.405" 69.354782
  250. "data:0.237" 69.322221
  251.   "data:0.9" 68.695729
  252. "data:0.158" 67.672375
  253. "data:0.478" 67.172660
  254. "data:0.271" 67.092337
  255.  "data:0.75" 67.087465
  256. "data:0.292" 66.485057
  257. "data:0.182" 66.089749
  258. "data:0.310" 65.513102
  259. "data:0.278" 65.458142
  260. "data:0.133" 65.434494
  261. "data:0.456" 64.723264
  262. "data:0.143" 64.703394
  263. "data:0.376" 64.556241
  264. "data:0.111" 64.309357
  265. "data:0.357" 64.276791
  266. "data:0.477" 64.193778
  267. "data:0.497" 64.022784
  268. "data:0.106" 63.566510
  269. "data:0.337" 63.439210
  270. "data:0.355" 63.230774
  271. "data:0.218" 62.779181
  272. "data:0.481" 62.765186
  273. "data:0.125" 62.707855
  274. "data:0.116" 62.102564
  275. "data:0.212" 61.792658
  276. "data:0.274" 61.692038
  277. "data:0.152" 61.622757
  278. "data:0.324" 61.198474
  279. "data:0.198" 60.735695
  280. "data:0.205" 60.699904
  281. "data:0.440" 60.347303
  282. "data:0.335" 60.117227
  283. "data:0.138" 60.078310
  284. "data:0.346" 59.971406
  285. "data:0.394" 59.808026
  286. "data:0.341" 59.554320
  287.  "data:0.96" 59.551486
  288. "data:0.460" 59.551096
  289. "data:0.306" 59.494014
  290. "data:0.452" 59.483657
  291. "data:0.332" 59.148763
  292. "data:0.476" 58.981851
  293.  "data:0.38" 58.663415
  294. "data:0.390" 58.658353
  295. "data:0.149" 58.635970
  296. "data:0.189" 58.168783
  297. "data:0.495" 57.913524
  298. "data:0.367" 57.514983
  299.  "data:0.73" 57.484268
  300.  "data:0.20" 56.904574
  301. "data:0.196" 56.785095
  302. "data:0.175" 56.702184
  303. "data:0.167" 56.587632
  304. "data:0.270" 56.553998
  305. "data:0.272" 56.404378
  306. "data:0.424" 56.275985
  307. "data:0.172" 56.180337
  308.  "data:0.24" 56.029723
  309. "data:0.396" 55.812866
  310. "data:0.160" 55.757943
  311. "data:0.142" 55.737759
  312. "data:0.231" 55.624233
  313. "data:0.360" 54.232081
  314.  "data:0.13" 53.412139
  315. "data:0.291" 53.368248
  316. "data:0.416" 52.445893
  317. "data:0.192" 52.059711
  318. "data:0.154" 51.572491
  319.  "data:0.35" 50.203010
  320. "data:0.254" 50.056627
  321. "data:0.329" 50.005549
  322. "data:0.276" 49.845215
  323.  "data:0.95" 49.545405
  324. "data:0.429" 49.223175
  325. "data:0.165" 49.090101
  326. "data:0.295" 48.433316
  327. "data:0.428" 48.358680
  328. "data:0.345" 48.103542
  329. "data:0.421" 47.955526
  330. "data:0.190" 47.913896
  331. "data:0.461" 47.845366
  332.  "data:0.64" 47.490309
  333. "data:0.261" 47.291677
  334. "data:0.393" 47.174336
  335.  "data:0.59" 46.966871
  336.  "data:0.60" 46.948596
  337. "data:0.203" 46.842235
  338. "data:0.128" 46.609780
  339.  "data:0.11" 46.289902
  340. "data:0.443" 46.259213
  341. "data:0.209" 45.999646
  342. "data:0.249" 45.951133
  343. "data:0.279" 45.811730
  344. "data:0.163" 45.564296
  345. "data:0.162" 45.237969
  346.  "data:0.53" 45.016636
  347. "data:0.492" 44.987688
  348.  "data:0.58" 44.887143
  349.  "data:0.29" 44.832809
  350. "data:0.406" 44.650140
  351.  "data:0.98" 44.635013
  352. "data:0.455" 44.587174
  353.   "data:0.5" 44.400191
  354. "data:0.343" 44.230477
  355.  "data:0.84" 44.193532
  356. "data:0.444" 44.077491
  357. "data:0.365" 43.683463
  358. "data:0.243" 43.410966
  359. "data:0.402" 43.168789
  360. "data:0.236" 43.115554
  361.  "data:0.27" 43.064729
  362.  "data:0.65" 43.028580
  363. "data:0.241" 42.803848
  364. "data:0.164" 42.780973
  365. "data:0.427" 42.731891
  366.  "data:0.50" 42.662178
  367. "data:0.177" 42.387389
  368. "data:0.135" 42.015532
  369. "data:0.290" 41.915454
  370. "data:0.323" 41.826401
  371. "data:0.273" 41.206222
  372. "data:0.188" 41.088280
  373. "data:0.307" 41.066168
  374. "data:0.453" 40.871801
  375. "data:0.358" 40.551539
  376. "data:0.173" 40.488384
  377. "data:0.409" 40.416606
  378. "data:0.134" 40.152969
  379. "data:0.180" 39.984602
  380.  "data:0.54" 39.844906
  381. "data:0.202" 39.844276
  382. "data:0.115" 39.819489
  383. "data:0.462" 39.437994
  384. "data:0.118" 39.085091
  385. "data:0.104" 39.022687
  386. "data:0.373" 38.879684
  387. "data:0.230" 38.876233
  388. "data:0.420" 38.775079
  389. "data:0.204" 38.628169
  390. "data:0.109" 38.368474
  391. "data:0.171" 38.216255
  392.  "data:0.10" 38.082586
  393.   "data:0.1" 37.535575
  394. "data:0.102" 37.126979
  395. "data:0.382" 36.946919
  396. "data:0.155" 36.777755
  397. "data:0.377" 36.701321
  398.  "data:0.45" 36.447867
  399. "data:0.374" 35.369179
  400. "data:0.464" 35.249559
  401. "data:0.431" 35.101466
  402. "data:0.201" 34.732981
  403. "data:0.381" 34.419625
  404. "data:0.184" 34.368653
  405. "data:0.240" 33.851110
  406. "data:0.311" 33.325224
  407. "data:0.297" 33.122972
  408. "data:0.141" 33.121632
  409.  "data:0.47" 33.034106
  410. "data:0.414" 32.946877
  411. "data:0.101" 32.900598
  412. "data:0.130" 32.784461
  413. "data:0.136" 32.715836
  414.  "data:0.83" 32.589069
  415. "data:0.383" 32.189764
  416. "data:0.442" 31.862890
  417. "data:0.146" 31.435158
  418. "data:0.110" 31.247737
  419.  "data:0.94" 31.241258
  420. "data:0.191" 30.747319
  421.  "data:0.69" 30.511407
  422. "data:0.397" 30.438605
  423.   "data:0.0" 30.417625
  424.  "data:0.91" 30.045567
  425. "data:0.197" 29.898017
  426. "data:0.213" 29.889252
  427.  "data:0.62" 29.859133
  428. "data:0.289" 29.737226
  429. "data:0.319" 29.557883
  430. "data:0.222" 29.543859
  431. "data:0.103" 29.532854
  432. "data:0.245" 29.501612
  433.  "data:0.39" 29.159209
  434. "data:0.404" 29.120235
  435.  "data:0.70" 28.929157
  436. "data:0.403" 28.852568
  437. "data:0.140" 28.849411
  438. "data:0.372" 28.739527
  439. "data:0.437" 28.490711
  440. "data:0.265" 28.434415
  441. "data:0.100" 28.157431
  442.  "data:0.57" 27.608847
  443. "data:0.284" 27.393785
  444. "data:0.301" 27.311907
  445.  "data:0.90" 27.285150
  446.  "data:0.51" 27.267361
  447. "data:0.386" 27.191289
  448.  "data:0.22" 27.099106
  449.  "data:0.26" 27.039320
  450. "data:0.408" 26.811433
  451. "data:0.300" 26.183398
  452. "data:0.484" 26.124726
  453. "data:0.247" 25.911905
  454. "data:0.395" 24.654597
  455.  "data:0.81" 24.045674
  456. "data:0.321" 23.772957
  457. "data:0.467" 23.623269
  458. "data:0.217" 23.281928
  459. "data:0.479" 23.074631
  460. "data:0.407" 22.858294
  461. "data:0.438" 22.845315
  462. "data:0.493" 22.838445
  463.  "data:0.78" 22.644885
  464. "data:0.465" 22.240778
  465.  "data:0.87" 22.227608
  466. "data:0.219" 21.588192
  467.  "data:0.68" 21.142925
  468. "data:0.215" 20.954920
  469. "data:0.352" 20.913418
  470. "data:0.156" 20.837684
  471. "data:0.326" 20.714890
  472. "data:0.170" 20.531260
  473. "data:0.322" 20.475300
  474. "data:0.363" 20.320656
  475. "data:0.256" 20.084280
  476. "data:0.281" 19.456480
  477. "data:0.418" 18.913596
  478. "data:0.223" 18.778677
  479. "data:0.366" 18.471155
  480.  "data:0.80" 18.448956
  481. "data:0.314" 18.420024
  482.  "data:0.76" 18.369827
  483. "data:0.315" 18.206708
  484. "data:0.262" 18.041655
  485.  "data:0.77" 17.964916
  486. "data:0.466" 17.576733
  487. "data:0.194" 17.384078
  488. "data:0.233" 16.692765
  489. "data:0.388" 16.560195
  490.  "data:0.46" 16.056189
  491. "data:0.235" 15.508088
  492. "data:0.349" 15.486860
  493.  "data:0.34" 14.498629
  494. "data:0.267" 14.354456
  495. "data:0.475" 14.201954
  496. "data:0.412" 13.403615
  497.  "data:0.61" 13.289705
  498.  "data:0.92" 11.756495
  499. "data:0.342" 11.746120
  500.   "data:0.7"  5.378347



Loss: BINOMIAL_LOG_LIKELIHOOD
Validation loss value: 0.772449
Number of trees per iteration: 1
Node format: NOT_SET
Number of trees: 1000
Total number of nodes: 151230

Number of nodes by tree:
Count: 1000 Average: 151.23 StdDev: 18.7232
Min: 99 Max: 205 Ignored: 0
----------------------------------------------
[  99, 104)   4   0.40%   0.40%
[ 104, 109)   5   0.50%   0.90%
[ 109, 115)  12   1.20%   2.10% #
[ 115, 120)  25   2.50%   4.60% ##
[ 120, 125)  30   3.00%   7.60% ##
[ 125, 131)  56   5.60%  13.20% #####
[ 131, 136)  85   8.50%  21.70% #######
[ 136, 141)  62   6.20%  27.90% #####
[ 141, 147) 120  12.00%  39.90% ##########
[ 147, 152) 124  12.40%  52.30% ##########
[ 152, 157)  80   8.00%  60.30% ######
[ 157, 163)  97   9.70%  70.00% ########
[ 163, 168) 101  10.10%  80.10% ########
[ 168, 173)  62   6.20%  86.30% #####
[ 173, 179)  63   6.30%  92.60% #####
[ 179, 184)  33   3.30%  95.90% ###
[ 184, 189)  15   1.50%  97.40% #
[ 189, 195)  14   1.40%  98.80% #
[ 195, 200)  10   1.00%  99.80% #
[ 200, 205]   2   0.20% 100.00%

Depth by leafs:
Count: 76115 Average: 6.58446 StdDev: 0.801085
Min: 2 Max: 7 Ignored: 0
----------------------------------------------
[ 2, 3)    46   0.06%   0.06%
[ 3, 4)   453   0.60%   0.66%
[ 4, 5)  2075   2.73%   3.38%
[ 5, 6)  5777   7.59%  10.97% #
[ 6, 7) 11808  15.51%  26.48% ##
[ 7, 7] 55956  73.52% 100.00% ##########

Number of training obs by leaf:
Count: 76115 Average: 183.808 StdDev: 816.092
Min: 5 Max: 10253 Ignored: 0
----------------------------------------------
[     5,   517) 71070  93.37%  93.37% ##########
[   517,  1029)  2608   3.43%  96.80%
[  1029,  1542)   677   0.89%  97.69%
[  1542,  2054)   439   0.58%  98.26%
[  2054,  2567)   238   0.31%  98.58%
[  2567,  3079)    55   0.07%  98.65%
[  3079,  3592)    32   0.04%  98.69%
[  3592,  4104)    21   0.03%  98.72%
[  4104,  4617)    58   0.08%  98.80%
[  4617,  5129)    79   0.10%  98.90%
[  5129,  5641)   107   0.14%  99.04%
[  5641,  6154)   134   0.18%  99.22%
[  6154,  6666)    94   0.12%  99.34%
[  6666,  7179)   110   0.14%  99.48%
[  7179,  7691)   117   0.15%  99.64%
[  7691,  8204)   108   0.14%  99.78%
[  8204,  8716)    76   0.10%  99.88%
[  8716,  9229)    55   0.07%  99.95%
[  9229,  9741)    33   0.04%  99.99%
[  9741, 10253]     4   0.01% 100.00%

Attribute in nodes:
	1265 : data:0.283 [NUMERICAL]
	1184 : data:0.33 [NUMERICAL]
	1060 : data:0.157 [NUMERICAL]
	948 : data:0.183 [NUMERICAL]
	934 : data:0.305 [NUMERICAL]
	814 : data:0.129 [NUMERICAL]
	800 : data:0.474 [NUMERICAL]
	746 : data:0.439 [NUMERICAL]
	728 : data:0.258 [NUMERICAL]
	675 : data:0.250 [NUMERICAL]
	650 : data:0.12 [NUMERICAL]
	626 : data:0.181 [NUMERICAL]
	620 : data:0.41 [NUMERICAL]
	593 : data:0.488 [NUMERICAL]
	554 : data:0.490 [NUMERICAL]
	552 : data:0.415 [NUMERICAL]
	549 : data:0.248 [NUMERICAL]
	540 : data:0.255 [NUMERICAL]
	515 : data:0.263 [NUMERICAL]
	506 : data:0.298 [NUMERICAL]
	496 : data:0.378 [NUMERICAL]
	489 : data:0.122 [NUMERICAL]
	485 : data:0.67 [NUMERICAL]
	475 : data:0.174 [NUMERICAL]
	453 : data:0.2 [NUMERICAL]
	445 : data:0.82 [NUMERICAL]
	443 : data:0.379 [NUMERICAL]
	443 : data:0.36 [NUMERICAL]
	442 : data:0.413 [NUMERICAL]
	442 : data:0.137 [NUMERICAL]
	432 : data:0.318 [NUMERICAL]
	430 : data:0.350 [NUMERICAL]
	415 : data:0.448 [NUMERICAL]
	406 : data:0.472 [NUMERICAL]
	406 : data:0.285 [NUMERICAL]
	397 : data:0.471 [NUMERICAL]
	394 : data:0.266 [NUMERICAL]
	382 : data:0.317 [NUMERICAL]
	377 : data:0.30 [NUMERICAL]
	376 : data:0.108 [NUMERICAL]
	370 : data:0.433 [NUMERICAL]
	368 : data:0.327 [NUMERICAL]
	367 : data:0.430 [NUMERICAL]
	364 : data:0.325 [NUMERICAL]
	355 : data:0.178 [NUMERICAL]
	354 : data:0.375 [NUMERICAL]
	353 : data:0.473 [NUMERICAL]
	349 : data:0.371 [NUMERICAL]
	348 : data:0.49 [NUMERICAL]
	346 : data:0.419 [NUMERICAL]
	343 : data:0.121 [NUMERICAL]
	342 : data:0.330 [NUMERICAL]
	340 : data:0.486 [NUMERICAL]
	338 : data:0.277 [NUMERICAL]
	331 : data:0.14 [NUMERICAL]
	330 : data:0.434 [NUMERICAL]
	327 : data:0.260 [NUMERICAL]
	326 : data:0.145 [NUMERICAL]
	323 : data:0.380 [NUMERICAL]
	321 : data:0.496 [NUMERICAL]
	319 : data:0.316 [NUMERICAL]
	316 : data:0.161 [NUMERICAL]
	310 : data:0.426 [NUMERICAL]
	303 : data:0.441 [NUMERICAL]
	303 : data:0.391 [NUMERICAL]
	298 : data:0.131 [NUMERICAL]
	293 : data:0.18 [NUMERICAL]
	292 : data:0.308 [NUMERICAL]
	292 : data:0.159 [NUMERICAL]
	290 : data:0.423 [NUMERICAL]
	290 : data:0.370 [NUMERICAL]
	289 : data:0.206 [NUMERICAL]
	289 : data:0.151 [NUMERICAL]
	288 : data:0.232 [NUMERICAL]
	281 : data:0.42 [NUMERICAL]
	280 : data:0.457 [NUMERICAL]
	276 : data:0.242 [NUMERICAL]
	272 : data:0.264 [NUMERICAL]
	262 : data:0.485 [NUMERICAL]
	260 : data:0.469 [NUMERICAL]
	256 : data:0.224 [NUMERICAL]
	255 : data:0.220 [NUMERICAL]
	254 : data:0.368 [NUMERICAL]
	253 : data:0.253 [NUMERICAL]
	252 : data:0.454 [NUMERICAL]
	252 : data:0.303 [NUMERICAL]
	251 : data:0.361 [NUMERICAL]
	250 : data:0.293 [NUMERICAL]
	248 : data:0.268 [NUMERICAL]
	246 : data:0.99 [NUMERICAL]
	241 : data:0.351 [NUMERICAL]
	240 : data:0.487 [NUMERICAL]
	237 : data:0.483 [NUMERICAL]
	237 : data:0.252 [NUMERICAL]
	235 : data:0.168 [NUMERICAL]
	233 : data:0.74 [NUMERICAL]
	229 : data:0.435 [NUMERICAL]
	228 : data:0.176 [NUMERICAL]
	227 : data:0.400 [NUMERICAL]
	225 : data:0.44 [NUMERICAL]
	223 : data:0.4 [NUMERICAL]
	221 : data:0.364 [NUMERICAL]
	221 : data:0.15 [NUMERICAL]
	216 : data:0.296 [NUMERICAL]
	215 : data:0.425 [NUMERICAL]
	209 : data:0.97 [NUMERICAL]
	209 : data:0.153 [NUMERICAL]
	209 : data:0.105 [NUMERICAL]
	207 : data:0.127 [NUMERICAL]
	207 : data:0.124 [NUMERICAL]
	202 : data:0.21 [NUMERICAL]
	200 : data:0.353 [NUMERICAL]
	199 : data:0.187 [NUMERICAL]
	198 : data:0.280 [NUMERICAL]
	197 : data:0.257 [NUMERICAL]
	197 : data:0.107 [NUMERICAL]
	188 : data:0.234 [NUMERICAL]
	188 : data:0.221 [NUMERICAL]
	185 : data:0.199 [NUMERICAL]
	184 : data:0.6 [NUMERICAL]
	184 : data:0.313 [NUMERICAL]
	184 : data:0.3 [NUMERICAL]
	184 : data:0.28 [NUMERICAL]
	183 : data:0.458 [NUMERICAL]
	181 : data:0.498 [NUMERICAL]
	180 : data:0.214 [NUMERICAL]
	174 : data:0.93 [NUMERICAL]
	172 : data:0.52 [NUMERICAL]
	172 : data:0.186 [NUMERICAL]
	170 : data:0.294 [NUMERICAL]
	169 : data:0.446 [NUMERICAL]
	167 : data:0.63 [NUMERICAL]
	166 : data:0.362 [NUMERICAL]
	166 : data:0.207 [NUMERICAL]
	166 : data:0.123 [NUMERICAL]
	164 : data:0.384 [NUMERICAL]
	164 : data:0.32 [NUMERICAL]
	163 : data:0.344 [NUMERICAL]
	162 : data:0.72 [NUMERICAL]
	160 : data:0.417 [NUMERICAL]
	158 : data:0.328 [NUMERICAL]
	157 : data:0.147 [NUMERICAL]
	157 : data:0.126 [NUMERICAL]
	156 : data:0.79 [NUMERICAL]
	155 : data:0.195 [NUMERICAL]
	152 : data:0.200 [NUMERICAL]
	151 : data:0.489 [NUMERICAL]
	151 : data:0.470 [NUMERICAL]
	151 : data:0.385 [NUMERICAL]
	150 : data:0.40 [NUMERICAL]
	150 : data:0.336 [NUMERICAL]
	150 : data:0.113 [NUMERICAL]
	149 : data:0.411 [NUMERICAL]
	147 : data:0.226 [NUMERICAL]
	147 : data:0.19 [NUMERICAL]
	145 : data:0.449 [NUMERICAL]
	145 : data:0.275 [NUMERICAL]
	142 : data:0.185 [NUMERICAL]
	138 : data:0.144 [NUMERICAL]
	137 : data:0.482 [NUMERICAL]
	136 : data:0.132 [NUMERICAL]
	135 : data:0.348 [NUMERICAL]
	134 : data:0.282 [NUMERICAL]
	132 : data:0.436 [NUMERICAL]
	132 : data:0.269 [NUMERICAL]
	132 : data:0.25 [NUMERICAL]
	131 : data:0.338 [NUMERICAL]
	130 : data:0.334 [NUMERICAL]
	129 : data:0.259 [NUMERICAL]
	129 : data:0.238 [NUMERICAL]
	129 : data:0.229 [NUMERICAL]
	129 : data:0.148 [NUMERICAL]
	128 : data:0.459 [NUMERICAL]
	128 : data:0.166 [NUMERICAL]
	127 : data:0.451 [NUMERICAL]
	126 : data:0.43 [NUMERICAL]
	124 : data:0.480 [NUMERICAL]
	123 : data:0.369 [NUMERICAL]
	122 : data:0.494 [NUMERICAL]
	122 : data:0.399 [NUMERICAL]
	122 : data:0.210 [NUMERICAL]
	121 : data:0.287 [NUMERICAL]
	120 : data:0.387 [NUMERICAL]
	120 : data:0.169 [NUMERICAL]
	119 : data:0.304 [NUMERICAL]
	118 : data:0.56 [NUMERICAL]
	117 : data:0.85 [NUMERICAL]
	116 : data:0.333 [NUMERICAL]
	116 : data:0.251 [NUMERICAL]
	116 : data:0.246 [NUMERICAL]
	115 : data:0.410 [NUMERICAL]
	115 : data:0.112 [NUMERICAL]
	114 : data:0.89 [NUMERICAL]
	114 : data:0.117 [NUMERICAL]
	114 : data:0.106 [NUMERICAL]
	113 : data:0.463 [NUMERICAL]
	112 : data:0.228 [NUMERICAL]
	112 : data:0.158 [NUMERICAL]
	111 : data:0.48 [NUMERICAL]
	111 : data:0.37 [NUMERICAL]
	111 : data:0.182 [NUMERICAL]
	110 : data:0.244 [NUMERICAL]
	110 : data:0.211 [NUMERICAL]
	110 : data:0.139 [NUMERICAL]
	110 : data:0.116 [NUMERICAL]
	109 : data:0.337 [NUMERICAL]
	108 : data:0.398 [NUMERICAL]
	108 : data:0.354 [NUMERICAL]
	107 : data:0.66 [NUMERICAL]
	107 : data:0.23 [NUMERICAL]
	106 : data:0.445 [NUMERICAL]
	106 : data:0.288 [NUMERICAL]
	105 : data:0.347 [NUMERICAL]
	105 : data:0.340 [NUMERICAL]
	104 : data:0.9 [NUMERICAL]
	104 : data:0.17 [NUMERICAL]
	103 : data:0.31 [NUMERICAL]
	103 : data:0.227 [NUMERICAL]
	103 : data:0.16 [NUMERICAL]
	103 : data:0.150 [NUMERICAL]
	102 : data:0.239 [NUMERICAL]
	102 : data:0.120 [NUMERICAL]
	101 : data:0.499 [NUMERICAL]
	101 : data:0.477 [NUMERICAL]
	100 : data:0.424 [NUMERICAL]
	100 : data:0.331 [NUMERICAL]
	100 : data:0.292 [NUMERICAL]
	98 : data:0.356 [NUMERICAL]
	98 : data:0.193 [NUMERICAL]
	98 : data:0.119 [NUMERICAL]
	97 : data:0.270 [NUMERICAL]
	97 : data:0.216 [NUMERICAL]
	97 : data:0.149 [NUMERICAL]
	96 : data:0.497 [NUMERICAL]
	96 : data:0.390 [NUMERICAL]
	96 : data:0.320 [NUMERICAL]
	96 : data:0.205 [NUMERICAL]
	95 : data:0.8 [NUMERICAL]
	95 : data:0.405 [NUMERICAL]
	95 : data:0.394 [NUMERICAL]
	95 : data:0.339 [NUMERICAL]
	94 : data:0.429 [NUMERICAL]
	94 : data:0.324 [NUMERICAL]
	94 : data:0.189 [NUMERICAL]
	92 : data:0.75 [NUMERICAL]
	92 : data:0.341 [NUMERICAL]
	92 : data:0.175 [NUMERICAL]
	91 : data:0.55 [NUMERICAL]
	91 : data:0.456 [NUMERICAL]
	91 : data:0.389 [NUMERICAL]
	91 : data:0.231 [NUMERICAL]
	91 : data:0.179 [NUMERICAL]
	90 : data:0.346 [NUMERICAL]
	90 : data:0.286 [NUMERICAL]
	89 : data:0.208 [NUMERICAL]
	89 : data:0.114 [NUMERICAL]
	88 : data:0.478 [NUMERICAL]
	87 : data:0.71 [NUMERICAL]
	87 : data:0.481 [NUMERICAL]
	87 : data:0.24 [NUMERICAL]
	86 : data:0.254 [NUMERICAL]
	86 : data:0.196 [NUMERICAL]
	85 : data:0.88 [NUMERICAL]
	85 : data:0.86 [NUMERICAL]
	85 : data:0.111 [NUMERICAL]
	84 : data:0.468 [NUMERICAL]
	84 : data:0.20 [NUMERICAL]
	83 : data:0.447 [NUMERICAL]
	83 : data:0.401 [NUMERICAL]
	83 : data:0.312 [NUMERICAL]
	83 : data:0.225 [NUMERICAL]
	83 : data:0.143 [NUMERICAL]
	82 : data:0.461 [NUMERICAL]
	82 : data:0.38 [NUMERICAL]
	82 : data:0.355 [NUMERICAL]
	82 : data:0.198 [NUMERICAL]
	82 : data:0.172 [NUMERICAL]
	81 : data:0.440 [NUMERICAL]
	81 : data:0.360 [NUMERICAL]
	81 : data:0.299 [NUMERICAL]
	81 : data:0.218 [NUMERICAL]
	80 : data:0.452 [NUMERICAL]
	80 : data:0.310 [NUMERICAL]
	79 : data:0.365 [NUMERICAL]
	79 : data:0.142 [NUMERICAL]
	79 : data:0.138 [NUMERICAL]
	78 : data:0.73 [NUMERICAL]
	78 : data:0.491 [NUMERICAL]
	78 : data:0.278 [NUMERICAL]
	77 : data:0.432 [NUMERICAL]
	76 : data:0.345 [NUMERICAL]
	76 : data:0.306 [NUMERICAL]
	76 : data:0.271 [NUMERICAL]
	76 : data:0.165 [NUMERICAL]
	75 : data:0.96 [NUMERICAL]
	75 : data:0.133 [NUMERICAL]
	74 : data:0.495 [NUMERICAL]
	74 : data:0.392 [NUMERICAL]
	74 : data:0.152 [NUMERICAL]
	73 : data:0.460 [NUMERICAL]
	73 : data:0.416 [NUMERICAL]
	73 : data:0.359 [NUMERICAL]
	73 : data:0.335 [NUMERICAL]
	73 : data:0.302 [NUMERICAL]
	73 : data:0.13 [NUMERICAL]
	73 : data:0.125 [NUMERICAL]
	72 : data:0.428 [NUMERICAL]
	72 : data:0.367 [NUMERICAL]
	72 : data:0.290 [NUMERICAL]
	72 : data:0.209 [NUMERICAL]
	71 : data:0.29 [NUMERICAL]
	71 : data:0.236 [NUMERICAL]
	70 : data:0.393 [NUMERICAL]
	70 : data:0.212 [NUMERICAL]
	69 : data:0.450 [NUMERICAL]
	69 : data:0.309 [NUMERICAL]
	68 : data:0.358 [NUMERICAL]
	68 : data:0.274 [NUMERICAL]
	68 : data:0.272 [NUMERICAL]
	68 : data:0.134 [NUMERICAL]
	66 : data:0.60 [NUMERICAL]
	66 : data:0.396 [NUMERICAL]
	66 : data:0.323 [NUMERICAL]
	66 : data:0.276 [NUMERICAL]
	66 : data:0.261 [NUMERICAL]
	66 : data:0.237 [NUMERICAL]
	66 : data:0.203 [NUMERICAL]
	66 : data:0.167 [NUMERICAL]
	66 : data:0.162 [NUMERICAL]
	66 : data:0.154 [NUMERICAL]
	65 : data:0.444 [NUMERICAL]
	65 : data:0.409 [NUMERICAL]
	65 : data:0.377 [NUMERICAL]
	65 : data:0.177 [NUMERICAL]
	64 : data:0.50 [NUMERICAL]
	64 : data:0.35 [NUMERICAL]
	64 : data:0.188 [NUMERICAL]
	63 : data:0.98 [NUMERICAL]
	63 : data:0.58 [NUMERICAL]
	63 : data:0.427 [NUMERICAL]
	63 : data:0.374 [NUMERICAL]
	63 : data:0.160 [NUMERICAL]
	62 : data:0.84 [NUMERICAL]
	62 : data:0.476 [NUMERICAL]
	62 : data:0.357 [NUMERICAL]
	61 : data:0.406 [NUMERICAL]
	61 : data:0.135 [NUMERICAL]
	61 : data:0.110 [NUMERICAL]
	61 : data:0.11 [NUMERICAL]
	61 : data:0.10 [NUMERICAL]
	60 : data:0.59 [NUMERICAL]
	60 : data:0.455 [NUMERICAL]
	60 : data:0.382 [NUMERICAL]
	60 : data:0.279 [NUMERICAL]
	60 : data:0.249 [NUMERICAL]
	60 : data:0.163 [NUMERICAL]
	60 : data:0.104 [NUMERICAL]
	59 : data:0.95 [NUMERICAL]
	59 : data:0.53 [NUMERICAL]
	59 : data:0.421 [NUMERICAL]
	59 : data:0.291 [NUMERICAL]
	58 : data:0.64 [NUMERICAL]
	58 : data:0.329 [NUMERICAL]
	58 : data:0.204 [NUMERICAL]
	57 : data:0.462 [NUMERICAL]
	57 : data:0.376 [NUMERICAL]
	57 : data:0.343 [NUMERICAL]
	57 : data:0.192 [NUMERICAL]
	57 : data:0.180 [NUMERICAL]
	57 : data:0.128 [NUMERICAL]
	56 : data:0.422 [NUMERICAL]
	56 : data:0.295 [NUMERICAL]
	56 : data:0.27 [NUMERICAL]
	56 : data:0.164 [NUMERICAL]
	55 : data:0.420 [NUMERICAL]
	55 : data:0.245 [NUMERICAL]
	55 : data:0.243 [NUMERICAL]
	55 : data:0.155 [NUMERICAL]
	54 : data:0.492 [NUMERICAL]
	54 : data:0.332 [NUMERICAL]
	54 : data:0.171 [NUMERICAL]
	54 : data:0.146 [NUMERICAL]
	53 : data:0.45 [NUMERICAL]
	53 : data:0.297 [NUMERICAL]
	53 : data:0.190 [NUMERICAL]
	52 : data:0.240 [NUMERICAL]
	52 : data:0.222 [NUMERICAL]
	52 : data:0.202 [NUMERICAL]
	52 : data:0.191 [NUMERICAL]
	51 : data:0.54 [NUMERICAL]
	51 : data:0.442 [NUMERICAL]
	51 : data:0.273 [NUMERICAL]
	51 : data:0.101 [NUMERICAL]
	51 : data:0.1 [NUMERICAL]
	50 : data:0.464 [NUMERICAL]
	50 : data:0.136 [NUMERICAL]
	50 : data:0.115 [NUMERICAL]
	50 : data:0.102 [NUMERICAL]
	49 : data:0.307 [NUMERICAL]
	49 : data:0.241 [NUMERICAL]
	48 : data:0.5 [NUMERICAL]
	48 : data:0.443 [NUMERICAL]
	48 : data:0.414 [NUMERICAL]
	48 : data:0.402 [NUMERICAL]
	47 : data:0.57 [NUMERICAL]
	47 : data:0.453 [NUMERICAL]
	47 : data:0.289 [NUMERICAL]
	47 : data:0.184 [NUMERICAL]
	47 : data:0.173 [NUMERICAL]
	47 : data:0.118 [NUMERICAL]
	46 : data:0.431 [NUMERICAL]
	46 : data:0.373 [NUMERICAL]
	46 : data:0.319 [NUMERICAL]
	45 : data:0.70 [NUMERICAL]
	45 : data:0.62 [NUMERICAL]
	45 : data:0.311 [NUMERICAL]
	45 : data:0.230 [NUMERICAL]
	45 : data:0.109 [NUMERICAL]
	44 : data:0.94 [NUMERICAL]
	44 : data:0.91 [NUMERICAL]
	43 : data:0.65 [NUMERICAL]
	43 : data:0.437 [NUMERICAL]
	43 : data:0.39 [NUMERICAL]
	43 : data:0.383 [NUMERICAL]
	42 : data:0.381 [NUMERICAL]
	42 : data:0.22 [NUMERICAL]
	42 : data:0.213 [NUMERICAL]
	42 : data:0.103 [NUMERICAL]
	41 : data:0.386 [NUMERICAL]
	41 : data:0.140 [NUMERICAL]
	40 : data:0.83 [NUMERICAL]
	40 : data:0.397 [NUMERICAL]
	40 : data:0.201 [NUMERICAL]
	39 : data:0.69 [NUMERICAL]
	39 : data:0.130 [NUMERICAL]
	38 : data:0.484 [NUMERICAL]
	38 : data:0.300 [NUMERICAL]
	38 : data:0.265 [NUMERICAL]
	37 : data:0.47 [NUMERICAL]
	37 : data:0.372 [NUMERICAL]
	37 : data:0.26 [NUMERICAL]
	37 : data:0.197 [NUMERICAL]
	36 : data:0.77 [NUMERICAL]
	36 : data:0.395 [NUMERICAL]
	36 : data:0.284 [NUMERICAL]
	36 : data:0.247 [NUMERICAL]
	35 : data:0.408 [NUMERICAL]
	35 : data:0.403 [NUMERICAL]
	35 : data:0.141 [NUMERICAL]
	35 : data:0.100 [NUMERICAL]
	34 : data:0.90 [NUMERICAL]
	34 : data:0.78 [NUMERICAL]
	34 : data:0.493 [NUMERICAL]
	34 : data:0.467 [NUMERICAL]
	34 : data:0.438 [NUMERICAL]
	34 : data:0.404 [NUMERICAL]
	34 : data:0.321 [NUMERICAL]
	34 : data:0.217 [NUMERICAL]
	33 : data:0.87 [NUMERICAL]
	33 : data:0.156 [NUMERICAL]
	33 : data:0.0 [NUMERICAL]
	32 : data:0.68 [NUMERICAL]
	32 : data:0.301 [NUMERICAL]
	31 : data:0.51 [NUMERICAL]
	31 : data:0.479 [NUMERICAL]
	31 : data:0.418 [NUMERICAL]
	31 : data:0.256 [NUMERICAL]
	31 : data:0.215 [NUMERICAL]
	30 : data:0.81 [NUMERICAL]
	30 : data:0.465 [NUMERICAL]
	30 : data:0.352 [NUMERICAL]
	30 : data:0.322 [NUMERICAL]
	30 : data:0.315 [NUMERICAL]
	30 : data:0.314 [NUMERICAL]
	29 : data:0.80 [NUMERICAL]
	29 : data:0.363 [NUMERICAL]
	28 : data:0.46 [NUMERICAL]
	28 : data:0.407 [NUMERICAL]
	28 : data:0.366 [NUMERICAL]
	28 : data:0.262 [NUMERICAL]
	28 : data:0.219 [NUMERICAL]
	28 : data:0.194 [NUMERICAL]
	27 : data:0.326 [NUMERICAL]
	27 : data:0.281 [NUMERICAL]
	27 : data:0.170 [NUMERICAL]
	26 : data:0.349 [NUMERICAL]
	25 : data:0.34 [NUMERICAL]
	24 : data:0.412 [NUMERICAL]
	23 : data:0.475 [NUMERICAL]
	23 : data:0.466 [NUMERICAL]
	23 : data:0.388 [NUMERICAL]
	23 : data:0.267 [NUMERICAL]
	23 : data:0.235 [NUMERICAL]
	22 : data:0.76 [NUMERICAL]
	22 : data:0.342 [NUMERICAL]
	22 : data:0.223 [NUMERICAL]
	21 : data:0.233 [NUMERICAL]
	18 : data:0.92 [NUMERICAL]
	18 : data:0.61 [NUMERICAL]
	16 : data:0.7 [NUMERICAL]

Attribute in nodes with depth <= 0:
	71 : data:0.33 [NUMERICAL]
	36 : data:0.488 [NUMERICAL]
	28 : data:0.41 [NUMERICAL]
	28 : data:0.183 [NUMERICAL]
	26 : data:0.151 [NUMERICAL]
	25 : data:0.36 [NUMERICAL]
	25 : data:0.258 [NUMERICAL]
	24 : data:0.30 [NUMERICAL]
	23 : data:0.137 [NUMERICAL]
	21 : data:0.474 [NUMERICAL]
	20 : data:0.49 [NUMERICAL]
	20 : data:0.318 [NUMERICAL]
	20 : data:0.15 [NUMERICAL]
	19 : data:0.14 [NUMERICAL]
	18 : data:0.471 [NUMERICAL]
	18 : data:0.317 [NUMERICAL]
	18 : data:0.145 [NUMERICAL]
	16 : data:0.298 [NUMERICAL]
	16 : data:0.122 [NUMERICAL]
	15 : data:0.419 [NUMERICAL]
	15 : data:0.413 [NUMERICAL]
	15 : data:0.277 [NUMERICAL]
	14 : data:0.423 [NUMERICAL]
	14 : data:0.18 [NUMERICAL]
	13 : data:0.248 [NUMERICAL]
	12 : data:0.303 [NUMERICAL]
	12 : data:0.12 [NUMERICAL]
	11 : data:0.99 [NUMERICAL]
	11 : data:0.206 [NUMERICAL]
	11 : data:0.199 [NUMERICAL]
	10 : data:0.448 [NUMERICAL]
	10 : data:0.446 [NUMERICAL]
	10 : data:0.425 [NUMERICAL]
	10 : data:0.368 [NUMERICAL]
	10 : data:0.330 [NUMERICAL]
	10 : data:0.220 [NUMERICAL]
	10 : data:0.195 [NUMERICAL]
	9 : data:0.52 [NUMERICAL]
	9 : data:0.487 [NUMERICAL]
	9 : data:0.441 [NUMERICAL]
	9 : data:0.327 [NUMERICAL]
	9 : data:0.28 [NUMERICAL]
	9 : data:0.224 [NUMERICAL]
	8 : data:0.375 [NUMERICAL]
	8 : data:0.255 [NUMERICAL]
	7 : data:0.82 [NUMERICAL]
	7 : data:0.417 [NUMERICAL]
	7 : data:0.399 [NUMERICAL]
	7 : data:0.361 [NUMERICAL]
	7 : data:0.353 [NUMERICAL]
	7 : data:0.334 [NUMERICAL]
	7 : data:0.234 [NUMERICAL]
	7 : data:0.168 [NUMERICAL]
	6 : data:0.449 [NUMERICAL]
	6 : data:0.379 [NUMERICAL]
	6 : data:0.312 [NUMERICAL]
	6 : data:0.282 [NUMERICAL]
	6 : data:0.21 [NUMERICAL]
	6 : data:0.127 [NUMERICAL]
	6 : data:0.108 [NUMERICAL]
	5 : data:0.496 [NUMERICAL]
	5 : data:0.457 [NUMERICAL]
	5 : data:0.430 [NUMERICAL]
	5 : data:0.378 [NUMERICAL]
	5 : data:0.351 [NUMERICAL]
	5 : data:0.242 [NUMERICAL]
	5 : data:0.185 [NUMERICAL]
	5 : data:0.147 [NUMERICAL]
	4 : data:0.486 [NUMERICAL]
	4 : data:0.483 [NUMERICAL]
	4 : data:0.380 [NUMERICAL]
	4 : data:0.333 [NUMERICAL]
	4 : data:0.124 [NUMERICAL]
	3 : data:0.72 [NUMERICAL]
	3 : data:0.498 [NUMERICAL]
	3 : data:0.468 [NUMERICAL]
	3 : data:0.44 [NUMERICAL]
	3 : data:0.439 [NUMERICAL]
	3 : data:0.415 [NUMERICAL]
	3 : data:0.401 [NUMERICAL]
	3 : data:0.280 [NUMERICAL]
	3 : data:0.181 [NUMERICAL]
	3 : data:0.117 [NUMERICAL]
	2 : data:0.93 [NUMERICAL]
	2 : data:0.489 [NUMERICAL]
	2 : data:0.469 [NUMERICAL]
	2 : data:0.42 [NUMERICAL]
	2 : data:0.384 [NUMERICAL]
	2 : data:0.357 [NUMERICAL]
	2 : data:0.356 [NUMERICAL]
	2 : data:0.3 [NUMERICAL]
	2 : data:0.283 [NUMERICAL]
	2 : data:0.259 [NUMERICAL]
	2 : data:0.214 [NUMERICAL]
	2 : data:0.211 [NUMERICAL]
	2 : data:0.207 [NUMERICAL]
	2 : data:0.200 [NUMERICAL]
	2 : data:0.2 [NUMERICAL]
	2 : data:0.123 [NUMERICAL]
	2 : data:0.120 [NUMERICAL]
	2 : data:0.113 [NUMERICAL]
	2 : data:0.105 [NUMERICAL]
	1 : data:0.97 [NUMERICAL]
	1 : data:0.89 [NUMERICAL]
	1 : data:0.55 [NUMERICAL]
	1 : data:0.499 [NUMERICAL]
	1 : data:0.494 [NUMERICAL]
	1 : data:0.491 [NUMERICAL]
	1 : data:0.490 [NUMERICAL]
	1 : data:0.47 [NUMERICAL]
	1 : data:0.450 [NUMERICAL]
	1 : data:0.398 [NUMERICAL]
	1 : data:0.387 [NUMERICAL]
	1 : data:0.369 [NUMERICAL]
	1 : data:0.367 [NUMERICAL]
	1 : data:0.296 [NUMERICAL]
	1 : data:0.237 [NUMERICAL]
	1 : data:0.226 [NUMERICAL]
	1 : data:0.216 [NUMERICAL]
	1 : data:0.187 [NUMERICAL]
	1 : data:0.159 [NUMERICAL]

Attribute in nodes with depth <= 1:
	135 : data:0.33 [NUMERICAL]
	79 : data:0.183 [NUMERICAL]
	75 : data:0.488 [NUMERICAL]
	68 : data:0.137 [NUMERICAL]
	62 : data:0.36 [NUMERICAL]
	61 : data:0.471 [NUMERICAL]
	60 : data:0.41 [NUMERICAL]
	58 : data:0.30 [NUMERICAL]
	58 : data:0.151 [NUMERICAL]
	57 : data:0.317 [NUMERICAL]
	49 : data:0.145 [NUMERICAL]
	48 : data:0.49 [NUMERICAL]
	46 : data:0.15 [NUMERICAL]
	45 : data:0.277 [NUMERICAL]
	44 : data:0.258 [NUMERICAL]
	42 : data:0.318 [NUMERICAL]
	41 : data:0.474 [NUMERICAL]
	40 : data:0.423 [NUMERICAL]
	39 : data:0.248 [NUMERICAL]
	39 : data:0.12 [NUMERICAL]
	38 : data:0.122 [NUMERICAL]
	35 : data:0.419 [NUMERICAL]
	33 : data:0.14 [NUMERICAL]
	32 : data:0.330 [NUMERICAL]
	32 : data:0.298 [NUMERICAL]
	31 : data:0.487 [NUMERICAL]
	31 : data:0.413 [NUMERICAL]
	31 : data:0.375 [NUMERICAL]
	31 : data:0.303 [NUMERICAL]
	30 : data:0.199 [NUMERICAL]
	30 : data:0.18 [NUMERICAL]
	30 : data:0.108 [NUMERICAL]
	28 : data:0.457 [NUMERICAL]
	27 : data:0.441 [NUMERICAL]
	26 : data:0.52 [NUMERICAL]
	26 : data:0.234 [NUMERICAL]
	25 : data:0.242 [NUMERICAL]
	25 : data:0.168 [NUMERICAL]
	24 : data:0.446 [NUMERICAL]
	24 : data:0.425 [NUMERICAL]
	24 : data:0.368 [NUMERICAL]
	24 : data:0.206 [NUMERICAL]
	23 : data:0.361 [NUMERICAL]
	22 : data:0.99 [NUMERICAL]
	22 : data:0.353 [NUMERICAL]
	22 : data:0.220 [NUMERICAL]
	21 : data:0.448 [NUMERICAL]
	21 : data:0.430 [NUMERICAL]
	21 : data:0.379 [NUMERICAL]
	21 : data:0.327 [NUMERICAL]
	21 : data:0.255 [NUMERICAL]
	20 : data:0.334 [NUMERICAL]
	20 : data:0.28 [NUMERICAL]
	20 : data:0.224 [NUMERICAL]
	20 : data:0.195 [NUMERICAL]
	19 : data:0.417 [NUMERICAL]
	18 : data:0.449 [NUMERICAL]
	17 : data:0.308 [NUMERICAL]
	17 : data:0.185 [NUMERICAL]
	16 : data:0.127 [NUMERICAL]
	15 : data:0.399 [NUMERICAL]
	15 : data:0.283 [NUMERICAL]
	15 : data:0.129 [NUMERICAL]
	14 : data:0.97 [NUMERICAL]
	14 : data:0.82 [NUMERICAL]
	14 : data:0.486 [NUMERICAL]
	14 : data:0.42 [NUMERICAL]
	14 : data:0.351 [NUMERICAL]
	13 : data:0.439 [NUMERICAL]
	13 : data:0.380 [NUMERICAL]
	13 : data:0.333 [NUMERICAL]
	12 : data:0.496 [NUMERICAL]
	12 : data:0.483 [NUMERICAL]
	12 : data:0.181 [NUMERICAL]
	12 : data:0.124 [NUMERICAL]
	12 : data:0.112 [NUMERICAL]
	11 : data:0.468 [NUMERICAL]
	10 : data:0.490 [NUMERICAL]
	10 : data:0.280 [NUMERICAL]
	10 : data:0.214 [NUMERICAL]
	10 : data:0.21 [NUMERICAL]
	10 : data:0.207 [NUMERICAL]
	10 : data:0.187 [NUMERICAL]
	10 : data:0.105 [NUMERICAL]
	9 : data:0.72 [NUMERICAL]
	9 : data:0.282 [NUMERICAL]
	9 : data:0.259 [NUMERICAL]
	9 : data:0.2 [NUMERICAL]
	9 : data:0.147 [NUMERICAL]
	8 : data:0.489 [NUMERICAL]
	8 : data:0.398 [NUMERICAL]
	8 : data:0.312 [NUMERICAL]
	8 : data:0.3 [NUMERICAL]
	8 : data:0.275 [NUMERICAL]
	8 : data:0.120 [NUMERICAL]
	8 : data:0.117 [NUMERICAL]
	7 : data:0.494 [NUMERICAL]
	7 : data:0.44 [NUMERICAL]
	7 : data:0.433 [NUMERICAL]
	7 : data:0.415 [NUMERICAL]
	7 : data:0.356 [NUMERICAL]
	7 : data:0.266 [NUMERICAL]
	7 : data:0.23 [NUMERICAL]
	7 : data:0.226 [NUMERICAL]
	7 : data:0.211 [NUMERICAL]
	6 : data:0.498 [NUMERICAL]
	6 : data:0.470 [NUMERICAL]
	6 : data:0.469 [NUMERICAL]
	6 : data:0.401 [NUMERICAL]
	6 : data:0.378 [NUMERICAL]
	6 : data:0.113 [NUMERICAL]
	5 : data:0.89 [NUMERICAL]
	5 : data:0.370 [NUMERICAL]
	5 : data:0.364 [NUMERICAL]
	5 : data:0.325 [NUMERICAL]
	5 : data:0.296 [NUMERICAL]
	5 : data:0.253 [NUMERICAL]
	5 : data:0.200 [NUMERICAL]
	5 : data:0.123 [NUMERICAL]
	4 : data:0.66 [NUMERICAL]
	4 : data:0.485 [NUMERICAL]
	4 : data:0.463 [NUMERICAL]
	4 : data:0.443 [NUMERICAL]
	4 : data:0.387 [NUMERICAL]
	4 : data:0.384 [NUMERICAL]
	4 : data:0.344 [NUMERICAL]
	4 : data:0.286 [NUMERICAL]
	4 : data:0.285 [NUMERICAL]
	4 : data:0.269 [NUMERICAL]
	4 : data:0.268 [NUMERICAL]
	4 : data:0.250 [NUMERICAL]
	4 : data:0.246 [NUMERICAL]
	4 : data:0.126 [NUMERICAL]
	3 : data:0.93 [NUMERICAL]
	3 : data:0.85 [NUMERICAL]
	3 : data:0.79 [NUMERICAL]
	3 : data:0.55 [NUMERICAL]
	3 : data:0.497 [NUMERICAL]
	3 : data:0.492 [NUMERICAL]
	3 : data:0.491 [NUMERICAL]
	3 : data:0.480 [NUMERICAL]
	3 : data:0.476 [NUMERICAL]
	3 : data:0.450 [NUMERICAL]
	3 : data:0.434 [NUMERICAL]
	3 : data:0.411 [NUMERICAL]
	3 : data:0.371 [NUMERICAL]
	3 : data:0.362 [NUMERICAL]
	3 : data:0.360 [NUMERICAL]
	3 : data:0.357 [NUMERICAL]
	3 : data:0.354 [NUMERICAL]
	3 : data:0.35 [NUMERICAL]
	3 : data:0.338 [NUMERICAL]
	3 : data:0.316 [NUMERICAL]
	3 : data:0.305 [NUMERICAL]
	3 : data:0.299 [NUMERICAL]
	3 : data:0.294 [NUMERICAL]
	3 : data:0.274 [NUMERICAL]
	3 : data:0.252 [NUMERICAL]
	3 : data:0.216 [NUMERICAL]
	3 : data:0.179 [NUMERICAL]
	3 : data:0.174 [NUMERICAL]
	3 : data:0.150 [NUMERICAL]
	3 : data:0.144 [NUMERICAL]
	2 : data:0.90 [NUMERICAL]
	2 : data:0.88 [NUMERICAL]
	2 : data:0.86 [NUMERICAL]
	2 : data:0.68 [NUMERICAL]
	2 : data:0.65 [NUMERICAL]
	2 : data:0.472 [NUMERICAL]
	2 : data:0.47 [NUMERICAL]
	2 : data:0.462 [NUMERICAL]
	2 : data:0.460 [NUMERICAL]
	2 : data:0.447 [NUMERICAL]
	2 : data:0.445 [NUMERICAL]
	2 : data:0.440 [NUMERICAL]
	2 : data:0.435 [NUMERICAL]
	2 : data:0.426 [NUMERICAL]
	2 : data:0.400 [NUMERICAL]
	2 : data:0.40 [NUMERICAL]
	2 : data:0.391 [NUMERICAL]
	2 : data:0.389 [NUMERICAL]
	2 : data:0.376 [NUMERICAL]
	2 : data:0.369 [NUMERICAL]
	2 : data:0.367 [NUMERICAL]
	2 : data:0.365 [NUMERICAL]
	2 : data:0.359 [NUMERICAL]
	2 : data:0.345 [NUMERICAL]
	2 : data:0.335 [NUMERICAL]
	2 : data:0.273 [NUMERICAL]
	2 : data:0.264 [NUMERICAL]
	2 : data:0.249 [NUMERICAL]
	2 : data:0.238 [NUMERICAL]
	2 : data:0.231 [NUMERICAL]
	2 : data:0.221 [NUMERICAL]
	2 : data:0.218 [NUMERICAL]
	2 : data:0.215 [NUMERICAL]
	2 : data:0.213 [NUMERICAL]
	2 : data:0.204 [NUMERICAL]
	2 : data:0.193 [NUMERICAL]
	2 : data:0.186 [NUMERICAL]
	2 : data:0.178 [NUMERICAL]
	2 : data:0.17 [NUMERICAL]
	2 : data:0.159 [NUMERICAL]
	2 : data:0.138 [NUMERICAL]
	2 : data:0.132 [NUMERICAL]
	2 : data:0.131 [NUMERICAL]
	2 : data:0.125 [NUMERICAL]
	2 : data:0.116 [NUMERICAL]
	2 : data:0.114 [NUMERICAL]
	2 : data:0.111 [NUMERICAL]
	1 : data:0.83 [NUMERICAL]
	1 : data:0.81 [NUMERICAL]
	1 : data:0.76 [NUMERICAL]
	1 : data:0.75 [NUMERICAL]
	1 : data:0.74 [NUMERICAL]
	1 : data:0.71 [NUMERICAL]
	1 : data:0.63 [NUMERICAL]
	1 : data:0.6 [NUMERICAL]
	1 : data:0.58 [NUMERICAL]
	1 : data:0.56 [NUMERICAL]
	1 : data:0.5 [NUMERICAL]
	1 : data:0.499 [NUMERICAL]
	1 : data:0.482 [NUMERICAL]
	1 : data:0.477 [NUMERICAL]
	1 : data:0.467 [NUMERICAL]
	1 : data:0.458 [NUMERICAL]
	1 : data:0.454 [NUMERICAL]
	1 : data:0.45 [NUMERICAL]
	1 : data:0.444 [NUMERICAL]
	1 : data:0.432 [NUMERICAL]
	1 : data:0.431 [NUMERICAL]
	1 : data:0.43 [NUMERICAL]
	1 : data:0.427 [NUMERICAL]
	1 : data:0.421 [NUMERICAL]
	1 : data:0.409 [NUMERICAL]
	1 : data:0.405 [NUMERICAL]
	1 : data:0.4 [NUMERICAL]
	1 : data:0.395 [NUMERICAL]
	1 : data:0.394 [NUMERICAL]
	1 : data:0.393 [NUMERICAL]
	1 : data:0.392 [NUMERICAL]
	1 : data:0.390 [NUMERICAL]
	1 : data:0.385 [NUMERICAL]
	1 : data:0.372 [NUMERICAL]
	1 : data:0.37 [NUMERICAL]
	1 : data:0.350 [NUMERICAL]
	1 : data:0.339 [NUMERICAL]
	1 : data:0.31 [NUMERICAL]
	1 : data:0.309 [NUMERICAL]
	1 : data:0.307 [NUMERICAL]
	1 : data:0.304 [NUMERICAL]
	1 : data:0.291 [NUMERICAL]
	1 : data:0.272 [NUMERICAL]
	1 : data:0.271 [NUMERICAL]
	1 : data:0.270 [NUMERICAL]
	1 : data:0.265 [NUMERICAL]
	1 : data:0.260 [NUMERICAL]
	1 : data:0.251 [NUMERICAL]
	1 : data:0.244 [NUMERICAL]
	1 : data:0.241 [NUMERICAL]
	1 : data:0.237 [NUMERICAL]
	1 : data:0.232 [NUMERICAL]
	1 : data:0.230 [NUMERICAL]
	1 : data:0.228 [NUMERICAL]
	1 : data:0.227 [NUMERICAL]
	1 : data:0.212 [NUMERICAL]
	1 : data:0.205 [NUMERICAL]
	1 : data:0.20 [NUMERICAL]
	1 : data:0.197 [NUMERICAL]
	1 : data:0.191 [NUMERICAL]
	1 : data:0.189 [NUMERICAL]
	1 : data:0.184 [NUMERICAL]
	1 : data:0.177 [NUMERICAL]
	1 : data:0.173 [NUMERICAL]
	1 : data:0.167 [NUMERICAL]
	1 : data:0.166 [NUMERICAL]
	1 : data:0.164 [NUMERICAL]
	1 : data:0.161 [NUMERICAL]
	1 : data:0.160 [NUMERICAL]
	1 : data:0.157 [NUMERICAL]
	1 : data:0.154 [NUMERICAL]
	1 : data:0.148 [NUMERICAL]
	1 : data:0.146 [NUMERICAL]
	1 : data:0.143 [NUMERICAL]
	1 : data:0.140 [NUMERICAL]
	1 : data:0.13 [NUMERICAL]
	1 : data:0.121 [NUMERICAL]
	1 : data:0.119 [NUMERICAL]
	1 : data:0.109 [NUMERICAL]
	1 : data:0.107 [NUMERICAL]
	1 : data:0.1 [NUMERICAL]

Attribute in nodes with depth <= 2:
	212 : data:0.33 [NUMERICAL]
	163 : data:0.183 [NUMERICAL]
	133 : data:0.488 [NUMERICAL]
	123 : data:0.36 [NUMERICAL]
	123 : data:0.137 [NUMERICAL]
	121 : data:0.471 [NUMERICAL]
	107 : data:0.41 [NUMERICAL]
	99 : data:0.317 [NUMERICAL]
	98 : data:0.258 [NUMERICAL]
	91 : data:0.151 [NUMERICAL]
	89 : data:0.474 [NUMERICAL]
	89 : data:0.30 [NUMERICAL]
	85 : data:0.298 [NUMERICAL]
	81 : data:0.145 [NUMERICAL]
	79 : data:0.49 [NUMERICAL]
	76 : data:0.277 [NUMERICAL]
	73 : data:0.419 [NUMERICAL]
	72 : data:0.375 [NUMERICAL]
	72 : data:0.248 [NUMERICAL]
	70 : data:0.14 [NUMERICAL]
	69 : data:0.318 [NUMERICAL]
	69 : data:0.12 [NUMERICAL]
	68 : data:0.18 [NUMERICAL]
	67 : data:0.303 [NUMERICAL]
	62 : data:0.122 [NUMERICAL]
	62 : data:0.108 [NUMERICAL]
	61 : data:0.441 [NUMERICAL]
	61 : data:0.413 [NUMERICAL]
	61 : data:0.15 [NUMERICAL]
	60 : data:0.423 [NUMERICAL]
	59 : data:0.487 [NUMERICAL]
	59 : data:0.330 [NUMERICAL]
	57 : data:0.430 [NUMERICAL]
	56 : data:0.206 [NUMERICAL]
	55 : data:0.457 [NUMERICAL]
	53 : data:0.439 [NUMERICAL]
	52 : data:0.283 [NUMERICAL]
	50 : data:0.168 [NUMERICAL]
	49 : data:0.129 [NUMERICAL]
	47 : data:0.242 [NUMERICAL]
	46 : data:0.353 [NUMERICAL]
	45 : data:0.448 [NUMERICAL]
	45 : data:0.368 [NUMERICAL]
	45 : data:0.199 [NUMERICAL]
	42 : data:0.52 [NUMERICAL]
	42 : data:0.379 [NUMERICAL]
	42 : data:0.195 [NUMERICAL]
	41 : data:0.99 [NUMERICAL]
	41 : data:0.425 [NUMERICAL]
	40 : data:0.361 [NUMERICAL]
	39 : data:0.327 [NUMERICAL]
	39 : data:0.224 [NUMERICAL]
	38 : data:0.417 [NUMERICAL]
	38 : data:0.234 [NUMERICAL]
	37 : data:0.490 [NUMERICAL]
	37 : data:0.308 [NUMERICAL]
	37 : data:0.255 [NUMERICAL]
	37 : data:0.220 [NUMERICAL]
	36 : data:0.82 [NUMERICAL]
	35 : data:0.28 [NUMERICAL]
	35 : data:0.127 [NUMERICAL]
	34 : data:0.97 [NUMERICAL]
	34 : data:0.486 [NUMERICAL]
	34 : data:0.446 [NUMERICAL]
	33 : data:0.181 [NUMERICAL]
	32 : data:0.334 [NUMERICAL]
	31 : data:0.380 [NUMERICAL]
	29 : data:0.415 [NUMERICAL]
	29 : data:0.2 [NUMERICAL]
	29 : data:0.187 [NUMERICAL]
	28 : data:0.333 [NUMERICAL]
	28 : data:0.185 [NUMERICAL]
	27 : data:0.351 [NUMERICAL]
	27 : data:0.305 [NUMERICAL]
	27 : data:0.280 [NUMERICAL]
	27 : data:0.266 [NUMERICAL]
	26 : data:0.42 [NUMERICAL]
	26 : data:0.259 [NUMERICAL]
	26 : data:0.124 [NUMERICAL]
	26 : data:0.105 [NUMERICAL]
	25 : data:0.496 [NUMERICAL]
	25 : data:0.449 [NUMERICAL]
	25 : data:0.399 [NUMERICAL]
	24 : data:0.296 [NUMERICAL]
	24 : data:0.268 [NUMERICAL]
	24 : data:0.21 [NUMERICAL]
	23 : data:0.483 [NUMERICAL]
	23 : data:0.312 [NUMERICAL]
	22 : data:0.364 [NUMERICAL]
	22 : data:0.214 [NUMERICAL]
	21 : data:0.23 [NUMERICAL]
	21 : data:0.147 [NUMERICAL]
	20 : data:0.489 [NUMERICAL]
	20 : data:0.470 [NUMERICAL]
	20 : data:0.325 [NUMERICAL]
	20 : data:0.3 [NUMERICAL]
	20 : data:0.226 [NUMERICAL]
	20 : data:0.157 [NUMERICAL]
	20 : data:0.126 [NUMERICAL]
	20 : data:0.120 [NUMERICAL]
	19 : data:0.253 [NUMERICAL]
	18 : data:0.282 [NUMERICAL]
	18 : data:0.207 [NUMERICAL]
	17 : data:0.72 [NUMERICAL]
	17 : data:0.66 [NUMERICAL]
	17 : data:0.498 [NUMERICAL]
	17 : data:0.433 [NUMERICAL]
	17 : data:0.378 [NUMERICAL]
	17 : data:0.250 [NUMERICAL]
	17 : data:0.159 [NUMERICAL]
	17 : data:0.112 [NUMERICAL]
	16 : data:0.44 [NUMERICAL]
	15 : data:0.89 [NUMERICAL]
	15 : data:0.494 [NUMERICAL]
	15 : data:0.398 [NUMERICAL]
	15 : data:0.391 [NUMERICAL]
	15 : data:0.285 [NUMERICAL]
	15 : data:0.113 [NUMERICAL]
	14 : data:0.469 [NUMERICAL]
	14 : data:0.468 [NUMERICAL]
	14 : data:0.387 [NUMERICAL]
	14 : data:0.275 [NUMERICAL]
	14 : data:0.117 [NUMERICAL]
	13 : data:0.389 [NUMERICAL]
	13 : data:0.384 [NUMERICAL]
	13 : data:0.369 [NUMERICAL]
	13 : data:0.179 [NUMERICAL]
	13 : data:0.161 [NUMERICAL]
	13 : data:0.123 [NUMERICAL]
	12 : data:0.445 [NUMERICAL]
	12 : data:0.370 [NUMERICAL]
	12 : data:0.238 [NUMERICAL]
	12 : data:0.216 [NUMERICAL]
	12 : data:0.211 [NUMERICAL]
	12 : data:0.200 [NUMERICAL]
	12 : data:0.138 [NUMERICAL]
	12 : data:0.131 [NUMERICAL]
	11 : data:0.93 [NUMERICAL]
	11 : data:0.458 [NUMERICAL]
	11 : data:0.400 [NUMERICAL]
	11 : data:0.371 [NUMERICAL]
	11 : data:0.356 [NUMERICAL]
	11 : data:0.239 [NUMERICAL]
	11 : data:0.174 [NUMERICAL]
	10 : data:0.79 [NUMERICAL]
	10 : data:0.67 [NUMERICAL]
	10 : data:0.56 [NUMERICAL]
	10 : data:0.482 [NUMERICAL]
	10 : data:0.480 [NUMERICAL]
	10 : data:0.43 [NUMERICAL]
	10 : data:0.401 [NUMERICAL]
	10 : data:0.354 [NUMERICAL]
	10 : data:0.350 [NUMERICAL]
	10 : data:0.344 [NUMERICAL]
	10 : data:0.269 [NUMERICAL]
	10 : data:0.264 [NUMERICAL]
	10 : data:0.252 [NUMERICAL]
	10 : data:0.150 [NUMERICAL]
	9 : data:0.88 [NUMERICAL]
	9 : data:0.55 [NUMERICAL]
	9 : data:0.473 [NUMERICAL]
	9 : data:0.472 [NUMERICAL]
	9 : data:0.463 [NUMERICAL]
	9 : data:0.435 [NUMERICAL]
	9 : data:0.434 [NUMERICAL]
	9 : data:0.339 [NUMERICAL]
	9 : data:0.338 [NUMERICAL]
	9 : data:0.299 [NUMERICAL]
	9 : data:0.293 [NUMERICAL]
	9 : data:0.246 [NUMERICAL]
	9 : data:0.144 [NUMERICAL]
	8 : data:0.71 [NUMERICAL]
	8 : data:0.485 [NUMERICAL]
	8 : data:0.476 [NUMERICAL]
	8 : data:0.411 [NUMERICAL]
	8 : data:0.359 [NUMERICAL]
	8 : data:0.316 [NUMERICAL]
	8 : data:0.260 [NUMERICAL]
	8 : data:0.232 [NUMERICAL]
	8 : data:0.186 [NUMERICAL]
	8 : data:0.178 [NUMERICAL]
	8 : data:0.153 [NUMERICAL]
	8 : data:0.11 [NUMERICAL]
	8 : data:0.107 [NUMERICAL]
	7 : data:0.90 [NUMERICAL]
	7 : data:0.85 [NUMERICAL]
	7 : data:0.74 [NUMERICAL]
	7 : data:0.65 [NUMERICAL]
	7 : data:0.491 [NUMERICAL]
	7 : data:0.462 [NUMERICAL]
	7 : data:0.447 [NUMERICAL]
	7 : data:0.440 [NUMERICAL]
	7 : data:0.4 [NUMERICAL]
	7 : data:0.376 [NUMERICAL]
	7 : data:0.362 [NUMERICAL]
	7 : data:0.340 [NUMERICAL]
	7 : data:0.336 [NUMERICAL]
	7 : data:0.335 [NUMERICAL]
	7 : data:0.29 [NUMERICAL]
	7 : data:0.274 [NUMERICAL]
	7 : data:0.273 [NUMERICAL]
	7 : data:0.257 [NUMERICAL]
	7 : data:0.251 [NUMERICAL]
	7 : data:0.205 [NUMERICAL]
	7 : data:0.193 [NUMERICAL]
	7 : data:0.17 [NUMERICAL]
	7 : data:0.132 [NUMERICAL]
	7 : data:0.128 [NUMERICAL]
	7 : data:0.121 [NUMERICAL]
	7 : data:0.119 [NUMERICAL]
	7 : data:0.114 [NUMERICAL]
	6 : data:0.63 [NUMERICAL]
	6 : data:0.450 [NUMERICAL]
	6 : data:0.443 [NUMERICAL]
	6 : data:0.426 [NUMERICAL]
	6 : data:0.385 [NUMERICAL]
	6 : data:0.286 [NUMERICAL]
	6 : data:0.271 [NUMERICAL]
	6 : data:0.237 [NUMERICAL]
	6 : data:0.221 [NUMERICAL]
	6 : data:0.189 [NUMERICAL]
	6 : data:0.166 [NUMERICAL]
	6 : data:0.149 [NUMERICAL]
	6 : data:0.130 [NUMERICAL]
	6 : data:0.116 [NUMERICAL]
	5 : data:0.96 [NUMERICAL]
	5 : data:0.75 [NUMERICAL]
	5 : data:0.68 [NUMERICAL]
	5 : data:0.6 [NUMERICAL]
	5 : data:0.53 [NUMERICAL]
	5 : data:0.5 [NUMERICAL]
	5 : data:0.478 [NUMERICAL]
	5 : data:0.456 [NUMERICAL]
	5 : data:0.427 [NUMERICAL]
	5 : data:0.420 [NUMERICAL]
	5 : data:0.40 [NUMERICAL]
	5 : data:0.357 [NUMERICAL]
	5 : data:0.35 [NUMERICAL]
	5 : data:0.345 [NUMERICAL]
	5 : data:0.328 [NUMERICAL]
	5 : data:0.304 [NUMERICAL]
	5 : data:0.302 [NUMERICAL]
	5 : data:0.270 [NUMERICAL]
	5 : data:0.244 [NUMERICAL]
	5 : data:0.227 [NUMERICAL]
	5 : data:0.218 [NUMERICAL]
	5 : data:0.212 [NUMERICAL]
	5 : data:0.167 [NUMERICAL]
	5 : data:0.154 [NUMERICAL]
	5 : data:0.142 [NUMERICAL]
	5 : data:0.13 [NUMERICAL]
	4 : data:0.86 [NUMERICAL]
	4 : data:0.83 [NUMERICAL]
	4 : data:0.73 [NUMERICAL]
	4 : data:0.50 [NUMERICAL]
	4 : data:0.499 [NUMERICAL]
	4 : data:0.497 [NUMERICAL]
	4 : data:0.477 [NUMERICAL]
	4 : data:0.460 [NUMERICAL]
	4 : data:0.454 [NUMERICAL]
	4 : data:0.45 [NUMERICAL]
	4 : data:0.444 [NUMERICAL]
	4 : data:0.431 [NUMERICAL]
	4 : data:0.428 [NUMERICAL]
	4 : data:0.424 [NUMERICAL]
	4 : data:0.421 [NUMERICAL]
	4 : data:0.405 [NUMERICAL]
	4 : data:0.396 [NUMERICAL]
	4 : data:0.395 [NUMERICAL]
	4 : data:0.392 [NUMERICAL]
	4 : data:0.37 [NUMERICAL]
	4 : data:0.367 [NUMERICAL]
	4 : data:0.365 [NUMERICAL]
	4 : data:0.360 [NUMERICAL]
	4 : data:0.331 [NUMERICAL]
	4 : data:0.329 [NUMERICAL]
	4 : data:0.311 [NUMERICAL]
	4 : data:0.31 [NUMERICAL]
	4 : data:0.307 [NUMERICAL]
	4 : data:0.294 [NUMERICAL]
	4 : data:0.291 [NUMERICAL]
	4 : data:0.289 [NUMERICAL]
	4 : data:0.272 [NUMERICAL]
	4 : data:0.249 [NUMERICAL]
	4 : data:0.241 [NUMERICAL]
	4 : data:0.231 [NUMERICAL]
	4 : data:0.229 [NUMERICAL]
	4 : data:0.228 [NUMERICAL]
	4 : data:0.225 [NUMERICAL]
	4 : data:0.208 [NUMERICAL]
	4 : data:0.204 [NUMERICAL]
	4 : data:0.198 [NUMERICAL]
	4 : data:0.182 [NUMERICAL]
	4 : data:0.173 [NUMERICAL]
	4 : data:0.172 [NUMERICAL]
	4 : data:0.160 [NUMERICAL]
	4 : data:0.16 [NUMERICAL]
	4 : data:0.136 [NUMERICAL]
	4 : data:0.118 [NUMERICAL]
	4 : data:0.111 [NUMERICAL]
	4 : data:0.109 [NUMERICAL]
	4 : data:0.104 [NUMERICAL]
	3 : data:0.8 [NUMERICAL]
	3 : data:0.69 [NUMERICAL]
	3 : data:0.59 [NUMERICAL]
	3 : data:0.495 [NUMERICAL]
	3 : data:0.492 [NUMERICAL]
	3 : data:0.48 [NUMERICAL]
	3 : data:0.467 [NUMERICAL]
	3 : data:0.453 [NUMERICAL]
	3 : data:0.452 [NUMERICAL]
	3 : data:0.432 [NUMERICAL]
	3 : data:0.409 [NUMERICAL]
	3 : data:0.404 [NUMERICAL]
	3 : data:0.393 [NUMERICAL]
	3 : data:0.383 [NUMERICAL]
	3 : data:0.377 [NUMERICAL]
	3 : data:0.373 [NUMERICAL]
	3 : data:0.372 [NUMERICAL]
	3 : data:0.355 [NUMERICAL]
	3 : data:0.347 [NUMERICAL]
	3 : data:0.343 [NUMERICAL]
	3 : data:0.322 [NUMERICAL]
	3 : data:0.320 [NUMERICAL]
	3 : data:0.314 [NUMERICAL]
	3 : data:0.313 [NUMERICAL]
	3 : data:0.309 [NUMERICAL]
	3 : data:0.290 [NUMERICAL]
	3 : data:0.279 [NUMERICAL]
	3 : data:0.263 [NUMERICAL]
	3 : data:0.262 [NUMERICAL]
	3 : data:0.256 [NUMERICAL]
	3 : data:0.213 [NUMERICAL]
	3 : data:0.209 [NUMERICAL]
	3 : data:0.202 [NUMERICAL]
	3 : data:0.192 [NUMERICAL]
	3 : data:0.191 [NUMERICAL]
	3 : data:0.19 [NUMERICAL]
	3 : data:0.184 [NUMERICAL]
	3 : data:0.177 [NUMERICAL]
	3 : data:0.175 [NUMERICAL]
	3 : data:0.169 [NUMERICAL]
	3 : data:0.165 [NUMERICAL]
	3 : data:0.148 [NUMERICAL]
	3 : data:0.139 [NUMERICAL]
	3 : data:0.125 [NUMERICAL]
	3 : data:0.115 [NUMERICAL]
	3 : data:0.101 [NUMERICAL]
	3 : data:0.1 [NUMERICAL]
	3 : data:0.0 [NUMERICAL]
	2 : data:0.9 [NUMERICAL]
	2 : data:0.81 [NUMERICAL]
	2 : data:0.76 [NUMERICAL]
	2 : data:0.70 [NUMERICAL]
	2 : data:0.64 [NUMERICAL]
	2 : data:0.58 [NUMERICAL]
	2 : data:0.51 [NUMERICAL]
	2 : data:0.481 [NUMERICAL]
	2 : data:0.47 [NUMERICAL]
	2 : data:0.464 [NUMERICAL]
	2 : data:0.451 [NUMERICAL]
	2 : data:0.436 [NUMERICAL]
	2 : data:0.429 [NUMERICAL]
	2 : data:0.422 [NUMERICAL]
	2 : data:0.418 [NUMERICAL]
	2 : data:0.410 [NUMERICAL]
	2 : data:0.397 [NUMERICAL]
	2 : data:0.390 [NUMERICAL]
	2 : data:0.388 [NUMERICAL]
	2 : data:0.332 [NUMERICAL]
	2 : data:0.321 [NUMERICAL]
	2 : data:0.32 [NUMERICAL]
	2 : data:0.315 [NUMERICAL]
	2 : data:0.310 [NUMERICAL]
	2 : data:0.306 [NUMERICAL]
	2 : data:0.288 [NUMERICAL]
	2 : data:0.287 [NUMERICAL]
	2 : data:0.27 [NUMERICAL]
	2 : data:0.261 [NUMERICAL]
	2 : data:0.254 [NUMERICAL]
	2 : data:0.25 [NUMERICAL]
	2 : data:0.247 [NUMERICAL]
	2 : data:0.243 [NUMERICAL]
	2 : data:0.24 [NUMERICAL]
	2 : data:0.230 [NUMERICAL]
	2 : data:0.222 [NUMERICAL]
	2 : data:0.217 [NUMERICAL]
	2 : data:0.215 [NUMERICAL]
	2 : data:0.201 [NUMERICAL]
	2 : data:0.20 [NUMERICAL]
	2 : data:0.196 [NUMERICAL]
	2 : data:0.176 [NUMERICAL]
	2 : data:0.171 [NUMERICAL]
	2 : data:0.164 [NUMERICAL]
	2 : data:0.158 [NUMERICAL]
	2 : data:0.155 [NUMERICAL]
	2 : data:0.152 [NUMERICAL]
	2 : data:0.146 [NUMERICAL]
	2 : data:0.140 [NUMERICAL]
	2 : data:0.135 [NUMERICAL]
	2 : data:0.134 [NUMERICAL]
	2 : data:0.100 [NUMERICAL]
	2 : data:0.10 [NUMERICAL]
	1 : data:0.95 [NUMERICAL]
	1 : data:0.91 [NUMERICAL]
	1 : data:0.78 [NUMERICAL]
	1 : data:0.62 [NUMERICAL]
	1 : data:0.61 [NUMERICAL]
	1 : data:0.54 [NUMERICAL]
	1 : data:0.493 [NUMERICAL]
	1 : data:0.479 [NUMERICAL]
	1 : data:0.466 [NUMERICAL]
	1 : data:0.465 [NUMERICAL]
	1 : data:0.461 [NUMERICAL]
	1 : data:0.455 [NUMERICAL]
	1 : data:0.437 [NUMERICAL]
	1 : data:0.414 [NUMERICAL]
	1 : data:0.403 [NUMERICAL]
	1 : data:0.402 [NUMERICAL]
	1 : data:0.394 [NUMERICAL]
	1 : data:0.382 [NUMERICAL]
	1 : data:0.374 [NUMERICAL]
	1 : data:0.366 [NUMERICAL]
	1 : data:0.363 [NUMERICAL]
	1 : data:0.358 [NUMERICAL]
	1 : data:0.349 [NUMERICAL]
	1 : data:0.348 [NUMERICAL]
	1 : data:0.346 [NUMERICAL]
	1 : data:0.342 [NUMERICAL]
	1 : data:0.341 [NUMERICAL]
	1 : data:0.324 [NUMERICAL]
	1 : data:0.323 [NUMERICAL]
	1 : data:0.319 [NUMERICAL]
	1 : data:0.301 [NUMERICAL]
	1 : data:0.300 [NUMERICAL]
	1 : data:0.297 [NUMERICAL]
	1 : data:0.278 [NUMERICAL]
	1 : data:0.267 [NUMERICAL]
	1 : data:0.265 [NUMERICAL]
	1 : data:0.245 [NUMERICAL]
	1 : data:0.235 [NUMERICAL]
	1 : data:0.233 [NUMERICAL]
	1 : data:0.223 [NUMERICAL]
	1 : data:0.22 [NUMERICAL]
	1 : data:0.219 [NUMERICAL]
	1 : data:0.203 [NUMERICAL]
	1 : data:0.197 [NUMERICAL]
	1 : data:0.194 [NUMERICAL]
	1 : data:0.190 [NUMERICAL]
	1 : data:0.188 [NUMERICAL]
	1 : data:0.180 [NUMERICAL]
	1 : data:0.170 [NUMERICAL]
	1 : data:0.162 [NUMERICAL]
	1 : data:0.156 [NUMERICAL]
	1 : data:0.143 [NUMERICAL]
	1 : data:0.141 [NUMERICAL]
	1 : data:0.133 [NUMERICAL]
	1 : data:0.110 [NUMERICAL]
	1 : data:0.103 [NUMERICAL]

Attribute in nodes with depth <= 3:
	327 : data:0.33 [NUMERICAL]
	261 : data:0.183 [NUMERICAL]
	217 : data:0.488 [NUMERICAL]
	191 : data:0.41 [NUMERICAL]
	186 : data:0.474 [NUMERICAL]
	186 : data:0.258 [NUMERICAL]
	182 : data:0.36 [NUMERICAL]
	177 : data:0.137 [NUMERICAL]
	173 : data:0.471 [NUMERICAL]
	154 : data:0.283 [NUMERICAL]
	153 : data:0.317 [NUMERICAL]
	153 : data:0.298 [NUMERICAL]
	136 : data:0.419 [NUMERICAL]
	133 : data:0.151 [NUMERICAL]
	131 : data:0.248 [NUMERICAL]
	128 : data:0.30 [NUMERICAL]
	128 : data:0.122 [NUMERICAL]
	127 : data:0.12 [NUMERICAL]
	122 : data:0.375 [NUMERICAL]
	118 : data:0.108 [NUMERICAL]
	117 : data:0.14 [NUMERICAL]
	116 : data:0.49 [NUMERICAL]
	115 : data:0.430 [NUMERICAL]
	115 : data:0.18 [NUMERICAL]
	111 : data:0.277 [NUMERICAL]
	109 : data:0.318 [NUMERICAL]
	109 : data:0.145 [NUMERICAL]
	107 : data:0.439 [NUMERICAL]
	104 : data:0.303 [NUMERICAL]
	104 : data:0.129 [NUMERICAL]
	103 : data:0.413 [NUMERICAL]
	100 : data:0.206 [NUMERICAL]
	99 : data:0.487 [NUMERICAL]
	99 : data:0.330 [NUMERICAL]
	97 : data:0.448 [NUMERICAL]
	97 : data:0.441 [NUMERICAL]
	96 : data:0.423 [NUMERICAL]
	93 : data:0.457 [NUMERICAL]
	92 : data:0.305 [NUMERICAL]
	89 : data:0.15 [NUMERICAL]
	86 : data:0.379 [NUMERICAL]
	86 : data:0.242 [NUMERICAL]
	85 : data:0.181 [NUMERICAL]
	85 : data:0.157 [NUMERICAL]
	84 : data:0.255 [NUMERICAL]
	83 : data:0.490 [NUMERICAL]
	83 : data:0.486 [NUMERICAL]
	81 : data:0.168 [NUMERICAL]
	80 : data:0.327 [NUMERICAL]
	76 : data:0.99 [NUMERICAL]
	76 : data:0.82 [NUMERICAL]
	76 : data:0.368 [NUMERICAL]
	74 : data:0.199 [NUMERICAL]
	73 : data:0.308 [NUMERICAL]
	72 : data:0.250 [NUMERICAL]
	72 : data:0.234 [NUMERICAL]
	72 : data:0.2 [NUMERICAL]
	70 : data:0.361 [NUMERICAL]
	70 : data:0.220 [NUMERICAL]
	70 : data:0.195 [NUMERICAL]
	67 : data:0.353 [NUMERICAL]
	65 : data:0.415 [NUMERICAL]
	63 : data:0.425 [NUMERICAL]
	63 : data:0.378 [NUMERICAL]
	62 : data:0.380 [NUMERICAL]
	62 : data:0.224 [NUMERICAL]
	60 : data:0.351 [NUMERICAL]
	59 : data:0.52 [NUMERICAL]
	58 : data:0.417 [NUMERICAL]
	58 : data:0.296 [NUMERICAL]
	58 : data:0.127 [NUMERICAL]
	57 : data:0.496 [NUMERICAL]
	57 : data:0.280 [NUMERICAL]
	56 : data:0.266 [NUMERICAL]
	55 : data:0.483 [NUMERICAL]
	54 : data:0.446 [NUMERICAL]
	53 : data:0.28 [NUMERICAL]
	52 : data:0.97 [NUMERICAL]
	52 : data:0.268 [NUMERICAL]
	52 : data:0.187 [NUMERICAL]
	49 : data:0.263 [NUMERICAL]
	49 : data:0.253 [NUMERICAL]
	49 : data:0.214 [NUMERICAL]
	48 : data:0.44 [NUMERICAL]
	48 : data:0.42 [NUMERICAL]
	48 : data:0.21 [NUMERICAL]
	47 : data:0.124 [NUMERICAL]
	46 : data:0.334 [NUMERICAL]
	46 : data:0.333 [NUMERICAL]
	46 : data:0.285 [NUMERICAL]
	45 : data:0.489 [NUMERICAL]
	45 : data:0.325 [NUMERICAL]
	44 : data:0.259 [NUMERICAL]
	44 : data:0.185 [NUMERICAL]
	44 : data:0.126 [NUMERICAL]
	44 : data:0.105 [NUMERICAL]
	43 : data:0.67 [NUMERICAL]
	42 : data:0.174 [NUMERICAL]
	42 : data:0.161 [NUMERICAL]
	42 : data:0.159 [NUMERICAL]
	42 : data:0.147 [NUMERICAL]
	41 : data:0.449 [NUMERICAL]
	41 : data:0.364 [NUMERICAL]
	41 : data:0.226 [NUMERICAL]
	40 : data:0.470 [NUMERICAL]
	40 : data:0.433 [NUMERICAL]
	39 : data:0.72 [NUMERICAL]
	39 : data:0.472 [NUMERICAL]
	39 : data:0.399 [NUMERICAL]
	39 : data:0.3 [NUMERICAL]
	38 : data:0.469 [NUMERICAL]
	38 : data:0.264 [NUMERICAL]
	38 : data:0.23 [NUMERICAL]
	37 : data:0.473 [NUMERICAL]
	36 : data:0.178 [NUMERICAL]
	35 : data:0.498 [NUMERICAL]
	35 : data:0.426 [NUMERICAL]
	35 : data:0.400 [NUMERICAL]
	35 : data:0.384 [NUMERICAL]
	35 : data:0.370 [NUMERICAL]
	35 : data:0.282 [NUMERICAL]
	35 : data:0.131 [NUMERICAL]
	34 : data:0.391 [NUMERICAL]
	34 : data:0.350 [NUMERICAL]
	34 : data:0.312 [NUMERICAL]
	34 : data:0.207 [NUMERICAL]
	34 : data:0.200 [NUMERICAL]
	34 : data:0.112 [NUMERICAL]
	33 : data:0.293 [NUMERICAL]
	33 : data:0.120 [NUMERICAL]
	32 : data:0.371 [NUMERICAL]
	32 : data:0.153 [NUMERICAL]
	32 : data:0.113 [NUMERICAL]
	31 : data:0.398 [NUMERICAL]
	30 : data:0.66 [NUMERICAL]
	30 : data:0.494 [NUMERICAL]
	30 : data:0.434 [NUMERICAL]
	30 : data:0.275 [NUMERICAL]
	29 : data:0.232 [NUMERICAL]
	28 : data:0.485 [NUMERICAL]
	28 : data:0.435 [NUMERICAL]
	28 : data:0.294 [NUMERICAL]
	27 : data:0.123 [NUMERICAL]
	27 : data:0.107 [NUMERICAL]
	26 : data:0.93 [NUMERICAL]
	26 : data:0.89 [NUMERICAL]
	26 : data:0.369 [NUMERICAL]
	26 : data:0.260 [NUMERICAL]
	26 : data:0.239 [NUMERICAL]
	26 : data:0.138 [NUMERICAL]
	26 : data:0.121 [NUMERICAL]
	25 : data:0.482 [NUMERICAL]
	25 : data:0.43 [NUMERICAL]
	25 : data:0.387 [NUMERICAL]
	25 : data:0.356 [NUMERICAL]
	25 : data:0.316 [NUMERICAL]
	25 : data:0.117 [NUMERICAL]
	24 : data:0.463 [NUMERICAL]
	24 : data:0.389 [NUMERICAL]
	24 : data:0.344 [NUMERICAL]
	24 : data:0.179 [NUMERICAL]
	24 : data:0.166 [NUMERICAL]
	24 : data:0.150 [NUMERICAL]
	23 : data:0.468 [NUMERICAL]
	23 : data:0.458 [NUMERICAL]
	23 : data:0.401 [NUMERICAL]
	23 : data:0.354 [NUMERICAL]
	23 : data:0.269 [NUMERICAL]
	23 : data:0.211 [NUMERICAL]
	23 : data:0.186 [NUMERICAL]
	22 : data:0.85 [NUMERICAL]
	22 : data:0.4 [NUMERICAL]
	22 : data:0.336 [NUMERICAL]
	22 : data:0.216 [NUMERICAL]
	21 : data:0.339 [NUMERICAL]
	21 : data:0.257 [NUMERICAL]
	21 : data:0.114 [NUMERICAL]
	20 : data:0.79 [NUMERICAL]
	20 : data:0.55 [NUMERICAL]
	20 : data:0.454 [NUMERICAL]
	20 : data:0.445 [NUMERICAL]
	20 : data:0.340 [NUMERICAL]
	20 : data:0.252 [NUMERICAL]
	20 : data:0.246 [NUMERICAL]
	20 : data:0.238 [NUMERICAL]
	20 : data:0.193 [NUMERICAL]
	19 : data:0.74 [NUMERICAL]
	19 : data:0.71 [NUMERICAL]
	19 : data:0.499 [NUMERICAL]
	19 : data:0.40 [NUMERICAL]
	19 : data:0.362 [NUMERICAL]
	19 : data:0.228 [NUMERICAL]
	19 : data:0.221 [NUMERICAL]
	18 : data:0.63 [NUMERICAL]
	18 : data:0.6 [NUMERICAL]
	18 : data:0.480 [NUMERICAL]
	18 : data:0.48 [NUMERICAL]
	18 : data:0.478 [NUMERICAL]
	18 : data:0.299 [NUMERICAL]
	18 : data:0.287 [NUMERICAL]
	18 : data:0.251 [NUMERICAL]
	18 : data:0.227 [NUMERICAL]
	18 : data:0.111 [NUMERICAL]
	18 : data:0.11 [NUMERICAL]
	17 : data:0.88 [NUMERICAL]
	17 : data:0.56 [NUMERICAL]
	17 : data:0.476 [NUMERICAL]
	17 : data:0.447 [NUMERICAL]
	17 : data:0.411 [NUMERICAL]
	17 : data:0.338 [NUMERICAL]
	17 : data:0.335 [NUMERICAL]
	17 : data:0.328 [NUMERICAL]
	17 : data:0.286 [NUMERICAL]
	17 : data:0.244 [NUMERICAL]
	17 : data:0.229 [NUMERICAL]
	17 : data:0.16 [NUMERICAL]
	17 : data:0.144 [NUMERICAL]
	17 : data:0.132 [NUMERICAL]
	16 : data:0.491 [NUMERICAL]
	16 : data:0.405 [NUMERICAL]
	16 : data:0.385 [NUMERICAL]
	16 : data:0.359 [NUMERICAL]
	16 : data:0.357 [NUMERICAL]
	16 : data:0.331 [NUMERICAL]
	16 : data:0.291 [NUMERICAL]
	16 : data:0.271 [NUMERICAL]
	16 : data:0.25 [NUMERICAL]
	16 : data:0.208 [NUMERICAL]
	16 : data:0.176 [NUMERICAL]
	15 : data:0.9 [NUMERICAL]
	15 : data:0.86 [NUMERICAL]
	15 : data:0.440 [NUMERICAL]
	15 : data:0.348 [NUMERICAL]
	15 : data:0.309 [NUMERICAL]
	15 : data:0.307 [NUMERICAL]
	15 : data:0.29 [NUMERICAL]
	15 : data:0.212 [NUMERICAL]
	15 : data:0.149 [NUMERICAL]
	15 : data:0.148 [NUMERICAL]
	14 : data:0.376 [NUMERICAL]
	14 : data:0.360 [NUMERICAL]
	14 : data:0.341 [NUMERICAL]
	14 : data:0.237 [NUMERICAL]
	14 : data:0.17 [NUMERICAL]
	14 : data:0.139 [NUMERICAL]
	13 : data:0.50 [NUMERICAL]
	13 : data:0.432 [NUMERICAL]
	13 : data:0.428 [NUMERICAL]
	13 : data:0.37 [NUMERICAL]
	13 : data:0.313 [NUMERICAL]
	13 : data:0.302 [NUMERICAL]
	13 : data:0.274 [NUMERICAL]
	13 : data:0.273 [NUMERICAL]
	13 : data:0.205 [NUMERICAL]
	13 : data:0.203 [NUMERICAL]
	13 : data:0.19 [NUMERICAL]
	13 : data:0.169 [NUMERICAL]
	13 : data:0.143 [NUMERICAL]
	12 : data:0.73 [NUMERICAL]
	12 : data:0.459 [NUMERICAL]
	12 : data:0.456 [NUMERICAL]
	12 : data:0.450 [NUMERICAL]
	12 : data:0.431 [NUMERICAL]
	12 : data:0.395 [NUMERICAL]
	12 : data:0.367 [NUMERICAL]
	12 : data:0.355 [NUMERICAL]
	12 : data:0.347 [NUMERICAL]
	12 : data:0.345 [NUMERICAL]
	12 : data:0.204 [NUMERICAL]
	12 : data:0.189 [NUMERICAL]
	12 : data:0.172 [NUMERICAL]
	12 : data:0.167 [NUMERICAL]
	12 : data:0.133 [NUMERICAL]
	12 : data:0.119 [NUMERICAL]
	12 : data:0.109 [NUMERICAL]
	11 : data:0.96 [NUMERICAL]
	11 : data:0.95 [NUMERICAL]
	11 : data:0.8 [NUMERICAL]
	11 : data:0.70 [NUMERICAL]
	11 : data:0.65 [NUMERICAL]
	11 : data:0.497 [NUMERICAL]
	11 : data:0.462 [NUMERICAL]
	11 : data:0.460 [NUMERICAL]
	11 : data:0.452 [NUMERICAL]
	11 : data:0.421 [NUMERICAL]
	11 : data:0.392 [NUMERICAL]
	11 : data:0.31 [NUMERICAL]
	11 : data:0.304 [NUMERICAL]
	11 : data:0.278 [NUMERICAL]
	11 : data:0.231 [NUMERICAL]
	11 : data:0.225 [NUMERICAL]
	11 : data:0.218 [NUMERICAL]
	11 : data:0.175 [NUMERICAL]
	11 : data:0.173 [NUMERICAL]
	11 : data:0.160 [NUMERICAL]
	11 : data:0.154 [NUMERICAL]
	11 : data:0.125 [NUMERICAL]
	11 : data:0.116 [NUMERICAL]
	10 : data:0.90 [NUMERICAL]
	10 : data:0.75 [NUMERICAL]
	10 : data:0.5 [NUMERICAL]
	10 : data:0.477 [NUMERICAL]
	10 : data:0.455 [NUMERICAL]
	10 : data:0.444 [NUMERICAL]
	10 : data:0.443 [NUMERICAL]
	10 : data:0.422 [NUMERICAL]
	10 : data:0.416 [NUMERICAL]
	10 : data:0.402 [NUMERICAL]
	10 : data:0.396 [NUMERICAL]
	10 : data:0.35 [NUMERICAL]
	10 : data:0.322 [NUMERICAL]
	10 : data:0.320 [NUMERICAL]
	10 : data:0.311 [NUMERICAL]
	10 : data:0.27 [NUMERICAL]
	10 : data:0.249 [NUMERICAL]
	10 : data:0.213 [NUMERICAL]
	10 : data:0.209 [NUMERICAL]
	10 : data:0.182 [NUMERICAL]
	10 : data:0.162 [NUMERICAL]
	10 : data:0.135 [NUMERICAL]
	10 : data:0.13 [NUMERICAL]
	10 : data:0.128 [NUMERICAL]
	9 : data:0.81 [NUMERICAL]
	9 : data:0.68 [NUMERICAL]
	9 : data:0.59 [NUMERICAL]
	9 : data:0.464 [NUMERICAL]
	9 : data:0.427 [NUMERICAL]
	9 : data:0.420 [NUMERICAL]
	9 : data:0.409 [NUMERICAL]
	9 : data:0.358 [NUMERICAL]
	9 : data:0.343 [NUMERICAL]
	9 : data:0.288 [NUMERICAL]
	9 : data:0.272 [NUMERICAL]
	9 : data:0.270 [NUMERICAL]
	9 : data:0.261 [NUMERICAL]
	9 : data:0.254 [NUMERICAL]
	9 : data:0.24 [NUMERICAL]
	9 : data:0.180 [NUMERICAL]
	9 : data:0.171 [NUMERICAL]
	9 : data:0.165 [NUMERICAL]
	9 : data:0.136 [NUMERICAL]
	8 : data:0.98 [NUMERICAL]
	8 : data:0.94 [NUMERICAL]
	8 : data:0.83 [NUMERICAL]
	8 : data:0.64 [NUMERICAL]
	8 : data:0.58 [NUMERICAL]
	8 : data:0.53 [NUMERICAL]
	8 : data:0.495 [NUMERICAL]
	8 : data:0.481 [NUMERICAL]
	8 : data:0.47 [NUMERICAL]
	8 : data:0.461 [NUMERICAL]
	8 : data:0.429 [NUMERICAL]
	8 : data:0.410 [NUMERICAL]
	8 : data:0.373 [NUMERICAL]
	8 : data:0.323 [NUMERICAL]
	8 : data:0.321 [NUMERICAL]
	8 : data:0.32 [NUMERICAL]
	8 : data:0.290 [NUMERICAL]
	8 : data:0.243 [NUMERICAL]
	8 : data:0.20 [NUMERICAL]
	8 : data:0.184 [NUMERICAL]
	8 : data:0.177 [NUMERICAL]
	8 : data:0.164 [NUMERICAL]
	8 : data:0.142 [NUMERICAL]
	8 : data:0.130 [NUMERICAL]
	8 : data:0.104 [NUMERICAL]
	7 : data:0.492 [NUMERICAL]
	7 : data:0.453 [NUMERICAL]
	7 : data:0.451 [NUMERICAL]
	7 : data:0.45 [NUMERICAL]
	7 : data:0.424 [NUMERICAL]
	7 : data:0.390 [NUMERICAL]
	7 : data:0.337 [NUMERICAL]
	7 : data:0.329 [NUMERICAL]
	7 : data:0.310 [NUMERICAL]
	7 : data:0.289 [NUMERICAL]
	7 : data:0.279 [NUMERICAL]
	7 : data:0.262 [NUMERICAL]
	7 : data:0.241 [NUMERICAL]
	7 : data:0.215 [NUMERICAL]
	7 : data:0.158 [NUMERICAL]
	7 : data:0.152 [NUMERICAL]
	7 : data:0.146 [NUMERICAL]
	7 : data:0.140 [NUMERICAL]
	7 : data:0.118 [NUMERICAL]
	7 : data:0.115 [NUMERICAL]
	7 : data:0.110 [NUMERICAL]
	7 : data:0.106 [NUMERICAL]
	7 : data:0.101 [NUMERICAL]
	7 : data:0.0 [NUMERICAL]
	6 : data:0.62 [NUMERICAL]
	6 : data:0.54 [NUMERICAL]
	6 : data:0.467 [NUMERICAL]
	6 : data:0.442 [NUMERICAL]
	6 : data:0.404 [NUMERICAL]
	6 : data:0.397 [NUMERICAL]
	6 : data:0.394 [NUMERICAL]
	6 : data:0.393 [NUMERICAL]
	6 : data:0.377 [NUMERICAL]
	6 : data:0.365 [NUMERICAL]
	6 : data:0.324 [NUMERICAL]
	6 : data:0.315 [NUMERICAL]
	6 : data:0.314 [NUMERICAL]
	6 : data:0.300 [NUMERICAL]
	6 : data:0.297 [NUMERICAL]
	6 : data:0.295 [NUMERICAL]
	6 : data:0.236 [NUMERICAL]
	6 : data:0.230 [NUMERICAL]
	6 : data:0.222 [NUMERICAL]
	6 : data:0.202 [NUMERICAL]
	6 : data:0.201 [NUMERICAL]
	6 : data:0.198 [NUMERICAL]
	6 : data:0.197 [NUMERICAL]
	6 : data:0.192 [NUMERICAL]
	6 : data:0.191 [NUMERICAL]
	6 : data:0.155 [NUMERICAL]
	6 : data:0.134 [NUMERICAL]
	6 : data:0.10 [NUMERICAL]
	5 : data:0.76 [NUMERICAL]
	5 : data:0.69 [NUMERICAL]
	5 : data:0.60 [NUMERICAL]
	5 : data:0.57 [NUMERICAL]
	5 : data:0.466 [NUMERICAL]
	5 : data:0.465 [NUMERICAL]
	5 : data:0.438 [NUMERICAL]
	5 : data:0.436 [NUMERICAL]
	5 : data:0.414 [NUMERICAL]
	5 : data:0.403 [NUMERICAL]
	5 : data:0.39 [NUMERICAL]
	5 : data:0.381 [NUMERICAL]
	5 : data:0.372 [NUMERICAL]
	5 : data:0.34 [NUMERICAL]
	5 : data:0.326 [NUMERICAL]
	5 : data:0.306 [NUMERICAL]
	5 : data:0.292 [NUMERICAL]
	5 : data:0.276 [NUMERICAL]
	5 : data:0.22 [NUMERICAL]
	5 : data:0.217 [NUMERICAL]
	5 : data:0.196 [NUMERICAL]
	5 : data:0.190 [NUMERICAL]
	5 : data:0.163 [NUMERICAL]
	4 : data:0.91 [NUMERICAL]
	4 : data:0.84 [NUMERICAL]
	4 : data:0.80 [NUMERICAL]
	4 : data:0.479 [NUMERICAL]
	4 : data:0.46 [NUMERICAL]
	4 : data:0.388 [NUMERICAL]
	4 : data:0.382 [NUMERICAL]
	4 : data:0.38 [NUMERICAL]
	4 : data:0.374 [NUMERICAL]
	4 : data:0.363 [NUMERICAL]
	4 : data:0.352 [NUMERICAL]
	4 : data:0.349 [NUMERICAL]
	4 : data:0.346 [NUMERICAL]
	4 : data:0.342 [NUMERICAL]
	4 : data:0.332 [NUMERICAL]
	4 : data:0.245 [NUMERICAL]
	4 : data:0.240 [NUMERICAL]
	4 : data:0.233 [NUMERICAL]
	4 : data:0.156 [NUMERICAL]
	4 : data:0.141 [NUMERICAL]
	4 : data:0.102 [NUMERICAL]
	4 : data:0.1 [NUMERICAL]
	3 : data:0.78 [NUMERICAL]
	3 : data:0.51 [NUMERICAL]
	3 : data:0.493 [NUMERICAL]
	3 : data:0.475 [NUMERICAL]
	3 : data:0.437 [NUMERICAL]
	3 : data:0.418 [NUMERICAL]
	3 : data:0.407 [NUMERICAL]
	3 : data:0.386 [NUMERICAL]
	3 : data:0.383 [NUMERICAL]
	3 : data:0.319 [NUMERICAL]
	3 : data:0.284 [NUMERICAL]
	3 : data:0.256 [NUMERICAL]
	3 : data:0.247 [NUMERICAL]
	3 : data:0.219 [NUMERICAL]
	3 : data:0.210 [NUMERICAL]
	3 : data:0.188 [NUMERICAL]
	3 : data:0.170 [NUMERICAL]
	3 : data:0.100 [NUMERICAL]
	2 : data:0.87 [NUMERICAL]
	2 : data:0.77 [NUMERICAL]
	2 : data:0.61 [NUMERICAL]
	2 : data:0.484 [NUMERICAL]
	2 : data:0.412 [NUMERICAL]
	2 : data:0.408 [NUMERICAL]
	2 : data:0.366 [NUMERICAL]
	2 : data:0.281 [NUMERICAL]
	2 : data:0.267 [NUMERICAL]
	2 : data:0.265 [NUMERICAL]
	2 : data:0.235 [NUMERICAL]
	2 : data:0.223 [NUMERICAL]
	2 : data:0.194 [NUMERICAL]
	2 : data:0.103 [NUMERICAL]
	1 : data:0.92 [NUMERICAL]
	1 : data:0.406 [NUMERICAL]
	1 : data:0.301 [NUMERICAL]
	1 : data:0.26 [NUMERICAL]

Attribute in nodes with depth <= 5:
	756 : data:0.33 [NUMERICAL]
	725 : data:0.283 [NUMERICAL]
	631 : data:0.183 [NUMERICAL]
	571 : data:0.157 [NUMERICAL]
	515 : data:0.305 [NUMERICAL]
	490 : data:0.474 [NUMERICAL]
	472 : data:0.258 [NUMERICAL]
	462 : data:0.129 [NUMERICAL]
	431 : data:0.41 [NUMERICAL]
	421 : data:0.488 [NUMERICAL]
	415 : data:0.439 [NUMERICAL]
	413 : data:0.12 [NUMERICAL]
	380 : data:0.250 [NUMERICAL]
	370 : data:0.181 [NUMERICAL]
	357 : data:0.248 [NUMERICAL]
	347 : data:0.137 [NUMERICAL]
	346 : data:0.298 [NUMERICAL]
	346 : data:0.122 [NUMERICAL]
	340 : data:0.36 [NUMERICAL]
	339 : data:0.255 [NUMERICAL]
	325 : data:0.490 [NUMERICAL]
	324 : data:0.415 [NUMERICAL]
	305 : data:0.471 [NUMERICAL]
	293 : data:0.317 [NUMERICAL]
	289 : data:0.413 [NUMERICAL]
	283 : data:0.30 [NUMERICAL]
	276 : data:0.379 [NUMERICAL]
	273 : data:0.378 [NUMERICAL]
	273 : data:0.263 [NUMERICAL]
	268 : data:0.448 [NUMERICAL]
	268 : data:0.318 [NUMERICAL]
	264 : data:0.430 [NUMERICAL]
	264 : data:0.375 [NUMERICAL]
	263 : data:0.49 [NUMERICAL]
	261 : data:0.419 [NUMERICAL]
	260 : data:0.2 [NUMERICAL]
	257 : data:0.108 [NUMERICAL]
	254 : data:0.82 [NUMERICAL]
	249 : data:0.174 [NUMERICAL]
	246 : data:0.330 [NUMERICAL]
	245 : data:0.67 [NUMERICAL]
	245 : data:0.145 [NUMERICAL]
	242 : data:0.327 [NUMERICAL]
	239 : data:0.277 [NUMERICAL]
	239 : data:0.151 [NUMERICAL]
	235 : data:0.14 [NUMERICAL]
	233 : data:0.266 [NUMERICAL]
	232 : data:0.350 [NUMERICAL]
	230 : data:0.423 [NUMERICAL]
	228 : data:0.486 [NUMERICAL]
	225 : data:0.206 [NUMERICAL]
	224 : data:0.441 [NUMERICAL]
	222 : data:0.433 [NUMERICAL]
	221 : data:0.18 [NUMERICAL]
	220 : data:0.285 [NUMERICAL]
	210 : data:0.308 [NUMERICAL]
	209 : data:0.496 [NUMERICAL]
	207 : data:0.472 [NUMERICAL]
	198 : data:0.380 [NUMERICAL]
	198 : data:0.303 [NUMERICAL]
	198 : data:0.121 [NUMERICAL]
	197 : data:0.178 [NUMERICAL]
	196 : data:0.473 [NUMERICAL]
	192 : data:0.457 [NUMERICAL]
	191 : data:0.487 [NUMERICAL]
	190 : data:0.371 [NUMERICAL]
	189 : data:0.242 [NUMERICAL]
	188 : data:0.426 [NUMERICAL]
	188 : data:0.325 [NUMERICAL]
	185 : data:0.368 [NUMERICAL]
	185 : data:0.316 [NUMERICAL]
	183 : data:0.131 [NUMERICAL]
	181 : data:0.260 [NUMERICAL]
	180 : data:0.99 [NUMERICAL]
	180 : data:0.220 [NUMERICAL]
	180 : data:0.161 [NUMERICAL]
	179 : data:0.159 [NUMERICAL]
	178 : data:0.434 [NUMERICAL]
	178 : data:0.264 [NUMERICAL]
	174 : data:0.224 [NUMERICAL]
	172 : data:0.15 [NUMERICAL]
	171 : data:0.268 [NUMERICAL]
	170 : data:0.253 [NUMERICAL]
	170 : data:0.168 [NUMERICAL]
	168 : data:0.42 [NUMERICAL]
	168 : data:0.391 [NUMERICAL]
	166 : data:0.361 [NUMERICAL]
	166 : data:0.351 [NUMERICAL]
	159 : data:0.296 [NUMERICAL]
	158 : data:0.483 [NUMERICAL]
	153 : data:0.293 [NUMERICAL]
	152 : data:0.370 [NUMERICAL]
	152 : data:0.105 [NUMERICAL]
	149 : data:0.44 [NUMERICAL]
	149 : data:0.435 [NUMERICAL]
	148 : data:0.232 [NUMERICAL]
	148 : data:0.199 [NUMERICAL]
	147 : data:0.353 [NUMERICAL]
	146 : data:0.127 [NUMERICAL]
	145 : data:0.425 [NUMERICAL]
	141 : data:0.97 [NUMERICAL]
	141 : data:0.469 [NUMERICAL]
	140 : data:0.187 [NUMERICAL]
	140 : data:0.124 [NUMERICAL]
	139 : data:0.21 [NUMERICAL]
	138 : data:0.280 [NUMERICAL]
	136 : data:0.52 [NUMERICAL]
	135 : data:0.28 [NUMERICAL]
	134 : data:0.485 [NUMERICAL]
	132 : data:0.252 [NUMERICAL]
	132 : data:0.234 [NUMERICAL]
	132 : data:0.214 [NUMERICAL]
	130 : data:0.400 [NUMERICAL]
	130 : data:0.364 [NUMERICAL]
	129 : data:0.454 [NUMERICAL]
	123 : data:0.176 [NUMERICAL]
	122 : data:0.446 [NUMERICAL]
	121 : data:0.4 [NUMERICAL]
	120 : data:0.195 [NUMERICAL]
	120 : data:0.153 [NUMERICAL]
	119 : data:0.417 [NUMERICAL]
	118 : data:0.74 [NUMERICAL]
	116 : data:0.3 [NUMERICAL]
	116 : data:0.186 [NUMERICAL]
	115 : data:0.498 [NUMERICAL]
	113 : data:0.294 [NUMERICAL]
	111 : data:0.200 [NUMERICAL]
	110 : data:0.147 [NUMERICAL]
	110 : data:0.107 [NUMERICAL]
	108 : data:0.257 [NUMERICAL]
	108 : data:0.126 [NUMERICAL]
	106 : data:0.470 [NUMERICAL]
	105 : data:0.221 [NUMERICAL]
	105 : data:0.123 [NUMERICAL]
	104 : data:0.489 [NUMERICAL]
	103 : data:0.458 [NUMERICAL]
	103 : data:0.449 [NUMERICAL]
	102 : data:0.93 [NUMERICAL]
	102 : data:0.226 [NUMERICAL]
	102 : data:0.207 [NUMERICAL]
	101 : data:0.72 [NUMERICAL]
	101 : data:0.384 [NUMERICAL]
	101 : data:0.344 [NUMERICAL]
	101 : data:0.113 [NUMERICAL]
	100 : data:0.185 [NUMERICAL]
	98 : data:0.6 [NUMERICAL]
	96 : data:0.275 [NUMERICAL]
	93 : data:0.362 [NUMERICAL]
	92 : data:0.112 [NUMERICAL]
	91 : data:0.259 [NUMERICAL]
	91 : data:0.144 [NUMERICAL]
	90 : data:0.494 [NUMERICAL]
	90 : data:0.411 [NUMERICAL]
	90 : data:0.336 [NUMERICAL]
	89 : data:0.63 [NUMERICAL]
	89 : data:0.482 [NUMERICAL]
	89 : data:0.282 [NUMERICAL]
	88 : data:0.399 [NUMERICAL]
	88 : data:0.333 [NUMERICAL]
	88 : data:0.313 [NUMERICAL]
	87 : data:0.56 [NUMERICAL]
	86 : data:0.334 [NUMERICAL]
	85 : data:0.385 [NUMERICAL]
	85 : data:0.328 [NUMERICAL]
	84 : data:0.40 [NUMERICAL]
	83 : data:0.238 [NUMERICAL]
	83 : data:0.132 [NUMERICAL]
	81 : data:0.79 [NUMERICAL]
	80 : data:0.463 [NUMERICAL]
	80 : data:0.354 [NUMERICAL]
	80 : data:0.23 [NUMERICAL]
	80 : data:0.166 [NUMERICAL]
	79 : data:0.89 [NUMERICAL]
	79 : data:0.269 [NUMERICAL]
	78 : data:0.43 [NUMERICAL]
	78 : data:0.32 [NUMERICAL]
	78 : data:0.148 [NUMERICAL]
	77 : data:0.369 [NUMERICAL]
	77 : data:0.229 [NUMERICAL]
	76 : data:0.387 [NUMERICAL]
	76 : data:0.338 [NUMERICAL]
	75 : data:0.66 [NUMERICAL]
	75 : data:0.120 [NUMERICAL]
	74 : data:0.244 [NUMERICAL]
	73 : data:0.85 [NUMERICAL]
	73 : data:0.451 [NUMERICAL]
	73 : data:0.37 [NUMERICAL]
	73 : data:0.348 [NUMERICAL]
	72 : data:0.398 [NUMERICAL]
	71 : data:0.436 [NUMERICAL]
	71 : data:0.211 [NUMERICAL]
	71 : data:0.16 [NUMERICAL]
	71 : data:0.139 [NUMERICAL]
	71 : data:0.117 [NUMERICAL]
	70 : data:0.445 [NUMERICAL]
	70 : data:0.228 [NUMERICAL]
	70 : data:0.19 [NUMERICAL]
	69 : data:0.480 [NUMERICAL]
	69 : data:0.356 [NUMERICAL]
	69 : data:0.239 [NUMERICAL]
	69 : data:0.227 [NUMERICAL]
	67 : data:0.499 [NUMERICAL]
	67 : data:0.459 [NUMERICAL]
	67 : data:0.251 [NUMERICAL]
	65 : data:0.340 [NUMERICAL]
	65 : data:0.339 [NUMERICAL]
	65 : data:0.286 [NUMERICAL]
	65 : data:0.17 [NUMERICAL]
	65 : data:0.150 [NUMERICAL]
	64 : data:0.55 [NUMERICAL]
	64 : data:0.341 [NUMERICAL]
	64 : data:0.216 [NUMERICAL]
	63 : data:0.48 [NUMERICAL]
	63 : data:0.478 [NUMERICAL]
	63 : data:0.410 [NUMERICAL]
	63 : data:0.304 [NUMERICAL]
	63 : data:0.246 [NUMERICAL]
	63 : data:0.175 [NUMERICAL]
	62 : data:0.8 [NUMERICAL]
	62 : data:0.25 [NUMERICAL]
	62 : data:0.179 [NUMERICAL]
	62 : data:0.116 [NUMERICAL]
	61 : data:0.287 [NUMERICAL]
	61 : data:0.158 [NUMERICAL]
	61 : data:0.106 [NUMERICAL]
	60 : data:0.468 [NUMERICAL]
	60 : data:0.401 [NUMERICAL]
	60 : data:0.346 [NUMERICAL]
	60 : data:0.205 [NUMERICAL]
	59 : data:0.389 [NUMERICAL]
	59 : data:0.331 [NUMERICAL]
	59 : data:0.270 [NUMERICAL]
	59 : data:0.114 [NUMERICAL]
	58 : data:0.477 [NUMERICAL]
	58 : data:0.355 [NUMERICAL]
	58 : data:0.193 [NUMERICAL]
	58 : data:0.119 [NUMERICAL]
	57 : data:0.405 [NUMERICAL]
	57 : data:0.390 [NUMERICAL]
	57 : data:0.320 [NUMERICAL]
	57 : data:0.299 [NUMERICAL]
	57 : data:0.288 [NUMERICAL]
	57 : data:0.225 [NUMERICAL]
	57 : data:0.210 [NUMERICAL]
	57 : data:0.169 [NUMERICAL]
	57 : data:0.111 [NUMERICAL]
	56 : data:0.497 [NUMERICAL]
	56 : data:0.447 [NUMERICAL]
	56 : data:0.347 [NUMERICAL]
	56 : data:0.312 [NUMERICAL]
	56 : data:0.310 [NUMERICAL]
	56 : data:0.208 [NUMERICAL]
	55 : data:0.337 [NUMERICAL]
	55 : data:0.189 [NUMERICAL]
	54 : data:0.88 [NUMERICAL]
	54 : data:0.73 [NUMERICAL]
	54 : data:0.71 [NUMERICAL]
	54 : data:0.360 [NUMERICAL]
	54 : data:0.218 [NUMERICAL]
	54 : data:0.182 [NUMERICAL]
	53 : data:0.495 [NUMERICAL]
	53 : data:0.456 [NUMERICAL]
	53 : data:0.424 [NUMERICAL]
	53 : data:0.335 [NUMERICAL]
	53 : data:0.306 [NUMERICAL]
	53 : data:0.292 [NUMERICAL]
	53 : data:0.24 [NUMERICAL]
	53 : data:0.231 [NUMERICAL]
	52 : data:0.9 [NUMERICAL]
	52 : data:0.86 [NUMERICAL]
	52 : data:0.481 [NUMERICAL]
	52 : data:0.345 [NUMERICAL]
	52 : data:0.254 [NUMERICAL]
	52 : data:0.138 [NUMERICAL]
	51 : data:0.429 [NUMERICAL]
	51 : data:0.416 [NUMERICAL]
	50 : data:0.476 [NUMERICAL]
	50 : data:0.461 [NUMERICAL]
	50 : data:0.432 [NUMERICAL]
	50 : data:0.394 [NUMERICAL]
	50 : data:0.359 [NUMERICAL]
	50 : data:0.271 [NUMERICAL]
	49 : data:0.491 [NUMERICAL]
	49 : data:0.309 [NUMERICAL]
	49 : data:0.198 [NUMERICAL]
	49 : data:0.149 [NUMERICAL]
	49 : data:0.143 [NUMERICAL]
	49 : data:0.125 [NUMERICAL]
	48 : data:0.31 [NUMERICAL]
	48 : data:0.20 [NUMERICAL]
	48 : data:0.196 [NUMERICAL]
	47 : data:0.75 [NUMERICAL]
	47 : data:0.396 [NUMERICAL]
	47 : data:0.276 [NUMERICAL]
	46 : data:0.392 [NUMERICAL]
	46 : data:0.367 [NUMERICAL]
	46 : data:0.209 [NUMERICAL]
	46 : data:0.172 [NUMERICAL]
	46 : data:0.133 [NUMERICAL]
	46 : data:0.13 [NUMERICAL]
	45 : data:0.365 [NUMERICAL]
	45 : data:0.278 [NUMERICAL]
	45 : data:0.274 [NUMERICAL]
	45 : data:0.212 [NUMERICAL]
	45 : data:0.154 [NUMERICAL]
	45 : data:0.11 [NUMERICAL]
	44 : data:0.96 [NUMERICAL]
	44 : data:0.50 [NUMERICAL]
	44 : data:0.452 [NUMERICAL]
	44 : data:0.428 [NUMERICAL]
	44 : data:0.324 [NUMERICAL]
	44 : data:0.152 [NUMERICAL]
	43 : data:0.357 [NUMERICAL]
	43 : data:0.290 [NUMERICAL]
	43 : data:0.29 [NUMERICAL]
	42 : data:0.460 [NUMERICAL]
	42 : data:0.444 [NUMERICAL]
	42 : data:0.291 [NUMERICAL]
	42 : data:0.165 [NUMERICAL]
	41 : data:0.98 [NUMERICAL]
	41 : data:0.440 [NUMERICAL]
	41 : data:0.421 [NUMERICAL]
	41 : data:0.393 [NUMERICAL]
	41 : data:0.376 [NUMERICAL]
	41 : data:0.237 [NUMERICAL]
	41 : data:0.164 [NUMERICAL]
	40 : data:0.295 [NUMERICAL]
	40 : data:0.167 [NUMERICAL]
	40 : data:0.135 [NUMERICAL]
	39 : data:0.64 [NUMERICAL]
	39 : data:0.59 [NUMERICAL]
	39 : data:0.58 [NUMERICAL]
	39 : data:0.409 [NUMERICAL]
	39 : data:0.329 [NUMERICAL]
	39 : data:0.302 [NUMERICAL]
	39 : data:0.243 [NUMERICAL]
	39 : data:0.162 [NUMERICAL]
	38 : data:0.60 [NUMERICAL]
	38 : data:0.450 [NUMERICAL]
	38 : data:0.45 [NUMERICAL]
	38 : data:0.38 [NUMERICAL]
	38 : data:0.377 [NUMERICAL]
	38 : data:0.35 [NUMERICAL]
	38 : data:0.279 [NUMERICAL]
	38 : data:0.261 [NUMERICAL]
	38 : data:0.203 [NUMERICAL]
	38 : data:0.192 [NUMERICAL]
	38 : data:0.160 [NUMERICAL]
	38 : data:0.142 [NUMERICAL]
	37 : data:0.95 [NUMERICAL]
	37 : data:0.53 [NUMERICAL]
	37 : data:0.462 [NUMERICAL]
	37 : data:0.273 [NUMERICAL]
	37 : data:0.272 [NUMERICAL]
	37 : data:0.202 [NUMERICAL]
	37 : data:0.104 [NUMERICAL]
	36 : data:0.358 [NUMERICAL]
	36 : data:0.323 [NUMERICAL]
	36 : data:0.249 [NUMERICAL]
	35 : data:0.5 [NUMERICAL]
	35 : data:0.492 [NUMERICAL]
	35 : data:0.455 [NUMERICAL]
	35 : data:0.420 [NUMERICAL]
	35 : data:0.307 [NUMERICAL]
	35 : data:0.27 [NUMERICAL]
	35 : data:0.236 [NUMERICAL]
	35 : data:0.204 [NUMERICAL]
	35 : data:0.190 [NUMERICAL]
	35 : data:0.180 [NUMERICAL]
	35 : data:0.128 [NUMERICAL]
	35 : data:0.101 [NUMERICAL]
	35 : data:0.10 [NUMERICAL]
	34 : data:0.54 [NUMERICAL]
	34 : data:0.427 [NUMERICAL]
	34 : data:0.332 [NUMERICAL]
	34 : data:0.311 [NUMERICAL]
	34 : data:0.171 [NUMERICAL]
	33 : data:0.422 [NUMERICAL]
	33 : data:0.343 [NUMERICAL]
	33 : data:0.163 [NUMERICAL]
	32 : data:0.94 [NUMERICAL]
	32 : data:0.442 [NUMERICAL]
	32 : data:0.300 [NUMERICAL]
	32 : data:0.240 [NUMERICAL]
	32 : data:0.213 [NUMERICAL]
	32 : data:0.188 [NUMERICAL]
	32 : data:0.173 [NUMERICAL]
	32 : data:0.134 [NUMERICAL]
	31 : data:0.84 [NUMERICAL]
	31 : data:0.57 [NUMERICAL]
	31 : data:0.443 [NUMERICAL]
	31 : data:0.431 [NUMERICAL]
	31 : data:0.402 [NUMERICAL]
	31 : data:0.241 [NUMERICAL]
	31 : data:0.191 [NUMERICAL]
	31 : data:0.177 [NUMERICAL]
	31 : data:0.155 [NUMERICAL]
	31 : data:0.146 [NUMERICAL]
	31 : data:0.1 [NUMERICAL]
	30 : data:0.70 [NUMERICAL]
	30 : data:0.453 [NUMERICAL]
	30 : data:0.382 [NUMERICAL]
	30 : data:0.319 [NUMERICAL]
	30 : data:0.297 [NUMERICAL]
	30 : data:0.110 [NUMERICAL]
	29 : data:0.406 [NUMERICAL]
	29 : data:0.381 [NUMERICAL]
	29 : data:0.245 [NUMERICAL]
	29 : data:0.184 [NUMERICAL]
	29 : data:0.130 [NUMERICAL]
	29 : data:0.115 [NUMERICAL]
	29 : data:0.109 [NUMERICAL]
	29 : data:0.103 [NUMERICAL]
	28 : data:0.83 [NUMERICAL]
	28 : data:0.65 [NUMERICAL]
	28 : data:0.374 [NUMERICAL]
	27 : data:0.395 [NUMERICAL]
	27 : data:0.373 [NUMERICAL]
	27 : data:0.289 [NUMERICAL]
	27 : data:0.136 [NUMERICAL]
	27 : data:0.118 [NUMERICAL]
	27 : data:0.102 [NUMERICAL]
	26 : data:0.69 [NUMERICAL]
	26 : data:0.62 [NUMERICAL]
	26 : data:0.414 [NUMERICAL]
	25 : data:0.493 [NUMERICAL]
	25 : data:0.464 [NUMERICAL]
	25 : data:0.397 [NUMERICAL]
	25 : data:0.383 [NUMERICAL]
	25 : data:0.321 [NUMERICAL]
	25 : data:0.247 [NUMERICAL]
	25 : data:0.230 [NUMERICAL]
	24 : data:0.91 [NUMERICAL]
	24 : data:0.39 [NUMERICAL]
	24 : data:0.222 [NUMERICAL]
	24 : data:0.197 [NUMERICAL]
	24 : data:0.140 [NUMERICAL]
	23 : data:0.90 [NUMERICAL]
	23 : data:0.484 [NUMERICAL]
	23 : data:0.372 [NUMERICAL]
	23 : data:0.22 [NUMERICAL]
	23 : data:0.217 [NUMERICAL]
	22 : data:0.68 [NUMERICAL]
	22 : data:0.47 [NUMERICAL]
	22 : data:0.256 [NUMERICAL]
	22 : data:0.201 [NUMERICAL]
	22 : data:0.141 [NUMERICAL]
	22 : data:0.0 [NUMERICAL]
	21 : data:0.81 [NUMERICAL]
	21 : data:0.467 [NUMERICAL]
	21 : data:0.438 [NUMERICAL]
	21 : data:0.437 [NUMERICAL]
	21 : data:0.386 [NUMERICAL]
	21 : data:0.314 [NUMERICAL]
	21 : data:0.215 [NUMERICAL]
	21 : data:0.156 [NUMERICAL]
	20 : data:0.87 [NUMERICAL]
	20 : data:0.77 [NUMERICAL]
	20 : data:0.403 [NUMERICAL]
	20 : data:0.366 [NUMERICAL]
	20 : data:0.363 [NUMERICAL]
	20 : data:0.322 [NUMERICAL]
	20 : data:0.284 [NUMERICAL]
	20 : data:0.265 [NUMERICAL]
	20 : data:0.262 [NUMERICAL]
	19 : data:0.80 [NUMERICAL]
	19 : data:0.46 [NUMERICAL]
	19 : data:0.404 [NUMERICAL]
	19 : data:0.194 [NUMERICAL]
	19 : data:0.170 [NUMERICAL]
	18 : data:0.465 [NUMERICAL]
	18 : data:0.407 [NUMERICAL]
	17 : data:0.78 [NUMERICAL]
	17 : data:0.479 [NUMERICAL]
	17 : data:0.475 [NUMERICAL]
	17 : data:0.26 [NUMERICAL]
	17 : data:0.100 [NUMERICAL]
	16 : data:0.408 [NUMERICAL]
	16 : data:0.352 [NUMERICAL]
	16 : data:0.326 [NUMERICAL]
	16 : data:0.235 [NUMERICAL]
	15 : data:0.466 [NUMERICAL]
	15 : data:0.388 [NUMERICAL]
	15 : data:0.315 [NUMERICAL]
	15 : data:0.223 [NUMERICAL]
	14 : data:0.51 [NUMERICAL]
	14 : data:0.412 [NUMERICAL]
	14 : data:0.349 [NUMERICAL]
	14 : data:0.301 [NUMERICAL]
	14 : data:0.267 [NUMERICAL]
	13 : data:0.76 [NUMERICAL]
	13 : data:0.342 [NUMERICAL]
	12 : data:0.34 [NUMERICAL]
	12 : data:0.233 [NUMERICAL]
	12 : data:0.219 [NUMERICAL]
	10 : data:0.418 [NUMERICAL]
	10 : data:0.281 [NUMERICAL]
	9 : data:0.92 [NUMERICAL]
	7 : data:0.7 [NUMERICAL]
	7 : data:0.61 [NUMERICAL]

Condition type in nodes:
	75115 : HigherCondition
Condition type in nodes with depth <= 0:
	1000 : HigherCondition
Condition type in nodes with depth <= 1:
	3000 : HigherCondition
Condition type in nodes with depth <= 2:
	6954 : HigherCondition
Condition type in nodes with depth <= 3:
	14409 : HigherCondition
Condition type in nodes with depth <= 5:
	47137 : HigherCondition

Training logs:
Number of iteration to final model: 1000
	Iter:1 train-loss:1.380890 valid-loss:1.381917  train-accuracy:0.529594 valid-accuracy:0.505303
	Iter:2 train-loss:1.375539 valid-loss:1.376615  train-accuracy:0.710112 valid-accuracy:0.690814
	Iter:3 train-loss:1.370238 valid-loss:1.371522  train-accuracy:0.728473 valid-accuracy:0.712027
	Iter:4 train-loss:1.365076 valid-loss:1.366500  train-accuracy:0.735328 valid-accuracy:0.717430
	Iter:5 train-loss:1.360067 valid-loss:1.361622  train-accuracy:0.742032 valid-accuracy:0.726236
	Iter:6 train-loss:1.355082 valid-loss:1.356914  train-accuracy:0.743934 valid-accuracy:0.727436
	Iter:16 train-loss:1.309881 valid-loss:1.313299  train-accuracy:0.740731 valid-accuracy:0.725435
	Iter:26 train-loss:1.270877 valid-loss:1.276035  train-accuracy:0.742483 valid-accuracy:0.726836
	Iter:36 train-loss:1.236839 valid-loss:1.244022  train-accuracy:0.744984 valid-accuracy:0.731439
	Iter:46 train-loss:1.206892 valid-loss:1.216093  train-accuracy:0.750938 valid-accuracy:0.738043
	Iter:56 train-loss:1.180718 valid-loss:1.192037  train-accuracy:0.752439 valid-accuracy:0.738643
	Iter:66 train-loss:1.156967 valid-loss:1.170815  train-accuracy:0.755891 valid-accuracy:0.738643
	Iter:76 train-loss:1.134670 valid-loss:1.151232  train-accuracy:0.765548 valid-accuracy:0.745848
	Iter:86 train-loss:1.114216 valid-loss:1.133613  train-accuracy:0.773353 valid-accuracy:0.756254
	Iter:96 train-loss:1.095828 valid-loss:1.117453  train-accuracy:0.775654 valid-accuracy:0.759256
	Iter:106 train-loss:1.078353 valid-loss:1.102318  train-accuracy:0.780107 valid-accuracy:0.762457
	Iter:116 train-loss:1.062239 valid-loss:1.088868  train-accuracy:0.783609 valid-accuracy:0.764058
	Iter:126 train-loss:1.046834 valid-loss:1.076085  train-accuracy:0.787412 valid-accuracy:0.764659
	Iter:136 train-loss:1.032292 valid-loss:1.064124  train-accuracy:0.789863 valid-accuracy:0.767260
	Iter:146 train-loss:1.018144 valid-loss:1.053385  train-accuracy:0.794066 valid-accuracy:0.770262
	Iter:156 train-loss:1.004971 valid-loss:1.042836  train-accuracy:0.797168 valid-accuracy:0.771663
	Iter:166 train-loss:0.992296 valid-loss:1.032908  train-accuracy:0.800370 valid-accuracy:0.773464
	Iter:176 train-loss:0.979772 valid-loss:1.023472  train-accuracy:0.804073 valid-accuracy:0.775665
	Iter:186 train-loss:0.968204 valid-loss:1.014493  train-accuracy:0.806824 valid-accuracy:0.777266
	Iter:196 train-loss:0.956652 valid-loss:1.005643  train-accuracy:0.809776 valid-accuracy:0.779067
	Iter:206 train-loss:0.946099 valid-loss:0.997553  train-accuracy:0.812278 valid-accuracy:0.779868
	Iter:216 train-loss:0.935730 valid-loss:0.990004  train-accuracy:0.815580 valid-accuracy:0.781669
	Iter:226 train-loss:0.926025 valid-loss:0.982634  train-accuracy:0.819233 valid-accuracy:0.784471
	Iter:236 train-loss:0.916659 valid-loss:0.975587  train-accuracy:0.821934 valid-accuracy:0.787272
	Iter:246 train-loss:0.907234 valid-loss:0.968998  train-accuracy:0.823936 valid-accuracy:0.789073
	Iter:256 train-loss:0.898052 valid-loss:0.962650  train-accuracy:0.826737 valid-accuracy:0.789674
	Iter:266 train-loss:0.889076 valid-loss:0.956547  train-accuracy:0.828489 valid-accuracy:0.790875
	Iter:276 train-loss:0.880740 valid-loss:0.950637  train-accuracy:0.831841 valid-accuracy:0.791875
	Iter:286 train-loss:0.872798 valid-loss:0.945083  train-accuracy:0.833692 valid-accuracy:0.792475
	Iter:296 train-loss:0.864747 valid-loss:0.939282  train-accuracy:0.835943 valid-accuracy:0.792275
	Iter:306 train-loss:0.856878 valid-loss:0.933903  train-accuracy:0.837895 valid-accuracy:0.795677
	Iter:316 train-loss:0.849386 valid-loss:0.928718  train-accuracy:0.839796 valid-accuracy:0.796878
	Iter:326 train-loss:0.842036 valid-loss:0.924027  train-accuracy:0.841047 valid-accuracy:0.797478
	Iter:336 train-loss:0.834758 valid-loss:0.919254  train-accuracy:0.842898 valid-accuracy:0.798879
	Iter:346 train-loss:0.827595 valid-loss:0.914740  train-accuracy:0.845299 valid-accuracy:0.799079
	Iter:356 train-loss:0.820429 valid-loss:0.910494  train-accuracy:0.847751 valid-accuracy:0.799680
	Iter:366 train-loss:0.813539 valid-loss:0.905967  train-accuracy:0.849402 valid-accuracy:0.800080
	Iter:376 train-loss:0.807058 valid-loss:0.901851  train-accuracy:0.851203 valid-accuracy:0.801081
	Iter:386 train-loss:0.800335 valid-loss:0.897593  train-accuracy:0.853355 valid-accuracy:0.801681
	Iter:396 train-loss:0.794182 valid-loss:0.893427  train-accuracy:0.854505 valid-accuracy:0.803482
	Iter:406 train-loss:0.788086 valid-loss:0.889395  train-accuracy:0.855506 valid-accuracy:0.804283
	Iter:416 train-loss:0.781969 valid-loss:0.885625  train-accuracy:0.856807 valid-accuracy:0.804883
	Iter:426 train-loss:0.776117 valid-loss:0.882092  train-accuracy:0.858258 valid-accuracy:0.805283
	Iter:436 train-loss:0.770524 valid-loss:0.878721  train-accuracy:0.859359 valid-accuracy:0.805884
	Iter:446 train-loss:0.765116 valid-loss:0.875634  train-accuracy:0.860659 valid-accuracy:0.807885
	Iter:456 train-loss:0.759642 valid-loss:0.872609  train-accuracy:0.861810 valid-accuracy:0.808285
	Iter:466 train-loss:0.754051 valid-loss:0.869520  train-accuracy:0.863061 valid-accuracy:0.808285
	Iter:476 train-loss:0.748754 valid-loss:0.866153  train-accuracy:0.864562 valid-accuracy:0.809886
	Iter:486 train-loss:0.743571 valid-loss:0.862910  train-accuracy:0.866463 valid-accuracy:0.811287
	Iter:496 train-loss:0.738415 valid-loss:0.859923  train-accuracy:0.867364 valid-accuracy:0.811287
	Iter:506 train-loss:0.733397 valid-loss:0.857313  train-accuracy:0.868565 valid-accuracy:0.811887
	Iter:516 train-loss:0.728428 valid-loss:0.854249  train-accuracy:0.870016 valid-accuracy:0.813088
	Iter:526 train-loss:0.723451 valid-loss:0.851443  train-accuracy:0.871567 valid-accuracy:0.813888
	Iter:536 train-loss:0.718627 valid-loss:0.849053  train-accuracy:0.872367 valid-accuracy:0.812888
	Iter:546 train-loss:0.713722 valid-loss:0.846313  train-accuracy:0.873918 valid-accuracy:0.814088
	Iter:556 train-loss:0.708946 valid-loss:0.843807  train-accuracy:0.875219 valid-accuracy:0.815289
	Iter:566 train-loss:0.704547 valid-loss:0.841310  train-accuracy:0.876270 valid-accuracy:0.816090
	Iter:576 train-loss:0.700028 valid-loss:0.839017  train-accuracy:0.876970 valid-accuracy:0.816090
	Iter:586 train-loss:0.695334 valid-loss:0.836908  train-accuracy:0.878671 valid-accuracy:0.817090
	Iter:596 train-loss:0.690915 valid-loss:0.834482  train-accuracy:0.879572 valid-accuracy:0.817491
	Iter:606 train-loss:0.686479 valid-loss:0.832562  train-accuracy:0.880622 valid-accuracy:0.818491
	Iter:616 train-loss:0.682238 valid-loss:0.830530  train-accuracy:0.881223 valid-accuracy:0.818691
	Iter:626 train-loss:0.678263 valid-loss:0.828531  train-accuracy:0.882524 valid-accuracy:0.819292
	Iter:636 train-loss:0.674207 valid-loss:0.826366  train-accuracy:0.884075 valid-accuracy:0.819692
	Iter:646 train-loss:0.670285 valid-loss:0.824285  train-accuracy:0.884475 valid-accuracy:0.819492
	Iter:656 train-loss:0.666287 valid-loss:0.822453  train-accuracy:0.885426 valid-accuracy:0.820092
	Iter:666 train-loss:0.662519 valid-loss:0.820456  train-accuracy:0.886376 valid-accuracy:0.820092
	Iter:676 train-loss:0.658496 valid-loss:0.818699  train-accuracy:0.886977 valid-accuracy:0.820893
	Iter:686 train-loss:0.654634 valid-loss:0.816900  train-accuracy:0.887877 valid-accuracy:0.820092
	Iter:696 train-loss:0.650947 valid-loss:0.814997  train-accuracy:0.888578 valid-accuracy:0.820292
	Iter:706 train-loss:0.647202 valid-loss:0.813166  train-accuracy:0.889778 valid-accuracy:0.820492
	Iter:716 train-loss:0.643500 valid-loss:0.811458  train-accuracy:0.890029 valid-accuracy:0.821293
	Iter:726 train-loss:0.640016 valid-loss:0.809444  train-accuracy:0.890979 valid-accuracy:0.821693
	Iter:736 train-loss:0.636382 valid-loss:0.807523  train-accuracy:0.891580 valid-accuracy:0.822293
	Iter:746 train-loss:0.632848 valid-loss:0.806193  train-accuracy:0.892380 valid-accuracy:0.821893
	Iter:756 train-loss:0.629320 valid-loss:0.804492  train-accuracy:0.893431 valid-accuracy:0.821893
	Iter:766 train-loss:0.625775 valid-loss:0.802768  train-accuracy:0.893681 valid-accuracy:0.822694
	Iter:776 train-loss:0.622257 valid-loss:0.801124  train-accuracy:0.895032 valid-accuracy:0.822694
	Iter:786 train-loss:0.618678 valid-loss:0.799547  train-accuracy:0.896082 valid-accuracy:0.823494
	Iter:796 train-loss:0.615337 valid-loss:0.797976  train-accuracy:0.896933 valid-accuracy:0.823494
	Iter:806 train-loss:0.612013 valid-loss:0.796608  train-accuracy:0.897533 valid-accuracy:0.824295
	Iter:816 train-loss:0.608827 valid-loss:0.795131  train-accuracy:0.898684 valid-accuracy:0.825295
	Iter:826 train-loss:0.605525 valid-loss:0.793691  train-accuracy:0.898984 valid-accuracy:0.824895
	Iter:836 train-loss:0.602527 valid-loss:0.792124  train-accuracy:0.899735 valid-accuracy:0.824695
	Iter:846 train-loss:0.599287 valid-loss:0.790879  train-accuracy:0.900986 valid-accuracy:0.826296
	Iter:856 train-loss:0.596142 valid-loss:0.789682  train-accuracy:0.901686 valid-accuracy:0.826696
	Iter:866 train-loss:0.593030 valid-loss:0.788376  train-accuracy:0.901936 valid-accuracy:0.827296
	Iter:876 train-loss:0.589835 valid-loss:0.786982  train-accuracy:0.902937 valid-accuracy:0.827697
	Iter:886 train-loss:0.586996 valid-loss:0.785859  train-accuracy:0.903637 valid-accuracy:0.827897
	Iter:896 train-loss:0.584018 valid-loss:0.784548  train-accuracy:0.904188 valid-accuracy:0.827496
	Iter:906 train-loss:0.581247 valid-loss:0.783313  train-accuracy:0.904638 valid-accuracy:0.828097
	Iter:916 train-loss:0.578193 valid-loss:0.781895  train-accuracy:0.905188 valid-accuracy:0.828297
	Iter:926 train-loss:0.575346 valid-loss:0.780741  train-accuracy:0.905889 valid-accuracy:0.829298
	Iter:936 train-loss:0.572445 valid-loss:0.779375  train-accuracy:0.906489 valid-accuracy:0.830098
	Iter:946 train-loss:0.569669 valid-loss:0.778196  train-accuracy:0.907090 valid-accuracy:0.829898
	Iter:956 train-loss:0.566793 valid-loss:0.777132  train-accuracy:0.907790 valid-accuracy:0.830298
	Iter:966 train-loss:0.564069 valid-loss:0.776177  train-accuracy:0.907990 valid-accuracy:0.829298
	Iter:976 train-loss:0.561383 valid-loss:0.774965  train-accuracy:0.909041 valid-accuracy:0.830098
	Iter:986 train-loss:0.558566 valid-loss:0.773807  train-accuracy:0.909641 valid-accuracy:0.829698
	Iter:996 train-loss:0.555870 valid-loss:0.772907  train-accuracy:0.910492 valid-accuracy:0.830698
24985
781/781 [==============================] - 7s 5ms/step
0.17102261356814086
RANDOM FOREST
trees: 1, Out-of-bag evaluation: accuracy:0.687763 logloss:11.2542
	trees: 11, Out-of-bag evaluation: accuracy:0.740156 logloss:2.09033
	trees: 21, Out-of-bag evaluation: accuracy:0.765097 logloss:0.925065
	trees: 31, Out-of-bag evaluation: accuracy:0.773653 logloss:0.663827
	trees: 41, Out-of-bag evaluation: accuracy:0.779907 logloss:0.596981
	trees: 51, Out-of-bag evaluation: accuracy:0.781158 logloss:0.538951
	trees: 61, Out-of-bag evaluation: accuracy:0.782659 logloss:0.521063
	trees: 71, Out-of-bag evaluation: accuracy:0.78411 logloss:0.506725
	trees: 81, Out-of-bag evaluation: accuracy:0.78461 logloss:0.501798
	trees: 91, Out-of-bag evaluation: accuracy:0.787862 logloss:0.493345
	trees: 101, Out-of-bag evaluation: accuracy:0.789813 logloss:0.492231
	trees: 111, Out-of-bag evaluation: accuracy:0.791164 logloss:0.486369
	trees: 121, Out-of-bag evaluation: accuracy:0.791064 logloss:0.486536
	trees: 131, Out-of-bag evaluation: accuracy:0.791114 logloss:0.484854
	trees: 141, Out-of-bag evaluation: accuracy:0.792215 logloss:0.485648
	trees: 151, Out-of-bag evaluation: accuracy:0.792465 logloss:0.484983
	trees: 161, Out-of-bag evaluation: accuracy:0.791364 logloss:0.485329
	trees: 171, Out-of-bag evaluation: accuracy:0.791214 logloss:0.482786
	trees: 181, Out-of-bag evaluation: accuracy:0.792115 logloss:0.481033
	trees: 191, Out-of-bag evaluation: accuracy:0.792165 logloss:0.480738
	trees: 201, Out-of-bag evaluation: accuracy:0.792715 logloss:0.479466
	trees: 211, Out-of-bag evaluation: accuracy:0.793416 logloss:0.476036
	trees: 221, Out-of-bag evaluation: accuracy:0.792565 logloss:0.474253
	trees: 231, Out-of-bag evaluation: accuracy:0.791214 logloss:0.474426
	trees: 241, Out-of-bag evaluation: accuracy:0.791264 logloss:0.474824
	trees: 251, Out-of-bag evaluation: accuracy:0.791865 logloss:0.472728
	trees: 261, Out-of-bag evaluation: accuracy:0.791615 logloss:0.471726
	trees: 271, Out-of-bag evaluation: accuracy:0.791464 logloss:0.472262
	trees: 281, Out-of-bag evaluation: accuracy:0.790964 logloss:0.472819
	trees: 291, Out-of-bag evaluation: accuracy:0.790564 logloss:0.473442
	trees: 301, Out-of-bag evaluation: accuracy:0.790914 logloss:0.473932
	trees: 311, Out-of-bag evaluation: accuracy:0.790714 logloss:0.474809
	trees: 321, Out-of-bag evaluation: accuracy:0.789613 logloss:0.474655
	trees: 331, Out-of-bag evaluation: accuracy:0.790414 logloss:0.474931
	trees: 341, Out-of-bag evaluation: accuracy:0.790214 logloss:0.474999
	trees: 351, Out-of-bag evaluation: accuracy:0.790064 logloss:0.474724
	trees: 361, Out-of-bag evaluation: accuracy:0.790364 logloss:0.474299
	trees: 371, Out-of-bag evaluation: accuracy:0.790514 logloss:0.474091
	trees: 381, Out-of-bag evaluation: accuracy:0.790814 logloss:0.472354
	trees: 391, Out-of-bag evaluation: accuracy:0.790614 logloss:0.472812
	trees: 401, Out-of-bag evaluation: accuracy:0.791164 logloss:0.472971
	trees: 411, Out-of-bag evaluation: accuracy:0.790664 logloss:0.473101
	trees: 421, Out-of-bag evaluation: accuracy:0.790714 logloss:0.473361
	trees: 431, Out-of-bag evaluation: accuracy:0.791014 logloss:0.473063
	trees: 441, Out-of-bag evaluation: accuracy:0.790914 logloss:0.472543
	trees: 451, Out-of-bag evaluation: accuracy:0.790764 logloss:0.472544
	trees: 461, Out-of-bag evaluation: accuracy:0.791164 logloss:0.472236
	trees: 471, Out-of-bag evaluation: accuracy:0.790714 logloss:0.472624
	trees: 481, Out-of-bag evaluation: accuracy:0.791114 logloss:0.4726
	trees: 491, Out-of-bag evaluation: accuracy:0.790664 logloss:0.473164
	trees: 501, Out-of-bag evaluation: accuracy:0.791114 logloss:0.473488
	trees: 511, Out-of-bag evaluation: accuracy:0.790964 logloss:0.473554
	trees: 521, Out-of-bag evaluation: accuracy:0.790914 logloss:0.473471
	trees: 531, Out-of-bag evaluation: accuracy:0.791464 logloss:0.47364
	trees: 541, Out-of-bag evaluation: accuracy:0.791815 logloss:0.473728
	trees: 551, Out-of-bag evaluation: accuracy:0.791114 logloss:0.473684
	trees: 561, Out-of-bag evaluation: accuracy:0.791615 logloss:0.473647
	trees: 571, Out-of-bag evaluation: accuracy:0.791965 logloss:0.473676
	trees: 581, Out-of-bag evaluation: accuracy:0.792065 logloss:0.473882
	trees: 591, Out-of-bag evaluation: accuracy:0.791164 logloss:0.474236
	trees: 601, Out-of-bag evaluation: accuracy:0.791865 logloss:0.474302
	trees: 611, Out-of-bag evaluation: accuracy:0.792065 logloss:0.474006
	trees: 621, Out-of-bag evaluation: accuracy:0.791815 logloss:0.474131
	trees: 631, Out-of-bag evaluation: accuracy:0.791715 logloss:0.474084
	trees: 641, Out-of-bag evaluation: accuracy:0.791665 logloss:0.474112
	trees: 651, Out-of-bag evaluation: accuracy:0.791414 logloss:0.474226
	trees: 661, Out-of-bag evaluation: accuracy:0.791414 logloss:0.474595
	trees: 671, Out-of-bag evaluation: accuracy:0.791114 logloss:0.474605
	trees: 681, Out-of-bag evaluation: accuracy:0.791314 logloss:0.474698
	trees: 691, Out-of-bag evaluation: accuracy:0.791715 logloss:0.4744
	trees: 701, Out-of-bag evaluation: accuracy:0.791264 logloss:0.47453
	trees: 711, Out-of-bag evaluation: accuracy:0.791414 logloss:0.474494
	trees: 721, Out-of-bag evaluation: accuracy:0.791264 logloss:0.474647
	trees: 731, Out-of-bag evaluation: accuracy:0.791014 logloss:0.474629
	trees: 741, Out-of-bag evaluation: accuracy:0.790864 logloss:0.474775
	trees: 751, Out-of-bag evaluation: accuracy:0.790714 logloss:0.474957
	trees: 761, Out-of-bag evaluation: accuracy:0.790914 logloss:0.475191
	trees: 771, Out-of-bag evaluation: accuracy:0.790614 logloss:0.475341
	trees: 781, Out-of-bag evaluation: accuracy:0.790013 logloss:0.475217
	trees: 791, Out-of-bag evaluation: accuracy:0.790264 logloss:0.475427
	trees: 801, Out-of-bag evaluation: accuracy:0.790013 logloss:0.475532
	trees: 811, Out-of-bag evaluation: accuracy:0.790013 logloss:0.475479
	trees: 821, Out-of-bag evaluation: accuracy:0.790864 logloss:0.475298
	trees: 831, Out-of-bag evaluation: accuracy:0.790514 logloss:0.474959
	trees: 841, Out-of-bag evaluation: accuracy:0.790514 logloss:0.474952
	trees: 851, Out-of-bag evaluation: accuracy:0.790664 logloss:0.475202
	trees: 861, Out-of-bag evaluation: accuracy:0.790464 logloss:0.475198
	trees: 871, Out-of-bag evaluation: accuracy:0.790714 logloss:0.475275
	trees: 881, Out-of-bag evaluation: accuracy:0.790614 logloss:0.474945
	trees: 891, Out-of-bag evaluation: accuracy:0.790864 logloss:0.474826
	trees: 901, Out-of-bag evaluation: accuracy:0.790664 logloss:0.47497
	trees: 911, Out-of-bag evaluation: accuracy:0.790614 logloss:0.475061
	trees: 921, Out-of-bag evaluation: accuracy:0.790714 logloss:0.475101
	trees: 931, Out-of-bag evaluation: accuracy:0.790564 logloss:0.475169
	trees: 941, Out-of-bag evaluation: accuracy:0.790514 logloss:0.47526
	trees: 951, Out-of-bag evaluation: accuracy:0.790614 logloss:0.475292
	trees: 961, Out-of-bag evaluation: accuracy:0.790414 logloss:0.475296
	trees: 971, Out-of-bag evaluation: accuracy:0.790414 logloss:0.47493
	trees: 981, Out-of-bag evaluation: accuracy:0.790564 logloss:0.475029
	trees: 991, Out-of-bag evaluation: accuracy:0.790764 logloss:0.47508
	trees: 1000, Out-of-bag evaluation: accuracy:0.790714 logloss:0.475121


NN_withoutdropout:
Epoch 1/30
625/625 [==============================] - 4s 5ms/step - loss: 0.6652 - accuracy: 0.6224 - val_loss: 0.6140 - val_accuracy: 0.7234
Epoch 2/30
625/625 [==============================] - 3s 4ms/step - loss: 0.5298 - accuracy: 0.7679 - val_loss: 0.4772 - val_accuracy: 0.7975
Epoch 3/30
625/625 [==============================] - 3s 5ms/step - loss: 0.4335 - accuracy: 0.8138 - val_loss: 0.4376 - val_accuracy: 0.8099
Epoch 4/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3977 - accuracy: 0.8287 - val_loss: 0.4223 - val_accuracy: 0.8195
Epoch 5/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3770 - accuracy: 0.8389 - val_loss: 0.4133 - val_accuracy: 0.8263
Epoch 6/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3613 - accuracy: 0.8457 - val_loss: 0.4087 - val_accuracy: 0.8277
Epoch 7/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3483 - accuracy: 0.8538 - val_loss: 0.4095 - val_accuracy: 0.8305
Epoch 8/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3374 - accuracy: 0.8596 - val_loss: 0.4026 - val_accuracy: 0.8327
Epoch 9/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3271 - accuracy: 0.8643 - val_loss: 0.4072 - val_accuracy: 0.8343
Epoch 10/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3181 - accuracy: 0.8695 - val_loss: 0.4044 - val_accuracy: 0.8343
Epoch 11/30
625/625 [==============================] - 3s 4ms/step - loss: 0.3089 - accuracy: 0.8746 - val_loss: 0.4032 - val_accuracy: 0.8347
Epoch 12/30
625/625 [==============================] - 3s 4ms/step - loss: 0.2997 - accuracy: 0.8776 - val_loss: 0.4037 - val_accuracy: 0.8365
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 500)               250500

 dense_1 (Dense)             (None, 400)               200400

 dense_2 (Dense)             (None, 300)               120300

 dense_3 (Dense)             (None, 200)               60200

 dense_4 (Dense)             (None, 100)               20100

 dense_5 (Dense)             (None, 1)                 101

=================================================================
Total params: 651,601
Trainable params: 651,601
Non-trainable params: 0
_________________________________________________________________
Epoch 0: Training loss = 0.665219783782959
Epoch 1: Training loss = 0.5297887325286865
Epoch 2: Training loss = 0.43348073959350586
Epoch 3: Training loss = 0.3977319598197937
Epoch 4: Training loss = 0.37697190046310425
Epoch 5: Training loss = 0.3613295555114746
Epoch 6: Training loss = 0.34834131598472595
Epoch 7: Training loss = 0.33737242221832275
Epoch 8: Training loss = 0.3271445631980896
Epoch 9: Training loss = 0.3181488811969757
Epoch 10: Training loss = 0.30891022086143494
Epoch 11: Training loss = 0.2996615767478943

2023-03-17 03:38:29.183012: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
24985
781/781 [==============================] - 2s 1ms/step
0.1636181709025415
