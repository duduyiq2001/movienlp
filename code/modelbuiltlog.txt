NN with reduced learning rate
Epoch 1/30
157/157 [==============================] - 3s 13ms/step - loss: 0.6081 - accuracy: 0.7100 - val_loss: 0.5036 - val_accuracy: 0.7525
Epoch 2/30
157/157 [==============================] - 2s 11ms/step - loss: 0.4135 - accuracy: 0.8044 - val_loss: 0.4917 - val_accuracy: 0.7617
Epoch 3/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3718 - accuracy: 0.8293 - val_loss: 0.4806 - val_accuracy: 0.7724
Epoch 4/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3339 - accuracy: 0.8559 - val_loss: 0.4941 - val_accuracy: 0.7719
Epoch 5/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3003 - accuracy: 0.8737 - val_loss: 0.4978 - val_accuracy: 0.7723
Epoch 6/30
157/157 [==============================] - 2s 11ms/step - loss: 0.2639 - accuracy: 0.8891 - val_loss: 0.5415 - val_accuracy: 0.7584
Epoch 7/30
157/157 [==============================] - 2s 12ms/step - loss: 0.2282 - accuracy: 0.9095 - val_loss: 0.5563 - val_accuracy: 0.7658
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 500)               192500

 dropout (Dropout)           (None, 500)               0

 dense_1 (Dense)             (None, 400)               200400

 dense_2 (Dense)             (None, 300)               120300

 dense_3 (Dense)             (None, 200)               60200

 dense_4 (Dense)             (None, 100)               20100

 dense_5 (Dense)             (None, 1)                 101

=================================================================
Total params: 593,601
Trainable params: 593,601
Non-trainable params: 0
_________________________________________________________________
Epoch 0: Training loss = 0.6080584526062012
Epoch 1: Training loss = 0.41353556513786316
Epoch 2: Training loss = 0.37176552414894104
Epoch 3: Training loss = 0.333913654088974
Epoch 4: Training loss = 0.30029913783073425
Epoch 5: Training loss = 0.2638636529445648
Epoch 6: Training loss = 0.22816653549671173
1/1 [==============================] - 0s 70ms/step
Model with reduced layers and learning rate:


Epoch 1/30
157/157 [==============================] - 3s 12ms/step - loss: 0.6614 - accuracy: 0.6581 - val_loss: 0.6034 - val_accuracy: 0.7503
Epoch 2/30
157/157 [==============================] - 2s 11ms/step - loss: 0.5170 - accuracy: 0.7822 - val_loss: 0.4939 - val_accuracy: 0.7627
Epoch 3/30
157/157 [==============================] - 2s 11ms/step - loss: 0.4281 - accuracy: 0.8048 - val_loss: 0.4690 - val_accuracy: 0.7731
Epoch 4/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3942 - accuracy: 0.8243 - val_loss: 0.4682 - val_accuracy: 0.7728
Epoch 5/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3776 - accuracy: 0.8265 - val_loss: 0.4667 - val_accuracy: 0.7741
Epoch 6/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3621 - accuracy: 0.8379 - val_loss: 0.4660 - val_accuracy: 0.7758
Epoch 7/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3449 - accuracy: 0.8495 - val_loss: 0.4661 - val_accuracy: 0.7775
Epoch 8/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3346 - accuracy: 0.8591 - val_loss: 0.4683 - val_accuracy: 0.7770
Epoch 9/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3230 - accuracy: 0.8643 - val_loss: 0.4783 - val_accuracy: 0.7738
Epoch 10/30
157/157 [==============================] - 2s 11ms/step - loss: 0.3113 - accuracy: 0.8705 - val_loss: 0.4745 - val_accuracy: 0.7769
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 dense (Dense)               (None, 500)               192500

 dropout (Dropout)           (None, 500)               0

 dense_1 (Dense)             (None, 200)               100200

 dense_2 (Dense)             (None, 1)                 201

=================================================================
Total params: 292,901
Trainable params: 292,901
Non-trainable params: 0
_________________________________________________________________
Epoch 0: Training loss = 0.6614459753036499
Epoch 1: Training loss = 0.5170077681541443
Epoch 2: Training loss = 0.4280579686164856
Epoch 3: Training loss = 0.3941812217235565
Epoch 4: Training loss = 0.37759435176849365
Epoch 5: Training loss = 0.36209195852279663
Epoch 6: Training loss = 0.34487858414649963
Epoch 7: Training loss = 0.33463531732559204
Epoch 8: Training loss = 0.32295510172843933
Epoch 9: Training loss = 0.31132420897483826

NN_resnet:
2023-03-13 22:26:24.023430: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 38376960 exceeds 10% of free system memory.
24985
781/781 [==============================] - 2s 1ms/step
0.21889133480088052

NN_reducedlayerslearningrate:
24985
781/781 [==============================] - 1s 1ms/step
0.21669001400840504
NN_NN_realwithdropoutreducedlearningrate:
0.21861116670002
NN_reslong:
 python NNlongresnet.py
2023-03-13 22:50:53.617048: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-13 22:50:54.189382: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
Epoch 1/30
157/157 [==============================] - 4s 15ms/step - loss: 0.6932 - accuracy: 0.5218 - val_loss: 0.6910 - val_accuracy: 0.5371
Epoch 2/30
157/157 [==============================] - 2s 13ms/step - loss: 0.6916 - accuracy: 0.5356 - val_loss: 0.6888 - val_accuracy: 0.5653
Epoch 3/30
157/157 [==============================] - 2s 13ms/step - loss: 0.6879 - accuracy: 0.5771 - val_loss: 0.6829 - val_accuracy: 0.6209
Epoch 4/30
157/157 [==============================] - 2s 13ms/step - loss: 0.6769 - accuracy: 0.6605 - val_loss: 0.6663 - val_accuracy: 0.7170
Epoch 5/30
157/157 [==============================] - 2s 13ms/step - loss: 0.6489 - accuracy: 0.7432 - val_loss: 0.6292 - val_accuracy: 0.7502
Epoch 6/30
157/157 [==============================] - 2s 12ms/step - loss: 0.5953 - accuracy: 0.7746 - val_loss: 0.5720 - val_accuracy: 0.7544
Epoch 7/30
157/157 [==============================] - 2s 13ms/step - loss: 0.5335 - accuracy: 0.7742 - val_loss: 0.5248 - val_accuracy: 0.7588
Epoch 8/30
157/157 [==============================] - 2s 13ms/step - loss: 0.4806 - accuracy: 0.7940 - val_loss: 0.4959 - val_accuracy: 0.7641
Epoch 9/30
157/157 [==============================] - 2s 13ms/step - loss: 0.4528 - accuracy: 0.7976 - val_loss: 0.4868 - val_accuracy: 0.7628
Epoch 10/30
157/157 [==============================] - 2s 13ms/step - loss: 0.4341 - accuracy: 0.8008 - val_loss: 0.4759 - val_accuracy: 0.7701
Epoch 11/30
157/157 [==============================] - 2s 13ms/step - loss: 0.4209 - accuracy: 0.8086 - val_loss: 0.4727 - val_accuracy: 0.7716
Epoch 12/30
157/157 [==============================] - 2s 13ms/step - loss: 0.4087 - accuracy: 0.8161 - val_loss: 0.4755 - val_accuracy: 0.7711
Epoch 13/30
157/157 [==============================] - 2s 12ms/step - loss: 0.4045 - accuracy: 0.8203 - val_loss: 0.4732 - val_accuracy: 0.7723
Epoch 14/30
157/157 [==============================] - 2s 13ms/step - loss: 0.3921 - accuracy: 0.8261 - val_loss: 0.4711 - val_accuracy: 0.7742
Epoch 15/30
157/157 [==============================] - 2s 13ms/step - loss: 0.3889 - accuracy: 0.8277 - val_loss: 0.4710 - val_accuracy: 0.7740
Epoch 16/30
157/157 [==============================] - 2s 13ms/step - loss: 0.3832 - accuracy: 0.8269 - val_loss: 0.4705 - val_accuracy: 0.7741
Epoch 17/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3814 - accuracy: 0.8341 - val_loss: 0.4694 - val_accuracy: 0.7755
Epoch 18/30
157/157 [==============================] - 2s 13ms/step - loss: 0.3705 - accuracy: 0.8325 - val_loss: 0.4740 - val_accuracy: 0.7738
Epoch 19/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3699 - accuracy: 0.8345 - val_loss: 0.4707 - val_accuracy: 0.7754
Epoch 20/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3674 - accuracy: 0.8325 - val_loss: 0.4737 - val_accuracy: 0.7745
Epoch 21/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3640 - accuracy: 0.8373 - val_loss: 0.4714 - val_accuracy: 0.7761
Epoch 22/30
157/157 [==============================] - 2s 12ms/step - loss: 0.3608 - accuracy: 0.8417 - val_loss: 0.4801 - val_accuracy: 0.7712
Model: "model"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 input_1 (InputLayer)           [(None, 384)]        0           []

 dense (Dense)                  (None, 500)          192500      ['input_1[0][0]']

 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']

 dense_1 (Dense)                (None, 400)          200400      ['dropout[0][0]']

 dense_2 (Dense)                (None, 300)          120300      ['dense_1[0][0]']

 dense_3 (Dense)                (None, 200)          60200       ['dense_2[0][0]']

 dense_4 (Dense)                (None, 100)          20100       ['dense_3[0][0]']

 concatenate (Concatenate)      (None, 484)          0           ['input_1[0][0]',
                                                                  'dense_4[0][0]']

 dense_5 (Dense)                (None, 1)            485         ['concatenate[0][0]']

==================================================================================================
Total params: 593,985
Trainable params: 593,985
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 0: Training loss = 0.6932463645935059
Epoch 1: Training loss = 0.6916493773460388
Epoch 2: Training loss = 0.6878948211669922
Epoch 3: Training loss = 0.6769357323646545
Epoch 4: Training loss = 0.6489042639732361
Epoch 5: Training loss = 0.5952885150909424
Epoch 6: Training loss = 0.5334519147872925
Epoch 7: Training loss = 0.4805859923362732
Epoch 8: Training loss = 0.45277369022369385
Epoch 9: Training loss = 0.434068500995636
Epoch 10: Training loss = 0.42086946964263916
Epoch 11: Training loss = 0.40868768095970154
Epoch 12: Training loss = 0.4045465588569641
Epoch 13: Training loss = 0.39208388328552246
Epoch 14: Training loss = 0.3889114558696747
Epoch 15: Training loss = 0.3832228481769562
Epoch 16: Training loss = 0.3814454674720764
Epoch 17: Training loss = 0.3704719543457031
Epoch 18: Training loss = 0.36994168162345886
Epoch 19: Training loss = 0.3673742115497589
Epoch 20: Training loss = 0.36402300000190735
Epoch 21: Training loss = 0.3608050048351288
2023-03-13 22:53:27.140700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-13 22:53:27.565750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
24985
781/781 [==============================] - 2s 1ms/step
0.21989193516109662

the correctness of randommix
['D', 'J', 'f', 'M', 'e', 'N', 'L', 'd', 'O', 'c', 'K', 'H', 'b', 'L', 'a', 'Q']
[-1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1]
(tf) PS C:\Users\Administrator\Desktop\nlp\movienlp\code> python random_mix.py
['WMDKW', 'WKNDWKD', 'f', 'AS', 'e', 'P', 'd', 'D', 'J', 'M', 'c', 'b', 'a', 'Q', 'L', 'H', 'K', 'O', 'L', 'N']
[-1, -1, 1, -1, 1, -1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1]


LOgreg with BOW:
2023-03-14 02:39:38.871496: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-14 02:39:39.286428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
[0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1]
Training error rate: 0.0009606147934678733
Validation error rate: 0.3035821492895737

sk Decision tree with BOW:
 python decision_tree_scikit.py
2023-03-14 02:50:37.603891: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-03-14 02:50:38.012661: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 Super with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 7.5
Training error rate: 0.08181235991034264
Validation error rate: 0.319231538923354
